---
title: "SIN CORRECCION"
author: "Karina Roitman"
date: "2024-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Modelos sin corrección de efecto batch

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```

sin dicotomizar


```{r}
library(dplyr)

# Eliminar la columna 'label' usando dplyr
#featureMatrix <- featureMatrix %>% select(-label)


```


```{r}
featureMatrix<-as.data.frame(featureMatrix)
featureMatrix_label<-featureMatrix[, c(2:74,76)]
```


```{r}
#featureMatrix_bind<- cbind(featureMatrix_label, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos
```



```{r}
# Renombrar la columna 'label' a 'Y' y convertirla en factor
#names(featureMatrix_label)[names(featureMatrix_label) == "covid"] <- "Y"

featureMatrix_label <- as.data.frame(featureMatrix_label)
featureMatrix_label <- featureMatrix_label %>% rename(Y = covid)
featureMatrix_label$Y<-as.factor(featureMatrix_label$Y)
str(featureMatrix_label)

# Verificar estructura del dataframe después de los cambios


```




```{r}

# Convertir columnas 1 a 18 a numérico
for (i in 1:(ncol(featureMatrix_label) - 1)) {
  featureMatrix_label[[i]] <- as.numeric(featureMatrix_label[[i]])
}

str(featureMatrix_label)
```

```{r}
# library(dplyr)
# 
# featureMatrix_label$Y<-as.factor(featureMatrix_label$Y)
# str(featureMatrix_label$Y)
# featureMatrix_label$Y <- as.numeric(featureMatrix_label$Y)
# featureMatrix_label$Y <- as.factor(featureMatrix_label$Y)
# str(featureMatrix_label)
```



```{r}
#str(featureMatrix_bind)
```

```{r}
featureMatrix_bind<-featureMatrix_label
```

```{r}
featureMatrix_bind_nodic_cc<- featureMatrix_bind[1:227,]
ciegoMatrix_nodic_cc<-featureMatrix_bind[228:311,]
```


```{r}
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_bind_nodic_cc, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)


# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
table(trainData$Y)
table(testData$Y)

```


```{r}
#set.seed(42)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```


```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



```{r}
# Submuestras y repeticiones
set.seed(42)
# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```


# Random forest

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry)
                               #min.node.size = seq(1, 30, 2),
                               #min.node.size=min.node.size,
                               #splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}
# genera un vector de nrow(hiperparametros) números aleatorios entre 1 y 500.
#Esto asegura que cada combinación de hiperparámetros tenga una semilla diferente en cada iteración.

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

trainData$Y <- factor(as.numeric(factor(trainData$Y)))
trainData$Y <- factor(trainData$Y, levels = c("1", "2"))
levels(trainData$Y) <- make.names(levels(trainData$Y))

```

```{r}

#trainData <- trainData[, c(1:61, 63)]
```


```{r}
library(caret)
#control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

# RF <- caret::train(Y ~ ., data = trainData, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación
RF_matrix_nodic_cargafx <- function(df_train, model, grid, metrica = "Accuracy", control) {

  
  # Entrenar el modelo
  RF <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(RF)
  plot(RF)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF)
}

RF<-RF_matrix_nodic_cargafx(df_train=trainData, model="rf", grid=grid_rf, metrica="Accuracy", control=cross_val_rf )
print(RF)
```




```{r}
plot(RF)
```



# KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_knn <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}
#tuneGrid <- expand.grid(k = 1:15)
#set.seed(42)
# Ajustar el modelo KNN
# KNN <- caret::train(formula, 
#                     data = trainData, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNN
KNN_matrix_cc_fx <- function(df_train, model, grid, metrica = "Accuracy",  control, preProcess) {

  
  # Entrenar el modelo
  KNN <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNN)
  plot(KNN)
  
  # Guardar resultado
  #save(KNN_matrix, file = paste0(title, ".rda"))
  
  return(KNN)
}

KNN<-KNN_matrix_cc_fx(df_train=trainData, model="knn", grid=tuneGrid, metrica="Accuracy", control=cross_val_knn ,preProcess = c("center","scale") )
print(KNN)
```


```{r}
plot(KNN)
```

# GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)
max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_glm <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```



```{r}

# #tuneGrid <- expand.grid(
# #  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
# #  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda)
# #set.seed(42)
# GLM <- caret::train(formula, 
#                   data = trainData,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLM
GLM_matrix_cc_fx <- function(df_train, model, grid, metrica = "Accuracy",  control) {

  
  # Entrenar el modelo
  GLM <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(GLM)
  plot(GLM)
  
  # Guardar resultado
 # save(GLM_matrix, file = paste0(title, ".rda"))
  
  return(GLM)
}

GLM<-GLM_matrix_cc_fx(df_train=trainData, model="glmnet", grid=tuneGrid, control=cross_val_glm )
print(GLM)
```


```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# Prediccion

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```


```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF_matrix_cc<-predict(RF, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF_matrix_cc)

prediccionesRF_matrix_cc <- factor(as.character(prediccionesRF_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesRF_matrix_cc <- factor(prediccionesRF_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
prob_predRF <- predict(RF, newdata = testData_NOID, type = "prob")
```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF[, "X2"], levels = c(1, 2))
AUC_RF_matrix_cc <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_matrix_cc, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```



```{r}
# confusion_matrixRF <- table(prediccionesRF, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRF)
RF_metrics_matrix_cc <- caret::confusionMatrix(prediccionesRF_matrix_cc, y_test, positive = "Cov.Pos")
```



```{r}
library(caret)
accuracyRF_matrix_cc <- RF_metrics_matrix_cc$overall["Accuracy"]
kappaRF_matrix_cc <- RF_metrics_matrix_cc$overall["Kappa"]
# Métricas por clase
sensitivityRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Sensitivity"]
specificityRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Specificity"]
precisionRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Pos Pred Value"]
recallRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRF_matrix_cc <- RF_metrics_matrix_cc$byClass["F1"]
npvRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Neg Pred Value"]
prevalenceRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Prevalence"]
detection_rateRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Detection Rate"]
balanced_accuracyRF_matrix_cc <- RF_metrics_matrix_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRF_matrix_cc <- sensitivityRF_matrix_cc / (1 - specificityRF_matrix_cc)
LR_minusRF_matrix_cc <- (1 - sensitivityRF_matrix_cc) / specificityRF_matrix_cc

# Para manejar valores especiales
LR_plusRF_matrix_cc <- ifelse(is.nan(LR_plusRF_matrix_cc) | is.infinite(LR_plusRF_matrix_cc), NA, LR_plusRF_matrix_cc)
LR_minusRF_matrix_cc <- ifelse(is.nan(LR_minusRF_matrix_cc) | is.infinite(LR_minusRF_matrix_cc), NA, LR_minusRF_matrix_cc)

# Crear un dataframe con todas las métricas
metricsRF_matrix_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RF_matrix_cc = c(accuracyRF_matrix_cc, kappaRF_matrix_cc, sensitivityRF_matrix_cc, specificityRF_matrix_cc, precisionRF_matrix_cc, 
            f1_scoreRF_matrix_cc, npvRF_matrix_cc, prevalenceRF_matrix_cc, detection_rateRF_matrix_cc, 
            balanced_accuracyRF_matrix_cc, LR_plusRF_matrix_cc, LR_minusRF_matrix_cc, AUC_RF_matrix_cc))

# Mostrar los resultados
print(metricsRF_matrix_cc)


# library(irr)
# 
# # Calcular Kappa
# kappa_RF <- kappa2(cbind(prediccionesRF, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RF$value, 3)))
# kappa_RF<-round(kappa_RF$value, 3)
```

# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_matrix_cc<-predict(GLM, newdata = testData_NOID)#matriz binarizada
print(predGLM_matrix_cc)

predGLM_matrix_cc <- factor(as.character(predGLM_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLM_matrix_cc <- factor(predGLM_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixGLM <- table(predGLM, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLM)
GLM_metrics_matrix_cc <- caret::confusionMatrix(predGLM_matrix_cc, y_test, positive = "Cov.Pos")
```


```{r}


# Predicciones probabilísticas con el modelo GLM
prob_predGLM <- predict(GLM, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_matrix_cc <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_matrix_cc, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```


```{r}
library(caret)
accuracyGLM_matrix_cc <- GLM_metrics_matrix_cc$overall["Accuracy"]
kappaGLM_matrix_cc <- GLM_metrics_matrix_cc$overall["Kappa"]
# Métricas por clase
sensitivityGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Sensitivity"]
specificityGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Specificity"]
precisionGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Pos Pred Value"]
recallGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["F1"]
npvGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Neg Pred Value"]
prevalenceGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Prevalence"]
detection_rateGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Detection Rate"]
balanced_accuracyGLM_matrix_cc <- GLM_metrics_matrix_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusGLM_matrix_cc <- sensitivityGLM_matrix_cc / (1 - specificityGLM_matrix_cc)
LR_minusGLM_matrix_cc <- (1 - sensitivityGLM_matrix_cc) / specificityGLM_matrix_cc

# Para manejar valores especiales
LR_plusGLM_matrix_cc <- ifelse(is.nan(LR_plusGLM_matrix_cc) | is.infinite(LR_plusGLM_matrix_cc), NA, LR_plusGLM_matrix_cc)
LR_minusGLM_matrix_cc <- ifelse(is.nan(LR_minusGLM_matrix_cc) | is.infinite(LR_minusGLM_matrix_cc), NA, LR_minusGLM_matrix_cc)

# Crear un dataframe con todas las métricas
metricsGLM_matrix_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLM_matrix_cc = c(accuracyGLM_matrix_cc, kappaGLM_matrix_cc, sensitivityGLM_matrix_cc, specificityGLM_matrix_cc, precisionGLM_matrix_cc, 
            f1_scoreGLM_matrix_cc, npvGLM_matrix_cc, prevalenceGLM_matrix_cc, detection_rateGLM_matrix_cc, 
            balanced_accuracyGLM_matrix_cc, LR_plusGLM_matrix_cc, LR_minusGLM_matrix_cc, AUC_GLM_matrix_cc))

# Mostrar los resultados
print(metricsGLM_matrix_cc)

# library(irr)
# 
# # Calcular Kappa
# kappa_GLM<- kappa2(cbind(predGLM,y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLM$value, 3)))
# kappa_GLM<-round(kappa_GLM$value, 3)
```


# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN_matrix_cc<-predict(KNN, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN_matrix_cc)

prediccionesKNN_matrix_cc <- factor(as.character(prediccionesKNN_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesKNN_matrix_cc <- factor(prediccionesKNN_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```


```{r}
# confusion_matrixKNN <- table(prediccionesKNN, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_equipo <- caret::confusionMatrix(prediccionesKNN_matrix_cc, y_test, positive = "Cov.Pos")
```

```{r}



prob_predKNN <- predict(KNN, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNN))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNN[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_MATRIX_cc <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_MATRIX_cc, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```


```{r}
library(caret)
accuracyKNN_MATRIX_cc <- KNN_metrics_equipo$overall["Accuracy"]
kappaKNN_MATRIX_cc <- KNN_metrics_equipo$overall["Kappa"]
# Métricas por clase
sensitivityKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Sensitivity"]
specificityKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Specificity"]
precisionKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Pos Pred Value"]
recallKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["F1"]
npvKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Neg Pred Value"]
prevalenceKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Prevalence"]
detection_rateKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Detection Rate"]
balanced_accuracyKNN_MATRIX_cc <- KNN_metrics_equipo$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusKNN_MATRIX_cc <- sensitivityKNN_MATRIX_cc / (1 - specificityKNN_MATRIX_cc)
LR_minusKNN_MATRIX_cc <- (1 - sensitivityKNN_MATRIX_cc) / specificityKNN_MATRIX_cc

# Para manejar valores especiales
LR_plusKNN_MATRIX_cc <- ifelse(is.nan(LR_plusKNN_MATRIX_cc) | is.infinite(LR_plusKNN_MATRIX_cc), NA, LR_plusKNN_MATRIX_cc)
LR_minusKNN_MATRIX_cc <- ifelse(is.nan(LR_minusKNN_MATRIX_cc) | is.infinite(LR_minusKNN_MATRIX_cc), NA, LR_minusKNN_MATRIX_cc)

# Crear un dataframe con todas las métricas
metricsKNN_MATRIX_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNN_MATRIX_cc = c(accuracyKNN_MATRIX_cc, kappaKNN_MATRIX_cc, sensitivityKNN_MATRIX_cc, specificityKNN_MATRIX_cc, precisionKNN_MATRIX_cc, 
            f1_scoreKNN_MATRIX_cc, npvKNN_MATRIX_cc, prevalenceKNN_MATRIX_cc, detection_rateKNN_MATRIX_cc, 
            balanced_accuracyKNN_MATRIX_cc, LR_plusKNN_MATRIX_cc, LR_minusKNN_MATRIX_cc, AUC_KNN_MATRIX_cc))

# Mostrar los resultados
print(metricsKNN_MATRIX_cc)

# library(irr)
# 
# # Calcular Kappa
# kappa_KNN <- kappa2(cbind(prediccionesKNN, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNN$value, 3)))
# kappa_KNN<-round(kappa_KNN$value, 3)
```



```{r}
Train.rf_matrix  <- as.data.frame(trainData) 

Test.rf_matrix <- as.data.frame(testData)
```

```{r}

Train.rf_matrix  <- as.data.frame(Train.rf_matrix) 
str(Train.rf_matrix)
Test.rf_matrix <- as.data.frame(Test.rf_matrix)


Test.rf_matrix$Y <- factor(Test.rf_matrix$Y, levels = c("Cov.Neg", "Cov.Pos"), labels = c(1, 2))
Train.rf_matrix$Y <- factor(Train.rf_matrix$Y, levels = c("X1", "X2"), labels = c(1, 2))
Test.rf_matrix$Y
Train.rf_matrix$Y



```




```{r}

library(recipes)


#  Definir el objeto recipe
objeto_recipe <- recipe(Y ~ ., data = Train.rf_matrix) %>%
  update_role(Y, new_role = "outcome")  # Especificar que Y es la variable de salid

#  Ajustar el recipe a los datos de entrenamiento
trained_recipe <- prep(objeto_recipe, training = Train.rf_matrix)

#  Aplicar la transformación a los datos de entrenamiento
Train.rf_matrix <- bake(trained_recipe, new_data = Train.rf_matrix)

#  Aplicar la transformación a los datos de prueba
Test.rf_matrix <- bake(trained_recipe, new_data = Test.rf_matrix)

# Verificar que no haya NA en Y después del procesamiento
table(Test.rf_matrix$Y)

```



```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = smatrix(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_ranger <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_matrix$Y <- factor(as.numeric(factor(Train.rf_matrix$Y)))
Train.rf_matrix$Y <- factor(Train.rf_matrix$Y, levels = c("1", "2"))
levels(Train.rf_matrix$Y) <- make.names(levels(Train.rf_matrix$Y))

```


```{r}
class(Train.rf_matrix)
```


```{r}
Train.rf_matrix <- as.data.frame(Train.rf_matrix)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_matrix <- as.data.frame(Train.rf_matrix)

# Convertimos Y a factor
Train.rf_matrix$Y <- as.factor(Train.rf_matrix$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_matrix <- caret::train(Y ~ .,
                data = Train.rf_matrix, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```




```{r}
# Definir los valores de num.trees a probar


num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_matriz <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_matriz[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_matrix,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}

```





```{r}
  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_matriz, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_matriz, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_matriz, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_matriz, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```


```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- resultados_modelos[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```

```{r}
# ranger_matriz <- caret::train(
#   Y ~ .,
#   data = Train.rf_matrix, 
#   method = "ranger",
#   tuneGrid = data.frame(mtry = 2, min.node.size = 1, splitrule = "gini"), 
#   metric = "Accuracy",
#   importance = "impurity",
#   trControl = cross_val,  # Si querés usar validación cruzada
#   num.trees = 1000  # Número de árboles fijo
# )
# print(ranger_matriz)
ranger_matrixcc_fx<-function(df_train, model, grid, metrica,  num.trees){

ranger_matriz <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = grid,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_matriz)
}

#

ranger_matriz<-ranger_matrixcc_fx(Train.rf_matrix,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)
```


```{r}
Test.rf_matrix$Y<-as.factor(Test.rf_matrix$Y)
str(Test.rf_matrix$Y)
```

```{r}
testRF_NOID_matrix <- Test.rf_matrix[, -which(names(Test.rf_matrix) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_matrix_cc<-predict(results_matrix, newdata = testRF_NOID_matrix)#matriz binarizada
print(predRANGER_matrix_cc)

predRANGER_matrix_cc <- factor(as.character(predRANGER_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRANGER_matrix_cc <- factor(predRANGER_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- Test.rf_matrix$Y

prob_predRanger_matrix <- predict(results_matrix, newdata = testRF_NOID_matrix, type = "prob")

colnames(prob_predRanger_matrix)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_matrix$Y)
                 #, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))
y_test <- factor(y_test, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))
```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_matrix[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_matrix_CC <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_matrix_CC, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)


levels(predRANGER_matrix_cc)
levels(y_test)

predRANGER_matrix_cc <- factor(predRANGER_matrix_cc, levels = c("Cov.Neg", "Cov.Pos"))
y_test <- factor(y_test, levels = c("Cov.Neg", "Cov.Pos"))


RANGER_metrics_matrix_cc <- caret::confusionMatrix(predRANGER_matrix_cc, y_test, positive = "Cov.Pos")
# confusion_matrixRANGER <- table(predRANGER_matrix, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRANGER)
```


```{r}
# Precisión (Accuracy)
# Carga los paquetes necesarios
library(caret)
accuracyRANGER_matrix_cc <- RANGER_metrics_matrix_cc$overall["Accuracy"]
kappaRANGER_matrix_cc <- RANGER_metrics_matrix_cc$overall["Kappa"]
# Métricas por clase
sensitivityRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Sensitivity"]
specificityRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Specificity"]
precisionRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Pos Pred Value"]
recallRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["F1"]
npvRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Neg Pred Value"]
prevalenceRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Prevalence"]
detection_rateRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Detection Rate"]
balanced_accuracyRANGER_matrix_cc <- RANGER_metrics_matrix_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRANGER_matrix_cc <- sensitivityRANGER_matrix_cc / (1 - specificityRANGER_matrix_cc)
LR_minusRANGER_matrix_cc <- (1 - sensitivityRANGER_matrix_cc) / specificityRANGER_matrix_cc

# Para manejar valores especiales
LR_plusRANGER_matrix_cc <- ifelse(is.nan(LR_plusRANGER_matrix_cc) | is.infinite(LR_plusRANGER_matrix_cc), NA, LR_plusRANGER_matrix_cc)
LR_minusRANGER_matrix_cc <- ifelse(is.nan(LR_minusRANGER_matrix_cc) | is.infinite(LR_minusRANGER_matrix_cc), NA, LR_minusRANGER_matrix_cc)

# Crear un dataframe con todas las métricas
metricsRANGER_matrix_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RANGER_matrix_cc = c(accuracyRANGER_matrix_cc, kappaRANGER_matrix_cc, sensitivityRANGER_matrix_cc, specificityRANGER_matrix_cc, precisionRANGER_matrix_cc, 
            f1_scoreRANGER_matrix_cc, npvRANGER_matrix_cc, prevalenceRANGER_matrix_cc, detection_rateRANGER_matrix_cc, 
            balanced_accuracyRANGER_matrix_cc, LR_plusRANGER_matrix_cc, LR_minusRANGER_matrix_cc, AUC_Ranger_matrix_CC))

# Mostrar los resultados
print(metricsRANGER_matrix_cc)


```





# Modelos desde matriz dicotomizada sin corregir por batch


```{r}

featureMatrix_bind<-as.data.frame(featureMatrix_bind)
featureMatrix_bind$Y <- as.factor(featureMatrix_bind$Y)
#featureMatrix_bind<- featureMatrix_bind[, c(1:61, 63)]
```


```{r}

featureMatrix_bind<-as.data.frame(featureMatrix_bind)
featureMatrix_bind$Y <- as.factor(featureMatrix_bind$Y)
```

```{r}
featureMatrix_bind_dic_cc<-featureMatrix_bind[1:227,]
ciegoMatrix_dico_cc<-featureMatrix_bind[228:311,]
```

```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_bind_dic_cc, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_matriz_dico <- training(split_data)
testData_matriz_dico <- testing(split_data)

```

```{r}
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
trainData_matriz_dico$Y<-as.factor(trainData_matriz_dico$Y)
str(trainData_matriz_dico)
```

```{r}
train_subset_matriz <- trainData_matriz_dico[, -ncol(trainData_matriz_dico)]
```



```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(train_subset_matriz, trainData_matriz_dico$Y)
train_subset_matriz <- dichotomize(train_subset_matriz, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA

```


```{r}
trainData_matriz_dico<- cbind(train_subset_matriz, trainData_matriz_dico$Y)#1=neg,2=pos
```



```{r}
str(trainData_matriz_dico)
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
```



```{r}
library(dplyr)
trainData_matriz_dico <- trainData_matriz_dico %>% rename(Y = ncol(trainData_matriz_dico))
str(trainData_matriz_dico$Y)
```


```{r}
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
trainData_matriz_dico$Y<-as.factor(trainData_matriz_dico$Y)
str(trainData_matriz_dico)
```



```{r}
zero_var_indices <- caret::nearZeroVar(trainData_matriz_dico[,1:(ncol(trainData_matriz_dico) - 1)])
if (length(zero_var_indices) > 0) {
    trainData_matriz_dico <- trainData_matriz_dico[, -zero_var_indices]
}

```



```{r}

thr_filtered <- thr[colnames(trainData_matriz_dico[1:(ncol(trainData_matriz_dico) - 1)])]
```


quedan 42 variables


```{r}
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)

str(testData_matriz_dico)
```




```{r}
test_subset_matriz <- testData_matriz_dico[, 1:(ncol(testData_matriz_dico) - 1)]
```


```{r}
library(dplyr)

str(testData_matriz_dico$Y)
```



```{r}
test_subset_matriz <- test_subset_matriz[, colnames(trainData_matriz_dico)[1:(ncol(trainData_matriz_dico) - 1)]]
#test_subset <-  test_subset[, colnames(trainData_equipo_dico)]
thr_filtered <-  thr[colnames(trainData_matriz_dico)[1:(ncol(trainData_matriz_dico) - 1)]]
#Dicotimizacion de la matriz de intensidad
test_subset_matriz <- dichotomize(test_subset_matriz, thr_filtered) 

```


```{r}
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico<- cbind(test_subset_matriz, testData_matriz_dico$Y)#1=neg,2=pos
```

```{r}
library(dplyr)
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico <- testData_matriz_dico %>% rename(Y = ncol(testData_matriz_dico))
str(testData_matriz_dico$Y)
```

```{r}
str(testData_matriz_dico)
```




```{r}
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)
str(testData_matriz_dico)
```




```{r}
# Ver la distribución de clases en ambos conjuntos
table(trainData_matriz_dico$Y)
table(testData_matriz_dico$Y)


```




```{r}
ciegoMatrix_dico_cc_subset <- ciegoMatrix_dico_cc[, -ncol(ciegoMatrix_dico_cc)]

ciegoMatrix_dico_cc_subset <-ciegoMatrix_dico_cc_subset[, colnames(trainData_equipo_dico)[1:ncol(trainData_equipo_dico)-1]]

ciegoMatrix_dico_cc_subset <- dichotomize(ciegoMatrix_dico_cc_subset, thr_filtered) 

library(dplyr)
ciegoMatrix_dico_cc <- ciegoMatrix_dico_cc %>% rename(Y = ncol(ciegoMatrix_dico_cc))
str(ciegoMatrix_dico_cc$Y)

ciegoMatrix_dico_cc<- cbind(ciego_combat_corrected_equipo_cc_dic_subset, ciegoMatrix_dico_cc$Y)

ciegoMatrix_dico_cc<-as.data.frame(ciegoMatrix_dico_cc)
```

```{r}
library(caret)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



# Random forest


```{r}
grid_rf <- expand.grid(mtry = c(2, 5, 10))

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(grid_rf)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# # Training control
# 
# cross_val <- trainControl(
#   method = "repeatedcv",
#   number = particiones,
#   repeats = repeticiones,
#   returnResamp = "final",
#   verboseIter = FALSE,
#   allowParallel = TRUE,
#   classProbs = TRUE,
#   seeds = seeds)
```

```{r}
trainData_matriz_dico$Y <- factor(trainData_matriz_dico$Y, labels = make.names(levels(trainData_matriz_dico$Y)))
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.
set.seed(42)
# RFd <- caret::train(Y ~ ., data = trainData_matriz_dico, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación
RF_matrix_dico_cc_fx <- function(df_train, model, grid, metrica = "Accuracy",  control) {

  
  # Entrenar el modelo
  RFd <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(RFd)
  plot(RFd)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RFd)
}

RFd<-RF_matrix_dico_cc_fx(df_train=trainData_matriz_dico, model="rf", grid=grid_rf, metrica="Accuracy" , control=cross_val_rf)
print(RFd)
```





```{r}
plot(RFd)
```


# KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
KNN_matrix_cc_dico_fx <- function(df_train, model, grid, metrica = "Accuracy", control, preProcess) {

  
  # Entrenar el modelo
  KNNd <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNNd)
  plot(KNNd)
  
  # Guardar resultado
  #save(KNN_matrix_dico, file = paste0(title, ".rda"))
  
  return(KNNd)
}

KNNd<-KNN_matrix_cc_dico_fx(df_train=trainData_matriz_dico, model="knn", grid=tuneGrid, metrica="Accuracy", control=cross_val_knn, preProcess = c("center","scale") )
print(KNNd)
```



```{r}
# # Training control
# 
# cross_val <- trainControl(
#   method = "repeatedcv",
#   number = particiones,
#   repeats = repeticiones,
#   returnResamp = "final",
#   verboseIter = FALSE,
#   allowParallel = TRUE,
#   classProbs = TRUE,
#   seeds = seeds)
```

```{r}
# #tuneGrid <- expand.grid(k = 1:15)
# 
# # Ajustar el modelo KNN
# KNNd <- caret::train(formula, 
#                     data = trainData_matriz_dico, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNNd
```

```{r}
plot(KNNd)
```

# GLMNET


```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)
max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# # Training control
# 
# cross_val <- trainControl(
#   method = "repeatedcv",
#   number = particiones,
#   repeats = repeticiones,
#   returnResamp = "final",
#   verboseIter = FALSE,
#   allowParallel = TRUE,
#   classProbs = TRUE,
#   seeds = seeds)
```


```{r}

#tuneGrid <- expand.grid(
#  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
#  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
#)
#set.seed(42)
# GLMd <- caret::train(formula, 
#                   data = trainData_matriz_dico,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLMd 
GLM_cc_dico_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  GLMd <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(GLMd)
  plot(GLMd)
  
  # Guardar resultado
  #save(GLM_equipo, file = paste0(title, ".rda"))
  
  return(GLMd)
}

GLMd<-GLM_cc_dico_fx(df_train=trainData_matriz_dico, model="glmnet", grid=tuneGrid, metrica="Accuracy", control=cross_val_glm )
print(GLMd)
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLMd$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLMd$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLMd$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# Prediccion

```{r}
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)
str(testData_matriz_dico$Y)
```

```{r}
testData_NOID_matriz <- testData_matriz_dico[, -which(names(testData_matriz_dico) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRFd_matrix_cc<-predict(RFd, newdata = testData_NOID_matriz)#matriz binarizada
print(prediccionesRFd_matrix_cc)
prediccionesRFd_matrix_cc <- factor(as.character(prediccionesRFd_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesRFd_matrix_cc <- factor(prediccionesRFd_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))



# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_matriz_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}

RF_metrics_matriz_dico_cc <- caret::confusionMatrix(prediccionesRFd_matrix_cc, y_test, positive = "Cov.Pos")
RF_metrics_matriz_dico_cc$overall
RF_metrics_matriz_dico_cc$byClass
```





```{r}


library(pROC)
prob_predRFd <- predict(RFd, newdata = testData_NOID, type = "prob")

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRFd[, "X2"], levels = c(1, 2))
AUC_RFd_matrix_cc <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RFd_matrix_cc, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

```{r}
library(caret)
accuracyRFd_matrix_cc <- RF_metrics_matriz_dico_cc$overall["Accuracy"]
kappaRFd_matrix_cc <- RF_metrics_matriz_dico_cc$overall["Kappa"]
# Métricas por clase
sensitivityRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Sensitivity"]
specificityRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Specificity"]
precisionRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Pos Pred Value"]
recallRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["F1"]
npvRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Neg Pred Value"]
prevalenceRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Prevalence"]
detection_rateRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Detection Rate"]
balanced_accuracyRFd_matrix_cc <- RF_metrics_matriz_dico_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRFd_matrix_cc <- sensitivityRFd_matrix_cc / (1 - specificityRFd_matrix_cc)
LR_minusRFd_matrix_cc <- (1 - sensitivityRFd_matrix_cc) / specificityRFd_matrix_cc

# Para manejar valores especiales
LR_plusRFd_matrix_cc <- ifelse(is.nan(LR_plusRFd_matrix_cc) | is.infinite(LR_plusRFd_matrix_cc), NA, LR_plusRFd_matrix_cc)
LR_minusRFd_matrix_cc <- ifelse(is.nan(LR_minusRFd_matrix_cc) | is.infinite(LR_minusRFd_matrix_cc), NA, LR_minusRFd_matrix_cc)

# Crear un dataframe con todas las métricas
metricsRFd_matrix_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RFd_matrix_cc = c(accuracyRFd_matrix_cc, kappaRFd_matrix_cc, sensitivityRFd_matrix_cc, specificityRFd_matrix_cc, precisionRFd_matrix_cc, 
            f1_scoreRFd_matrix_cc, npvRFd_matrix_cc, prevalenceRFd_matrix_cc, detection_rateRFd_matrix_cc, 
            balanced_accuracyRFd_matrix_cc, LR_plusRFd_matrix_cc, LR_minusRFd_matrix_cc, AUC_RFd_matrix_cc))

# Mostrar los resultados
print(metricsRFd_matrix_cc)

# library(irr)
# 
# # Calcular Kappa
# kappa_RFd_dic <- kappa2(cbind(prediccionesRFd, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RFd_dic$value, 3)))
# kappa_RFd_dic<-round(kappa_RFd_dic$value, 3)
```


# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNNd_matrix_cc<-predict(KNNd, newdata = testData_matriz_dico)#matriz binarizada
print(prediccionesKNNd_matrix_cc)

prediccionesKNNd_matrix_cc <- factor(as.character(prediccionesKNNd_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesKNNd_matrix_cc <- factor(prediccionesKNNd_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_matriz_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixKNN <- table(prediccionesKNNd, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_fecha_matrix_cc <- caret::confusionMatrix(prediccionesKNNd_matrix_cc, y_test, positive = "Cov.Pos")
```

```{r}

prob_predKNNd <- predict(KNNd, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNNd))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNd[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNNd_matrix_cc <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNNd_matrix_cc, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```


```{r}

library(caret)
accuracyKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$overall["Accuracy"]
kappaKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$overall["Kappa"]
# Métricas por clase
sensitivityKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Sensitivity"]
specificityKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Specificity"]
precisionKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Pos Pred Value"]
recallKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["F1"]
npvKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Neg Pred Value"]
prevalenceKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Prevalence"]
detection_rateKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Detection Rate"]
balanced_accuracyKNNd_matrix_cc <- KNN_metrics_fecha_matrix_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusKNNd_matrix_cc <- sensitivityKNNd_matrix_cc / (1 - specificityKNNd_matrix_cc)
LR_minusKNNd_matrix_cc <- (1 - sensitivityKNNd_matrix_cc) / specificityKNNd_matrix_cc

# Para manejar valores especiales
LR_plusKNNd_matrix_cc <- ifelse(is.nan(LR_plusKNNd_matrix_cc) | is.infinite(LR_plusKNNd_matrix_cc), NA, LR_plusKNNd_matrix_cc)
LR_minusKNNd_matrix_cc <- ifelse(is.nan(LR_minusKNNd_matrix_cc) | is.infinite(LR_minusKNNd_matrix_cc), NA, LR_minusKNNd_matrix_cc)

# Crear un dataframe con todas las métricas
metricsKNNd_matrix_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNNd_matrix_cc = c(accuracyKNNd_matrix_cc, kappaKNNd_matrix_cc, sensitivityKNNd_matrix_cc, specificityKNNd_matrix_cc, precisionKNNd_matrix_cc, 
            f1_scoreKNNd_matrix_cc, npvKNNd_matrix_cc, prevalenceKNNd_matrix_cc, detection_rateKNNd_matrix_cc, 
            balanced_accuracyKNNd_matrix_cc, LR_plusKNNd_matrix_cc, LR_minusKNNd_matrix_cc, AUC_KNNd_matrix_cc))

# Mostrar los resultados
print(metricsKNNd_matrix_cc)

```


# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLMd_matrix_cc<-predict(GLMd, newdata = testData_matriz_dico)#matriz binarizada
print(predGLMd_matrix_cc)

predGLMd_matrix_cc <- factor(as.character(predGLMd_matrix_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLMd_matrix_cc <- factor(predGLMd_matrix_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_matriz_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixGLMd <- table(predGLMd, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLMd)
GLMd_metrics_matrix_cc <- caret::confusionMatrix(predGLMd_matrix_cc, y_test, positive = "Cov.Pos")
```


```{r}


# Predicciones probabilísticas con el modelo GLM
prob_predGLMd <- predict(GLMd, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLMd))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLMd[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLMd_matrix_cc <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLMd_matrix_cc, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```


```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_GLMd <- kappa2(cbind(predGLMd, testData_fecha_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLMd$value, 3)))
# kappa_GLMd<-round(kappa_GLMd$value, 3)
library(caret)
accuracyGLMd_matrix_cc <- GLMd_metrics_matrix_cc$overall["Accuracy"]
kappaGLMd_matrix_cc <- GLMd_metrics_matrix_cc$overall["Kappa"]
# Métricas por clase
sensitivityGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Sensitivity"]
specificityGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Specificity"]
precisionGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Pos Pred Value"]
recallGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["F1"]
npvGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Neg Pred Value"]
prevalenceGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Prevalence"]
detection_rateGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Detection Rate"]
balanced_accuracyGLMd_matrix_cc <- GLMd_metrics_matrix_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusGLMd_matrix_cc <- sensitivityGLMd_matrix_cc / (1 - specificityGLMd_matrix_cc)
LR_minusGLMd_matrix_cc <- (1 - sensitivityGLMd_matrix_cc) / specificityGLMd_matrix_cc

# Para manejar valores especiales
LR_plusGLMd_matrix_cc <- ifelse(is.nan(LR_plusGLMd_matrix_cc) | is.infinite(LR_plusGLMd_matrix_cc), NA, LR_plusGLMd_matrix_cc)
LR_minusGLMd_matrix_cc <- ifelse(is.nan(LR_minusGLMd_matrix_cc) | is.infinite(LR_minusGLMd_matrix_cc), NA, LR_minusGLMd_matrix_cc)

# Crear un dataframe con todas las métricas
metricsGLMd_matrix_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLMd_matrix_cc = c(accuracyGLMd_matrix_cc, kappaGLMd_matrix_cc, sensitivityGLMd_matrix_cc, specificityGLMd_matrix_cc, precisionGLMd_matrix_cc, 
            f1_scoreGLMd_matrix_cc, npvGLMd_matrix_cc, prevalenceGLMd_matrix_cc, detection_rateGLMd_matrix_cc, 
            balanced_accuracyGLMd_matrix_cc, LR_plusGLMd_matrix_cc, LR_minusGLMd_matrix_cc, AUC_GLMd_matrix_cc))

# Mostrar los resultados
print(metricsGLMd_matrix_cc)

```

#ranger


```{r}
library(keras)
library(tensorflow)
library(reticulate)
library(caret)
```


```{r}
Train.rf_matrix_dico  <- as.data.frame(trainData_matriz_dico) 

Test.rf_matrix_dico <- as.data.frame(testData_matriz_dico)
```

```{r}
# objeto_recipe <- recipe(formula = Y ~ .,
#                         data =  Train.rf_matrix_dico)
# 
# 
# trained_recipe <- prep(objeto_recipe, training = Train.rf_matrix_dico)
# 
# Train.rf_matrix_dico <- bake(trained_recipe, new_data = Train.rf_matrix_dico)
# Test.rf_matrix_dico  <- bake(trained_recipe, new_data = Test.rf_matrix_dico)
```


```{r}
Train.rf_matrix_dico  <- as.data.frame(Train.rf_matrix_dico) 
str(Train.rf_matrix_dico)
Test.rf_matrix_dico <- as.data.frame(Test.rf_matrix_dico)
Test.rf_matrix_dico$Y
Train.rf_matrix_dico$Y

Test.rf_matrix_dico$Y <- factor(Test.rf_matrix_dico$Y, levels = c(1, 2), labels = c("X1", "X2"))

Test.rf_matrix_dico$Y
```

```{r}

library(recipes)


#  Definir el objeto recipe
objeto_recipe <- recipe(Y ~ ., data = Train.rf_matrix_dico) %>%
  update_role(Y, new_role = "outcome")  # Especificar que Y es la variable de salid

#  Ajustar el recipe a los datos de entrenamiento
trained_recipe <- prep(objeto_recipe, training = Train.rf_matrix_dico)

#  Aplicar la transformación a los datos de entrenamiento
Train.rf_matrix_dico <- bake(trained_recipe, new_data = Train.rf_matrix_dico)

#  Aplicar la transformación a los datos de prueba
Test.rf_matrix_dico <- bake(trained_recipe, new_data = Test.rf_matrix_dico)

# Verificar que no haya NA en Y después del procesamiento
table(Test.rf_matrix_dico$Y)

```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = smatrix(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control
set.seed(42)
# cross_val <- trainControl(
#   method = "repeatedcv",
#   number = particiones,
#   repeats = repeticiones,
#   returnResamp = "final",
#   verboseIter = FALSE,
#   allowParallel = TRUE,
#   classProbs = TRUE,
#   seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_matrix_dico$Y <- factor(as.numeric(factor(Train.rf_matrix_dico$Y)))
Train.rf_matrix_dico$Y <- factor(Train.rf_matrix_dico$Y, levels = c("1", "2"))
levels(Train.rf_matrix_dico$Y) <- make.names(levels(Train.rf_matrix_dico$Y))

```





```{r}
class(Train.rf_matrix_dico)
```


```{r}
Train.rf_matrix_dico <- as.data.frame(Train.rf_matrix_dico)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_matrix_dico <- as.data.frame(Train.rf_matrix_dico)

# Convertimos Y a factor
Train.rf_matrix_dico$Y <- as.factor(Train.rf_matrix_dico$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_matrix_dico <- caret::train(Y ~ .,
                data = Train.rf_matrix_dico, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  # Aquí estaba el error, había texto adicional

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```




```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_matrix_dico <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_matrix_dico[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_matrix_dico,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}

```


```{r}
lengths_list <- list(
  num_trees = length(num_trees_range),
  Accuracy = length(sapply(modelos_matrix_dico, function(m) max(m$results$Accuracy, na.rm = TRUE))),
  Kappa = length(sapply(modelos_matrix_dico, function(m) max(m$results$Kappa, na.rm = TRUE))),
  mtry = length(sapply(modelos_matrix_dico, function(m) m$bestTune$mtry)),
  min.node.size = length(sapply(modelos_matrix_dico, function(m) m$bestTune$min.node.size))
)

print(lengths_list)

  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_matrix_dico, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_matrix_dico, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_matrix_dico, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_matrix_dico, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```




```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_matrix_dico[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```


```{r}
# ranger_matriz_dico <- caret::train(
#   Y ~ .,
#   data = Train.rf_matrix_dico, 
#   method = "ranger",
#   tuneGrid = data.frame(mtry = 3, min.node.size = 1, splitrule = "gini"), 
#   metric = "Accuracy",
#   importance = "impurity",
#   trControl = cross_val,  # Si querés usar validación cruzada
#   num.trees = 1500  # Número de árboles fijo
# )
# print(ranger_matriz_dico)
ranger_matrix_cc_dico_fx<-function(df_train, model, grid, metrica, num.trees){

ranger_matriz_dico <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = grid,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_matriz_dico)
}



ranger_matriz_dico<-ranger_matrix_cc_dico_fx(Train.rf_matrix_dico,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)
```


```{r}
Test.rf_matrix_dico$Y<-as.factor(Test.rf_matrix_dico$Y)
str(Test.rf_matrix_dico$Y)
```

```{r}
testRF_NOID_matrix_dico <- Test.rf_matrix_dico[, -which(names(Test.rf_matrix_dico) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_matrix_dico_cc<-predict(ranger_matriz_dico, newdata = testRF_NOID_matrix_dico)#matriz binarizada
print(predRANGER_matrix_dico_cc)

predRANGER_matrix_dico_cc <- factor(as.character(predRANGER_matrix_dico_cc), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRANGER_matrix_dico_cc <- factor(predRANGER_matrix_dico_cc, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))

# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- Test.rf_matrix_dico$Y

prob_predRanger <- predict(ranger_matriz_dico, newdata = testRF_NOID_matrix_dico, type = "prob")

colnames(prob_predRanger)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_matrix_dico$Y, levels = c("X1", "X2"), labels = c("Cov.Neg", "Cov.Pos"))

#Test.rf_matrix_dico$Y<-as.numeric(Test.rf_matrix_dico$Y)

```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(Test.rf_matrix_dico$Y)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_matrix_dich_cc <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_matrix_dich_cc, 3)))

# Visualiza la curva RO
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

# confusion_matrixRANGER <- table(predRANGER_matrix_dico, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRANGER)
RANGER_metrics_dic_fecha_cc <- caret::confusionMatrix(predRANGER_matrix_dico_cc, y_test, positive = "Cov.Pos")
```

```{r}
library(caret)
accuracyRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$overall["Accuracy"]
kappaRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$overall["Kappa"]
# Métricas por clase
sensitivityRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Sensitivity"]
specificityRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Specificity"]
precisionRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Pos Pred Value"]
recallRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["F1"]
npvRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Neg Pred Value"]
prevalenceRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Prevalence"]
detection_rateRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Detection Rate"]
balanced_accuracyRanger_matrix_dich_cc <- RANGER_metrics_dic_fecha_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRanger_matrix_dich_cc <- sensitivityRanger_matrix_dich_cc / (1 - specificityRanger_matrix_dich_cc)
LR_minusRanger_matrix_dich_cc <- (1 - sensitivityRanger_matrix_dich_cc) / specificityRanger_matrix_dich_cc

# Para manejar valores especiales
LR_plusRanger_matrix_dich_cc <- ifelse(is.nan(LR_plusRanger_matrix_dich_cc) | is.infinite(LR_plusRanger_matrix_dich_cc), NA, LR_plusRanger_matrix_dich_cc)
LR_minusRanger_matrix_dich_cc <- ifelse(is.nan(LR_minusRanger_matrix_dich_cc) | is.infinite(LR_minusRanger_matrix_dich_cc), NA, LR_minusRanger_matrix_dich_cc)

# Crear un dataframe con todas las métricas
metricsRanger_matrix_dich_cc <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  Ranger_matrix_dich_cc  = c(accuracyRanger_matrix_dich_cc, kappaRanger_matrix_dich_cc, sensitivityRanger_matrix_dich_cc, specificityRanger_matrix_dich_cc, precisionRanger_matrix_dich_cc, 
            f1_scoreRanger_matrix_dich_cc, npvRanger_matrix_dich_cc, prevalenceRanger_matrix_dich_cc, detection_rateRanger_matrix_dich_cc, 
            balanced_accuracyRanger_matrix_dich_cc, LR_plusRanger_matrix_dich_cc, LR_minusRanger_matrix_dich_cc, AUC_Ranger_matrix_dich_cc))

# Mostrar los resultados
print(metricsRanger_matrix_dich_cc)


```




```{r}
ciego_data <- as.data.frame(ciegoMatrix_dico_cc)

ciego_data <- ciego_data %>% rename(Y = ncol(ciego_data))

ciego_data$Y <- factor(
  ciego_data$Y,
  levels = c(1, 2),
  labels = c("Cov.Neg", "Cov.Pos"))

# Si viene con columna Y (conocida), separala:
if ("Y" %in% colnames(ciego_data)) {
  Y_ciego <- ciego_data$Y
  X_ciego <- ciego_data[, setdiff(colnames(ciego_data), "Y")]
} else {
  X_ciego <- ciego_data
  Y_ciego <- NULL
}

```



```{r}

predicciones_ciego_matrix_cc <- predict(ranger_matriz_dico, newdata = X_ciego)
levels(predicciones_ciego_matrix_cc) <- c("Cov.Neg", "Cov.Pos")

# Predicciones (probabilidades)
prob_ciego <- predict(ranger_matriz_dico, newdata = X_ciego, type = "prob")
#prob_ciego$Cov.Neg<-prob_ciego$X1
#prob_ciego$Cov.Pos<-prob_ciego$X2


levels(predicciones_ciego_matrix_cc) <- c("Cov.Neg", "Cov.Pos")
levels(Y_ciego) <- c("Cov.Neg", "Cov.Pos")



  # Confusion matrix
  ciego_ranger_matrix_cc<-caret::confusionMatrix(predicciones_ciego_matrix_cc, Y_ciego, positive = "Cov.Pos")

  # AUC
  library(pROC)
  Y_ciego_num <- as.numeric(Y_ciego)
  roc_ciego <- roc(Y_ciego_num, prob_ciego[, "X2"])
  plot(roc_ciego, main = "ROC en Datos Ciegos", col = "darkgreen", lwd = 2)
  AUC_Ranger_matrix_dich_cc_ciego<-auc(roc_ciego)
  
  
  
  
  library(caret)
accuracyRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$overall["Accuracy"]
kappaRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$overall["Kappa"]
# Métricas por clase
sensitivityRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Sensitivity"]
specificityRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Specificity"]
precisionRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Pos Pred Value"]
recallRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["F1"]
npvRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Neg Pred Value"]
prevalenceRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Prevalence"]
detection_rateRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Detection Rate"]
balanced_accuracyRanger_matrix_dich_cc_ciego <- ciego_ranger_matrix_cc$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRanger_matrix_dich_cc_ciego <- sensitivityRanger_matrix_dich_cc_ciego / (1 - specificityRanger_matrix_dich_cc_ciego)
LR_minusRanger_matrix_dich_cc_ciego <- (1 - sensitivityRanger_matrix_dich_cc_ciego) / specificityRanger_matrix_dich_cc_ciego

# Para manejar valores especiales
LR_plusRanger_matrix_dich_cc_ciego <- ifelse(is.nan(LR_plusRanger_matrix_dich_cc_ciego) | is.infinite(LR_plusRanger_matrix_dich_cc_ciego), NA, LR_plusRanger_matrix_dich_cc_ciego)
LR_minusRanger_matrix_dich_cc_ciego <- ifelse(is.nan(LR_minusRanger_matrix_dich_cc_ciego) | is.infinite(LR_minusRanger_matrix_dich_cc_ciego), NA, LR_minusRanger_matrix_dich_cc_ciego)

# Crear un dataframe con todas las métricas
metricsRanger_matrix_dich_cc_ciego <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  Ranger_matrix_dich_cc  = c(accuracyRanger_matrix_dich_cc_ciego, kappaRanger_matrix_dich_cc_ciego, sensitivityRanger_matrix_dich_cc_ciego, specificityRanger_matrix_dich_cc_ciego, precisionRanger_matrix_dich_cc_ciego, 
            f1_scoreRanger_matrix_dich_cc_ciego, npvRanger_matrix_dich_cc_ciego, prevalenceRanger_matrix_dich_cc_ciego, detection_rateRanger_matrix_dich_cc_ciego, 
            balanced_accuracyRanger_matrix_dich_cc_ciego, LR_plusRanger_matrix_dich_cc_ciego, LR_minusRanger_matrix_dich_cc_ciego, AUC_Ranger_matrix_dich_cc_ciego))

# Mostrar los resultados
print(metricsRanger_matrix_dich_cc_ciego)

```




