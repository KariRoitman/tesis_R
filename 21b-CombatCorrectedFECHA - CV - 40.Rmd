---
title: "fecha"
author: "Karina Roitman"
date: "2025-01-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```


# Modelos (con datos combat_corrected POR FECHA dicotomizado)


```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_fecha_tot, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_fecha <- training(split_data)
testData_fecha <- testing(split_data)

```

```{r}
trainData_fecha<-as.data.frame(trainData_fecha)
trainData_fecha$Y<-as.factor(trainData_fecha$Y)
str(trainData_fecha)
```

```{r}
train_subset_fecha <- trainData_fecha[, -ncol(trainData_fecha)]
```



```{r}
labels <- trainData_fecha$Y

#Definir número de bootstraps
n_boot <- 10
set.seed(123)

#Inicializar lista para guardar umbrales
thresholds_list <- vector("list", n_boot)

#  Ejecutar bootstrap
for (i in 1:n_boot) {
  sample_indices <- sample(1:nrow(train_subset_fecha), replace = TRUE)
  sampled_data <- train_subset_fecha[sample_indices, ]
  sampled_labels <- labels[sample_indices]

  thresholds <- optimizeThreshold(sampled_data, sampled_labels)
  thresholds_list[[i]] <- thresholds
}

# Unir todos los umbrales en un data frame
thresholds_df <- do.call(rbind, thresholds_list)

# Calcular el CV de cada pico
cv_thresholds <- apply(thresholds_df, 2, function(x) {
  media <- mean(x, na.rm = TRUE)
  sd <- sd(x, na.rm = TRUE)
  if (media == 0) return(Inf) else return((sd / media) * 100)
})

#Seleccionar los picos con CV < 40 (estables)
picos_estables <- names(cv_thresholds[cv_thresholds < 40])

#  Filtrar la matriz original para quedarte solo con esos picos
train_subset_fecha <- train_subset_fecha[, picos_estables]

#  Volver a combinar con la variable Y
#train_subset_fecha$Y <- labels
```


```{r}
# Filtrar el df de thresholds para los picos estables
thresholds_estables <- thresholds_df[, picos_estables, drop = FALSE]

# Calcular la media por columna (pico)
thr <- colMeans(thresholds_estables, na.rm = TRUE)

# Mostrar
print(thr)

```


```{r}
library(dplyr)
#trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = label)
#trainData_fecha_dico<-trainData_fecha_dico[,-ncol(trainData_fecha_dico)]
str(trainData_fecha$Y)
```



```{r}
trainData_fecha<- cbind(train_subset_fecha, trainData_fecha$Y)#1=neg,2=pos
```



```{r}
str(trainData_fecha)
trainData_fecha<-as.data.frame(trainData_fecha)
```





```{r}
library(dplyr)
trainData_fecha <- trainData_fecha %>% rename(Y = ncol(trainData_fecha))
str(trainData_fecha$Y)
```


```{r}
trainData_fecha<-as.data.frame(trainData_fecha)
trainData_fecha$Y<-as.factor(trainData_fecha$Y)
str(trainData_fecha)
```

```{r}
trainData_fecha <- cbind(train_subset_fecha, trainData_fecha$Y)
```


```{r}
testData_fecha<-as.data.frame(testData_fecha)
testData_fecha$Y<-as.factor(testData_fecha$Y)

str(testData_fecha)
```



```{r}
library(dplyr)
#testData_fecha_dico <- testData_fecha_dico %>% rename(Y = label)
str(testData_fecha$Y)
```


```{r}
test_subset_fecha <- testData_fecha[, -ncol(testData_fecha)]
```



```{r}
test_subset_fecha <- test_subset_fecha[, colnames(trainData_fecha[,-ncol(trainData_fecha)])]

```


```{r}
testData_fecha<- cbind(test_subset_fecha,testData_fecha$Y)#1=neg,2=pos<- cbind(test_subset_fecha, testData_fecha_dico$Y)#1=neg,2=pos
#testData_fecha_dico<- cbind(test_subset_fecha, testData_fecha_dico$Y)#1=neg,2=pos
```

```{r}
trainData_fecha<-as.data.frame(trainData_fecha)
trainData_fecha <- trainData_fecha %>% rename(Y = ncol(trainData_fecha))
str(trainData_fecha$Y)
library(dplyr)
testData_fecha<-as.data.frame(testData_fecha)
testData_fecha <- testData_fecha %>% rename(Y = ncol(testData_fecha))
str(testData_fecha$Y)


# library(dplyr)
# testData_fecha_dico<-as.data.frame(testData_fecha_dico)
# testData_fecha_dico <- testData_fecha_dico %>% rename(Y = ncol(testData_fecha_dico))
# str(testData_fecha_dico$Y)
```


```{r}
testData_fecha<-as.data.frame(testData_fecha)
testData_fecha$Y<-as.factor(testData_fecha$Y)
str(testData_fecha)
```




```{r}
trainData_fecha<-as.data.frame(trainData_fecha)
trainData_fecha$Y<-as.factor(trainData_fecha$Y)
testData_fecha<-as.data.frame(testData_fecha)
# Ver la distribución de clases en ambos conjuntos
table(trainData_fecha$Y)
table(testData_fecha$Y)


```






# Modelos (con datos combat_corrected pero sin dictomizar)





```{r}
# Submuestras y repeticiones
set.seed(42)

particiones <- 3
repeticiones <- 5
```



```{r}
#set.seed(42)
library(caret)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



# Random forest


```{r}
hiperparametros <- expand.grid(mtry = c(1, 2, 3))

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```



```{r}
trainData_fecha$Y <- factor(trainData_fecha$Y)  # Asegura que Y sea un factor
levels(trainData_fecha$Y) <- make.names(levels(trainData_fecha$Y))  # Corrige los nombres
library(caret)

# RF_fecha <- caret::train(Y ~ ., data = trainData_fecha, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación
RF_fecha_tot_fx <- function(df_train, model, grid, metrica = "Accuracy",  control) {

  # Entrenar el modelo
  RF_fecha_tot <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(RF_fecha_tot)
  plot(RF_fecha_tot, main = title)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF_fecha_tot)
}

RF_fecha_tot<-RF_fecha_tot_fx(df_train=trainData_fecha, model="rf", grid=hiperparametros, metrica="Accuracy", control=cross_val_rf)
print(RF_fecha_tot)
```

```{r}
RF_fecha_tot
```

```{r}
plot(RF_fecha_tot)
```


<!-- # ranger -->


<!-- ```{r} -->
<!-- Train.rf_FECHA  <- as.data.frame(trainData_fecha)  -->

<!-- Test.rf_FECHA <- as.data.frame(testData_fecha) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- library(keras) -->
<!-- library(tensorflow) -->
<!-- library(reticulate) -->
<!-- library(caret) -->
<!-- ``` -->


```{r}

# objeto_recipe <- recipe(formula = Y ~ .,
#                         data =  Train.rf_FECHA)
# 
# objeto_recipe <- objeto_recipe %>% 
#   step_nzv(all_predictors())
# 
# trained_recipe <- prep(objeto_recipe, training = Train.rf_FECHA)
# 
# Train.rf_FECHA <- bake(trained_recipe, new_data = Train.rf_FECHA)
# Test.rf_FECHA  <- bake(trained_recipe, new_data = Test.rf_FECHA)
```

 ```{r}
# # Submuestras y repeticiones
# 
# # particiones  <- 5
# # repeticiones <- 15
# particiones <- 3
# repeticiones <- 5
# ```
# 
# ```{r}
# # Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
# seed.rf <- 42
# set.seed(seed.rf) 
# 
# 
# mtry <- c(1, 2, 3)
# min.node.size <- seq(1, 10, 2)  # Reducir el rango
# hiperparametros <- expand.grid(mtry =  mtry,
#                                #min.node.size = sfecha(1, 30, 2),
#                                min.node.size=min.node.size,
#                                splitrule = "gini")
# 
# 
# seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
# 
# for (i in 1:(particiones * repeticiones)) {
#   seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
# }
# 
# seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)
# 
# # Training control
# ```
# 
# 
# 
# ```{r}
# # Training control
# 
# cross_val_ranger <- trainControl(
#   method = "repeatedcv",
#   number = particiones,
#   repeats = repeticiones,
#   returnResamp = "final",
#   verboseIter = FALSE,
#   allowParallel = TRUE,
#   classProbs = TRUE,
#   seeds = seeds)
# 
# # Training 
# 
# # Convertir los niveles de Train.rf$sensi a números también
# 
# Train.rf_FECHA$Y <- factor(as.numeric(factor(Train.rf_FECHA$Y)))
# Train.rf_FECHA$Y <- factor(Train.rf_FECHA$Y, levels = c("1", "2"))
# levels(Train.rf_FECHA$Y) <- make.names(levels(Train.rf_FECHA$Y))

```





<!-- ```{r} -->
<!-- class(Train.rf_FECHA) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- Train.rf_FECHA <- as.data.frame(Train.rf_FECHA) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # Primero aseguramos que Train.rf es un dataframe -->
<!-- Train.rf_FECHA <- as.data.frame(Train.rf_FECHA) -->

<!-- # Convertimos Y a factor -->
<!-- Train.rf_FECHA$Y <- as.factor(Train.rf_FECHA$Y) -->

<!-- # Definimos número de árboles -->
<!-- #n_trees <- 500 # default -->
<!-- n_trees<-200 -->
<!-- # Establecemos semilla para reproducibilidad -->
<!-- set.seed(80) -->

<!-- # Ejecutamos el entrenamiento -->
<!-- results_FECHA <- caret::train(Y ~ ., -->
<!--                 data = Train.rf_FECHA,  -->
<!--                 method = "ranger", -->
<!--                 tuneGrid = hiperparametros, -->
<!--                 metric = "Accuracy", -->
<!--                 importance = "impurity", -->
<!--                 trControl = cross_val_ranger, -->
<!--                 num.trees = n_trees) -->
<!--                #allowParallel=FALSE)   -->

<!-- # Vector para probar diferentes números de árboles -->
<!-- #num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # Definir los valores de num.trees a probar -->
<!-- num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500) -->

<!-- # Lista para almacenar los modelos -->
<!-- modelos_fecha <- list() -->

<!-- # Establecer semilla para reproducibilidad -->
<!-- set.seed(80) -->

<!-- # Iterar sobre cada cantidad de árboles -->
<!-- for (nt in num_trees_range) { -->
<!--   cat("Entrenando modelo con", nt, "árboles...\n") -->

<!--   modelos_fecha[[as.character(nt)]] <- caret::train( -->
<!--     Y ~ ., -->
<!--     data = Train.rf_FECHA, -->
<!--     method = "ranger", -->
<!--     tuneGrid = hiperparametros, -->
<!--     metric = "Accuracy", -->
<!--     importance = "impurity", -->
<!--     trControl = cross_val_ranger, -->
<!--     num.trees = nt -->
<!--   ) -->
<!-- } -->


<!-- ``` -->





<!-- ```{r} -->
<!--   resultados_modelos <- data.frame( -->
<!--   num_trees = num_trees_range, -->
<!--   Accuracy = sapply(modelos_fecha, function(m) max(m$results$Accuracy, na.rm = TRUE)), -->
<!--   Kappa = sapply(modelos_fecha, function(m) max(m$results$Kappa, na.rm = TRUE)), -->
<!--   mtry = sapply(modelos_fecha, function(m) m$bestTune$mtry), -->
<!--   min.node.size = sapply(modelos_fecha, function(m) m$bestTune$min.node.size) -->
<!-- ) -->

<!-- # Ordenamos los modelos de mejor a peor Accuracy -->
<!-- resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ] -->

<!-- print(head(resultados_modelos, 20))  -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados -->
<!-- mejor_idx <- which.max(resultados_modelos$Accuracy) -->

<!-- # Extraer la mejor combinación de hiperparámetros -->
<!-- mejor_num_trees <- resultados_modelos$num_trees[mejor_idx] -->
<!-- mejor_mtry <- resultados_modelos$mtry[mejor_idx] -->
<!-- mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx] -->

<!-- # Extraer el modelo correspondiente en la lista -->
<!-- mejor_modelo <- modelos_fecha_dico[[as.character(mejor_num_trees)]] -->

<!-- # Mostrar los hiperparámetros seleccionados -->
<!-- cat("Mejor modelo seleccionado:\n") -->
<!-- cat("Número de árboles:", mejor_num_trees, "\n") -->
<!-- cat("mtry:", mejor_mtry, "\n") -->
<!-- cat("min.node.size:", mejor_min_node_size, "\n") -->

<!-- # Mostrar detalles del mejor modelo -->
<!-- print(mejor_modelo) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # ranger_fecha <- caret::train( -->
<!-- #   Y ~ ., -->
<!-- #   data = Train.rf_fecha_dico,  -->
<!-- #   method = "ranger", -->
<!-- #   tuneGrid = data.frame(mtry = 3, min.node.size = 1, splitrule = "gini"),  -->
<!-- #   metric = "Accuracy", -->
<!-- #   importance = "impurity", -->
<!-- #   trControl = cross_val,  # Si querés usar validación cruzada -->
<!-- #   num.trees = 500  # Número de árboles fijo -->
<!-- # ) -->
<!-- # print(ranger_fecha_dico) -->

<!-- ranger_fecha_tot_fx<-function(df_train, model, grid, metrica,  num.trees){ -->

<!-- ranger_fecha_tot <- caret::train( -->
<!--   Y ~ ., -->
<!--   data = df_train, -->
<!--   method = "ranger", -->
<!--   tuneGrid = grid, -->
<!--   metric = "Accuracy", -->
<!--   importance = "impurity", -->
<!--   trControl = cross_val_ranger,  # Si querés usar validación cruzada -->
<!--   num.trees=num.trees  # Número de árboles fijo -->
<!-- ) -->

<!-- return(ranger_fecha_tot) -->
<!-- } -->

<!-- # -->

<!-- ranger_fecha_tot<-ranger_fecha_tot_fx(Train.rf_fecha_dico,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees) -->


<!-- ``` -->


<!-- ```{r} -->
<!-- Test.rf_FECHA$Y<-as.factor(Test.rf_FECHA$Y) -->
<!-- str(Test.rf_FECHA$Y) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- testRF_NOID_FECHA <- Test.rf_FECHA[, -which(names(Test.rf_FECHA) == "Y")] -->

<!-- ``` -->



<!-- ```{r} -->
<!-- #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test) -->
<!-- predRANGER_FECHA<-predict(ranger_fecha_tot, newdata = testRF_NOID_FECHA)#matriz binarizada -->
<!-- print(predRANGER_FECHA) -->
<!-- predRANGER_FECHA <- factor(as.character(predRANGER_FECHA),  -->
<!--                              levels = c("X1", "X2"), -->
<!--                              labels = c("1", "2")) -->
<!-- predRANGER_FECHA <- factor(predRANGER_FECHA, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->




<!-- # Obtener las etiquetas reales del conjunto de datos de prueba -->
<!-- y_test <- Test.rf_FECHA$Y -->

<!-- prob_predRanger_FECHA <- predict(ranger_fecha_tot, newdata = testRF_NOID_FECHA, type = "prob") -->

<!-- colnames(prob_predRanger_FECHA) -->
<!-- # Asegúrate de que y_test sea un factor con los niveles correctos -->
<!-- y_test <- factor(Test.rf_FECHA$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos")) -->



<!-- ``` -->



<!-- ```{r} -->
<!-- library(pROC) -->

<!-- # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2 -->
<!-- y_test_numeric <- as.numeric(y_test) -->

<!-- roc_curve <- roc(response = y_test_numeric,  -->
<!--                  predictor = prob_predRanger_FECHA[, "X2"],  -->
<!--                  levels = c(1, 2)) -->
<!-- AUC_Ranger_FECHA <- auc(roc_curve) -->
<!-- print(paste("El valor de AUC es:", round(AUC_Ranger_FECHA, 3))) -->

<!-- # Visualiza la curva ROC -->
<!-- plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2) -->

<!-- # confusion_matrixRANGER <- table(predRANGER_FECHA, y_test) -->
<!-- # Imprimir la matriz de confusión -->
<!-- # print(confusion_matrixRANGER) -->
<!-- Ranger_metrics_fecha <- caret::confusionMatrix(predRANGER_FECHA, y_test, positive = "Cov.Pos") -->
<!-- ``` -->


```{r}
# # Precisión (Accuracy)
# accuracy_RANGER_fecha <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_fecha,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RANGER_fecha <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RANGER_fecha,2)))
# 
# # Especificidad (TNR)
# specificity_RANGER_fecha <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
# print(paste("la especificidad es:", round(specificity_RANGER_fecha,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RANGER_fecha<- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
# print(paste("el VPP es:", round(ppv_RANGER_fecha,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RANGER_fecha<- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
# print(paste("el VPN es: ",  round(npv_RANGER_fecha,2)))


```



<!-- ```{r} -->
<!-- #  -->
<!-- # library(irr) -->
<!-- #  -->
<!-- # # Calcular Kappa -->
<!-- # kappa_RANGERfecha <- kappa2(cbind(predRANGER_FECHA, Test.rf_FECHA$Y)) -->
<!-- #  -->
<!-- # # Ver el valor de Kappa -->
<!-- # print(paste("El índice Kappa es:", round(kappa_RANGERfecha$value, 3))) -->
<!-- # kappa_RANGERfecha<-round(kappa_RANGERfecha$value, 3) -->
<!-- # Carga los paquetes necesarios -->
<!-- library(caret) -->
<!-- accuracyRANGER_FECHA <- Ranger_metrics_fecha$overall["Accuracy"] -->
<!-- kappaRANGER_FECHA <- Ranger_metrics_fecha$overall["Kappa"] -->
<!-- # Métricas por clase -->
<!-- sensitivityRANGER_FECHA <- Ranger_metrics_fecha$byClass["Sensitivity"] -->
<!-- specificityRANGER_FECHA <- Ranger_metrics_fecha$byClass["Specificity"] -->
<!-- precisionRANGER_FECHA <- Ranger_metrics_fecha$byClass["Pos Pred Value"] -->
<!-- recallRANGER_FECHA <- Ranger_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity -->
<!-- f1_scoreRANGER_FECHA <- Ranger_metrics_fecha$byClass["F1"] -->
<!-- npvRANGER_FECHA <- Ranger_metrics_fecha$byClass["Neg Pred Value"] -->
<!-- prevalenceRANGER_FECHA <- Ranger_metrics_fecha$byClass["Prevalence"] -->
<!-- detection_rateRANGER_FECHA <- Ranger_metrics_fecha$byClass["Detection Rate"] -->
<!-- balanced_accuracyRANGER_FECHA <- Ranger_metrics_fecha$byClass["Balanced Accuracy"] -->

<!-- # Calcular LR+ y LR- que no vienen directamente en confusionMatrix -->
<!-- LR_plusRANGER_FECHA <- sensitivityRANGER_FECHA / (1 - specificityRANGER_FECHA) -->
<!-- LR_minusRANGER_FECHA <- (1 - sensitivityRANGER_FECHA) / specificityRANGER_FECHA -->

<!-- # Para manejar valores especiales -->
<!-- LR_plusRANGER_FECHA <- ifelse(is.nan(LR_plusRANGER_FECHA) | is.infinite(LR_plusRANGER_FECHA), NA, LR_plusRANGER_FECHA) -->
<!-- LR_minusRANGER_FECHA <- ifelse(is.nan(LR_minusRANGER_FECHA) | is.infinite(LR_minusRANGER_FECHA), NA, LR_minusRANGER_FECHA) -->

<!-- # Crear un dataframe con todas las métricas -->
<!-- metrics_ranger_fecha <- data.frame( -->
<!--   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision",  -->
<!--              "F1_Score", "NPV", "Prevalence", "Detection_Rate",  -->
<!--              "Balanced_Accuracy", "LR+", "LR-", "AUC"), -->
<!--   ranger_fecha_tot = c(accuracyRANGER_FECHA, kappaRANGER_FECHA, sensitivityRANGER_FECHA, specificityRANGER_FECHA, precisionRANGER_FECHA, f1_scoreRANGER_FECHA, npvRANGER_FECHA, prevalenceRANGER_FECHA, detection_rateRANGER_FECHA, balanced_accuracyRANGER_FECHA, LR_plusRANGER_FECHA, LR_minusRANGER_FECHA, AUC_Ranger_FECHA)) -->

<!-- # Mostrar los resultados -->
<!-- print(metrics_ranger_fecha) -->

<!-- ``` -->



<!-- # KNN -->


<!-- ```{r} -->
<!-- tuneGrid <- expand.grid(k = 1:15) -->
<!-- # Seeds -->
<!-- seed.rf <- 42 -->
<!-- set.seed(seed.rf) -->

<!-- seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1) -->

<!-- for (i in 1:(particiones * repeticiones)) { -->
<!--   seeds[[i]] <- sample.int(500, nrow(tuneGrid))  -->
<!-- } -->

<!-- seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1) -->

<!-- # Training control -->
<!-- ``` -->



<!-- ```{r} -->
<!-- # Training control -->

<!-- cross_val_knn <- trainControl( -->
<!--   method = "repeatedcv", -->
<!--   number = particiones, -->
<!--   repeats = repeticiones, -->
<!--   returnResamp = "final", -->
<!--   verboseIter = FALSE, -->
<!--   allowParallel = TRUE, -->
<!--   classProbs = TRUE, -->
<!--   seeds = seeds) -->
<!-- ``` -->


<!-- ```{r} -->

<!-- # KNN_fecha <- caret::train(formula,  -->
<!-- #                     data = trainData_fecha,  -->
<!-- #                     trControl = cross_val,  -->
<!-- #                     method = "knn",  -->
<!-- #                     metric = "Accuracy",  -->
<!-- #                     preProcess = c("center","scale"), -->
<!-- #                     tuneGrid = tuneGrid) -->
<!-- #  -->
<!-- # KNN_fecha -->
<!-- KNN_fecha_tot_fx <- function(df_train, model, grid, metrica = "Accuracy",  control, preProcess) { -->


<!--   # Entrenar el modelo -->
<!--   KNN_fecha_tot <- caret::train( -->
<!--     Y ~ ., -->
<!--     data = df_train, -->
<!--     method = model, -->
<!--     tuneGrid = grid, -->
<!--     metric = metrica, -->
<!--     trControl = cross_val_knn, -->
<!--     preProcess = preProcess -->
<!--   ) -->

<!--   # Mostrar resumen -->
<!--   print(KNN_fecha_tot) -->
<!--   plot(KNN_fecha_tot, main = title) -->

<!--   # Guardar resultado -->
<!--  # save(KNN_fecha_tot_fx, file = paste0(title, ".rda")) -->

<!--   return(KNN_fecha_tot) -->
<!-- } -->

<!-- KNN_fecha_tot<-KNN_fecha_tot_fx(df_train=trainData_fecha, model="knn", grid=tuneGrid, metrica="Accuracy",  preProcess = c("center","scale") ) -->
<!-- print(KNN_fecha_tot) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- plot(KNN_fecha_tot) -->
<!-- ``` -->

<!-- # GLMNET -->


<!-- ```{r} -->

<!-- tuneGrid <- expand.grid( -->
<!--   alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso) -->
<!--   lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda -->
<!-- ) -->
<!-- # Seeds -->
<!-- seed.rf <- 42 -->
<!-- set.seed(seed.rf) -->
<!-- max_sample <- max(500, nrow(tuneGrid) * 2) -->
<!-- seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1) -->

<!-- for (i in 1:(particiones * repeticiones)) { -->
<!--   seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid))  -->
<!-- } -->

<!-- seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1) -->

<!-- # Training control -->
<!-- ``` -->



<!-- ```{r} -->
<!-- # Training control -->

<!-- cross_val_glm <- trainControl( -->
<!--   method = "repeatedcv", -->
<!--   number = particiones, -->
<!--   repeats = repeticiones, -->
<!--   returnResamp = "final", -->
<!--   verboseIter = FALSE, -->
<!--   allowParallel = TRUE, -->
<!--   classProbs = TRUE, -->
<!--   seeds = seeds) -->
<!-- ``` -->


<!-- ```{r} -->

<!-- #set.seed(42) -->
<!-- # GLM_fecha <- caret::train(formula,  -->
<!-- #                   data = trainData_fecha,   -->
<!-- #                    method = "glmnet", -->
<!-- #                    metric = "Accuracy", -->
<!-- #                    tuneLength = 3, -->
<!-- #                    trControl = cross_val, -->
<!-- #                   tuneGrid=tuneGrid) -->
<!-- #  -->
<!-- # GLM_fecha  -->
<!-- GLM_fecha_tot_fx <- function(df_train, model, grid, metrica = "Accuracy", control) { -->


<!--   # Entrenar el modelo -->
<!--   GLM_fecha_tot <- caret::train( -->
<!--     Y ~ ., -->
<!--     data = df_train, -->
<!--     method = model, -->
<!--     tuneGrid = grid, -->
<!--     metric = metrica, -->
<!--     trControl = cross_val_glm -->
<!--   ) -->

<!--   # Mostrar resumen -->
<!--   print(GLM_fecha_tot) -->
<!--   plot(GLM_fecha_tot, main = title) -->

<!--   # Guardar resultado -->
<!--  # save(GLM_fecha_tot, file = paste0(title, ".rda")) -->

<!--   return(GLM_fecha_tot) -->
<!-- } -->

<!-- GLM_fecha_tot<-GLM_fecha_tot_fx(df_train=trainData_fecha, model="glmnet", grid=tuneGrid, metrica="Accuracy") -->
<!-- print(GLM_fecha_tot) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Imprimir los mejores hiperparámetros -->
<!-- best_params <- GLM_fecha_tot$bestTune -->
<!-- print("Best Hyperparameters:") -->
<!-- print(best_params) -->

<!-- # Extraer accuracy y kappa del mejor modelo -->
<!-- # Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results -->
<!-- best_model_index <- apply(GLM_fecha_tot$results, 1, function(row) { -->
<!--   all(row[1:length(best_params)] == as.numeric(best_params)) -->
<!-- }) -->

<!-- best_model_results <- GLM_fecha_tot$results[best_model_index, ] -->



<!-- # Imprimir accuracy y kappa del mejor modelo -->
<!-- accuracy <- best_model_results$Accuracy -->
<!-- kappa <- best_model_results$Kappa -->

<!-- print("Accuracy of the Best Model:") -->
<!-- print(accuracy) -->

<!-- print("Kappa of the Best Model:") -->
<!-- print(kappa) -->

<!-- ``` -->

<!-- # Prediccion -->

<!-- ```{r} -->
<!-- testData_fecha$Y<-as.factor(testData_fecha$Y) -->
<!-- str(testData_fecha$Y) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- testData_NOID_fecha <- testData_fecha[, -which(names(testData_fecha) == "Y")] -->

<!-- ``` -->

# RF

# ```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# predRF_fecha<-predict(RF_fecha_tot, newdata = testData_NOID_fecha)#matriz binarizada
# print(predRF_fecha)
# 
# predRF_fecha <- factor(as.character(predRF_fecha), 
#                              levels = c("X1", "X2"),
#                              labels = c("1", "2"))
# predRF_fecha <- factor(predRF_fecha, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
# 
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData_fecha$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
# y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
# ```
# 
# ```{r}
# # confusion_matrixRF <- table(predRF_fecha, y_test)
# # Imprimir la matriz de confusión
# # print(confusion_matrixRF)
# RF_metrics_fecha <- caret::confusionMatrix(predRF_fecha, y_test, positive = "Cov.Pos")
# 
# 
# ```
# 
# ```{r}
# 
# library(pROC)
# 
# prob_predRF_fecha <- predict(RF_fecha_tot, newdata = testData_NOID_fecha, type = "prob")
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF_fecha[, "X2"], levels = c(1, 2))
# AUC_RF_fecha <- auc(roc_curve)
# 
# print(paste("El valor de AUC es:", round(AUC_RF_fecha, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
# 
# ```
# 
# 
```{r}
# # 
# # library(irr)
# # 
# # # Calcular Kappa
# # kappa_RFfecha <- kappa2(cbind(predRF_fecha, testData_fecha$Y))
# # 
# # # Ver el valor de Kappa
# # print(paste("El índice Kappa es:", round(kappa_RFfecha$value, 3)))
# # kappa_RFfecha<-round(kappa_RFfecha$value, 3)
# # Carga los paquetes necesarios
# library(caret)
# accuracyRF_fecha <- RF_metrics_fecha$overall["Accuracy"]
# kappaRF_fecha <- RF_metrics_fecha$overall["Kappa"]
# # Métricas por clase
# sensitivityRF_fecha <- RF_metrics_fecha$byClass["Sensitivity"]
# specificityRF_fecha <- RF_metrics_fecha$byClass["Specificity"]
# precisionRF_fecha <- RF_metrics_fecha$byClass["Pos Pred Value"]
# recallRF_fecha <- RF_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity
# f1_scoreRF_fecha <- RF_metrics_fecha$byClass["F1"]
# npvRF_fecha <- RF_metrics_fecha$byClass["Neg Pred Value"]
# prevalenceRF_fecha <- RF_metrics_fecha$byClass["Prevalence"]
# detection_rateRF_fecha <- RF_metrics_fecha$byClass["Detection Rate"]
# balanced_accuracyRF_fecha <- RF_metrics_fecha$byClass["Balanced Accuracy"]
# 
# # Calcular LR+ y LR- que no vienen directamente en confusionMatrix
# LR_plusRF_fecha <- sensitivityRF_fecha / (1 - specificityRF_fecha)
# LR_minusRF_fecha <- (1 - sensitivityRF_fecha) / specificityRF_fecha
# 
# # Para manejar valores especiales
# LR_plusRF_fecha <- ifelse(is.nan(LR_plusRF_fecha) | is.infinite(LR_plusRF_fecha), NA, LR_plusRF_fecha)
# LR_minusRF_fecha <- ifelse(is.nan(LR_minusRF_fecha) | is.infinite(LR_minusRF_fecha), NA, LR_minusRF_fecha)
# 
# # Crear un dataframe con todas las métricas
# metrics_rf_fecha <- data.frame(
#   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
#              "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
#              "Balanced_Accuracy", "LR+", "LR-", "AUC"),
#   rf_fecha_tot = c(accuracyRF_fecha, kappaRF_fecha, sensitivityRF_fecha, specificityRF_fecha, precisionRF_fecha, 
#             f1_scoreRF_fecha, npvRF_fecha, prevalenceRF_fecha, detection_rateRF_fecha, 
#             balanced_accuracyRF_fecha, LR_plusRF_fecha, LR_minusRF_fecha, AUC_RF_fecha))
# 
# # Mostrar los resultados
# print(metrics_rf_fecha)

```



```{r}
ciego_combat_corrected_fecha_tot_nodic <- as.data.frame(featureMatrix_ciegos_total)
ciego_combat_corrected_fecha_tot_nodic <- ciego_combat_corrected_fecha_tot_nodic[, colnames(trainData_fecha)]
# Si viene con columna Y (conocida), separala:
if ("Y" %in% colnames(ciego_combat_corrected_fecha_tot_nodic)) {
  Y_ciego <- ciego_combat_corrected_fecha_tot_nodic$Y
  X_ciego <- ciego_combat_corrected_fecha_tot_nodic[, setdiff(colnames(ciego_combat_corrected_fecha_tot_nodic), "Y")]
} else {
  X_ciego <- ciego_combat_corrected_fecha_tot_nodic
  Y_ciego <- NULL
}


# Renombrar niveles
levels(Y_ciego) <- c("Cov.Neg", "Cov.Pos")



```



```{r}



# Predicciones (clase)
predicciones_ciego_fecha_tot <- predict(RF_fecha_tot, newdata = X_ciego)
levels(predicciones_ciego_fecha_tot) <- c("Cov.Neg", "Cov.Pos")

# Predicciones (probabilidades)
prob_ciego <- predict(RF_fecha_tot, newdata = X_ciego, type = "prob")
#prob_ciego$Cov.Neg<-prob_ciego$X1
#prob_ciego$Cov.Pos<-prob_ciego$X2


  # Confusion matrix
  ciego_tot_fecha<-caret::confusionMatrix(predicciones_ciego_fecha_tot, Y_ciego, positive = "Cov.Pos")

  # AUC
  library(pROC)
  Y_ciego_num <- as.numeric(Y_ciego)
  roc_ciego <- roc(Y_ciego_num, prob_ciego[, "X2"])
  plot(roc_ciego, main = "ROC en Datos Ciegos", col = "darkgreen", lwd = 2)
  auc_fecha_tot_rf_ciego<-auc(roc_ciego)
  AUC_RF_fecha_ciego<-print(auc_fecha_tot_rf_ciego)
  
  
  
  library(caret)
accuracyRF_fecha_ciego <- ciego_tot_fecha$overall["Accuracy"]
kappaRF_fecha_ciego <- ciego_tot_fecha$overall["Kappa"]
# Métricas por clase
sensitivityRF_fecha_ciego <- ciego_tot_fecha$byClass["Sensitivity"]
specificityRF_fecha_ciego <- ciego_tot_fecha$byClass["Specificity"]
precisionRF_fecha_ciego <- ciego_tot_fecha$byClass["Pos Pred Value"]
recallRF_fecha_ciego <- ciego_tot_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRF_fecha_ciego <- ciego_tot_fecha$byClass["F1"]
npvRF_fecha_ciego <- ciego_tot_fecha$byClass["Neg Pred Value"]
prevalenceRF_fecha_ciego <- ciego_tot_fecha$byClass["Prevalence"]
detection_rateRF_fecha_ciego <- ciego_tot_fecha$byClass["Detection Rate"]
balanced_accuracyRF_fecha_ciego <- ciego_tot_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRF_fecha_ciego <- sensitivityRF_fecha_ciego / (1 - specificityRF_fecha_ciego)
LR_minusRF_fecha_ciego <- (1 - sensitivityRF_fecha_ciego) / specificityRF_fecha_ciego

# Para manejar valores especiales
LR_plusRF_fecha_ciego <- ifelse(is.nan(LR_plusRF_fecha_ciego) | is.infinite(LR_plusRF_fecha_ciego), NA, LR_plusRF_fecha_ciego)
LR_minusRF_fecha_ciego <- ifelse(is.nan(LR_minusRF_fecha_ciego) | is.infinite(LR_minusRF_fecha_ciego), NA, LR_minusRF_fecha_ciego)

# Crear un dataframe con todas las métricas
metrics_rf_fecha_ciego_cv <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  rf_fecha_tot_cv = c(accuracyRF_fecha_ciego, kappaRF_fecha_ciego, sensitivityRF_fecha_ciego, specificityRF_fecha_ciego, precisionRF_fecha_ciego, 
            f1_scoreRF_fecha_ciego, npvRF_fecha_ciego, prevalenceRF_fecha_ciego, detection_rateRF_fecha_ciego,
            balanced_accuracyRF_fecha_ciego, LR_plusRF_fecha_ciego, LR_minusRF_fecha_ciego, AUC_RF_fecha_ciego))

# Mostrar los resultados
print(metrics_rf_fecha_ciego_cv)
```





<!-- # Prediccion KNN -->

<!-- ```{r} -->
<!-- #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test) -->
<!-- predKNN_fecha<-predict(KNN_fecha_tot, newdata = testData_NOID_fecha)#matriz binarizada -->
<!-- print(predKNN_fecha) -->

<!-- predKNN_fecha <- factor(as.character(predKNN_fecha),  -->
<!--                              levels = c("X1", "X2"), -->
<!--                              labels = c("1", "2")) -->
<!-- predKNN_fecha <- factor(predKNN_fecha, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->



<!-- # Obtener las etiquetas reales del conjunto de datos de prueba -->
<!-- y_test <- testData_fecha$Y -->
<!-- #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada -->
<!-- y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # confusion_matrixKNN <- table(predKNN_fecha, y_test) -->
<!-- # # Imprimir la matriz de confusión -->
<!-- # print(confusion_matrixKNN) -->
<!-- KNN_metrics_fecha <- caret::confusionMatrix(predKNN_fecha, y_test, positive = "Cov.Pos") -->
<!-- ``` -->

<!-- ```{r} -->



<!-- prob_predKNNfecha <- predict(KNN_fecha_tot, newdata = testData_NOID_fecha, type = "prob") -->

<!-- # Verifica los nombres de las columnas (clases) -->
<!-- print(colnames(prob_predKNNfecha))  # Deberían ser "1" y "2" o los nombres de las clases -->

<!-- # Asegúrate de que y_test sea numérico con niveles 1 y 2 -->
<!-- y_test_numeric <- as.numeric(testData_fecha$Y) -->

<!-- # Calcula la curva ROC usando la probabilidad de la clase positiva ("2") -->
<!-- roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNfecha[, "X2"], levels = c(1, 2)) -->

<!-- # Calcula el AUC -->
<!-- AUC_KNN_fecha <- auc(roc_curve_knn) -->
<!-- print(paste("El valor de AUC es:", round(AUC_KNN_fecha, 3))) -->

<!-- # Visualiza la curva ROC -->
<!-- plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2) -->
<!-- ``` -->



<!-- ```{r} -->
<!-- #  -->
<!-- # library(irr) -->
<!-- #  -->
<!-- # # Calcular Kappa -->
<!-- # kappa_KNNfecha <- kappa2(cbind(predKNN_fecha, testData_fecha$Y)) -->
<!-- #  -->
<!-- # # Ver el valor de Kappa -->
<!-- # print(paste("El índice Kappa es:", round(kappa_KNNfecha$value, 3))) -->
<!-- # kappa_KNNfecha<-round(kappa_KNNfecha$value, 3) -->
<!-- # Carga los paquetes necesarios -->
<!-- library(caret) -->
<!-- accuracyKNN_fecha <- KNN_metrics_fecha$overall["Accuracy"] -->
<!-- kappaKNN_fecha <- KNN_metrics_fecha$overall["Kappa"] -->
<!-- # Métricas por clase -->
<!-- sensitivityKNN_fecha <- KNN_metrics_fecha$byClass["Sensitivity"] -->
<!-- specificityKNN_fecha <- KNN_metrics_fecha$byClass["Specificity"] -->
<!-- precisionKNN_fecha <- KNN_metrics_fecha$byClass["Pos Pred Value"] -->
<!-- recallKNN_fecha <- KNN_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity -->
<!-- f1_scoreKNN_fecha <- KNN_metrics_fecha$byClass["F1"] -->
<!-- npvKNN_fecha <- KNN_metrics_fecha$byClass["Neg Pred Value"] -->
<!-- prevalenceKNN_fecha <- KNN_metrics_fecha$byClass["Prevalence"] -->
<!-- detection_rateKNN_fecha <- KNN_metrics_fecha$byClass["Detection Rate"] -->
<!-- balanced_accuracyKNN_fecha <- KNN_metrics_fecha$byClass["Balanced Accuracy"] -->

<!-- # Calcular LR+ y LR- que no vienen directamente en confusionMatrix -->
<!-- LR_plusKNN_fecha <- sensitivityKNN_fecha / (1 - specificityKNN_fecha) -->
<!-- LR_minusKNN_fecha <- (1 - sensitivityKNN_fecha) / specificityKNN_fecha -->

<!-- # Para manejar valores especiales -->
<!-- LR_plusKNN_fecha <- ifelse(is.nan(LR_plusKNN_fecha) | is.infinite(LR_plusKNN_fecha), NA, LR_plusKNN_fecha) -->
<!-- LR_minusKNN_fecha <- ifelse(is.nan(LR_minusKNN_fecha) | is.infinite(LR_minusKNN_fecha), NA, LR_minusKNN_fecha) -->

<!-- # Crear un dataframe con todas las métricas -->
<!-- metrics_KNN_fecha <- data.frame( -->
<!--   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision",  -->
<!--              "F1_Score", "NPV", "Prevalence", "Detection_Rate",  -->
<!--              "Balanced_Accuracy", "LR+", "LR-", "AUC"), -->
<!--   KNN_fecha_tot = c(accuracyKNN_fecha, kappaKNN_fecha, sensitivityKNN_fecha, specificityKNN_fecha, precisionKNN_fecha,  -->
<!--             f1_scoreKNN_fecha, npvKNN_fecha, prevalenceKNN_fecha, detection_rateKNN_fecha,  -->
<!--             balanced_accuracyKNN_fecha, LR_plusKNN_fecha, LR_minusKNN_fecha, AUC_KNN_fecha)) -->

<!-- # Mostrar los resultados -->
<!-- print(metrics_KNN_fecha) -->

<!-- ``` -->




<!-- ```{r} -->
<!-- #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test) -->
<!-- predGLM_fecha<-predict(GLM_fecha_tot, newdata = testData_NOID_fecha)#matriz binarizada -->
<!-- print(predGLM_fecha) -->

<!-- predGLM_fecha <- factor(as.character(predGLM_fecha),  -->
<!--                              levels = c("X1", "X2"), -->
<!--                              labels = c("1", "2")) -->
<!-- predGLM_fecha <- factor(predGLM_fecha, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->


<!-- # Obtener las etiquetas reales del conjunto de datos de prueba -->
<!-- y_test <- testData_fecha$Y -->
<!-- #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada -->
<!-- y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # confusion_matrixGLM <- table(predGLM_fecha, y_test) -->
<!-- # # Imprimir la matriz de confusión -->
<!-- # print(confusion_matrixGLM) -->
<!-- GLM_metrics_fecha <- caret::confusionMatrix(predGLM_fecha, y_test, positive = "Cov.Pos") -->
<!-- ``` -->


<!-- ```{r} -->

<!-- # Predicciones probabilísticas con el modelo GLM -->
<!-- prob_predGLM_fecha <- predict(GLM_fecha_tot, newdata = testData_NOID_fecha, type = "prob") -->

<!-- # Verifica los nombres de las columnas (clases) -->
<!-- print(colnames(prob_predGLM_fecha))  # Deberían ser "1" y "2" o los nombres de las clases -->

<!-- # Asegúrate de que y_test sea numérico con niveles 1 y 2 -->
<!-- y_test_numeric <- as.numeric(testData_fecha$Y) -->

<!-- # Calcula la curva ROC usando la probabilidad de la clase positiva ("2") -->
<!-- roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM_fecha[, "X2"], levels = c(1, 2)) -->

<!-- # Calcula el AUC -->
<!-- AUC_GLM_fecha <- auc(roc_curve_glm) -->
<!-- print(paste("El valor de AUC es:", round(AUC_GLM_fecha, 3))) -->

<!-- # Visualiza la curva ROC -->
<!-- plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #  -->
<!-- # library(irr) -->
<!-- #  -->
<!-- # # Calcular Kappa -->
<!-- # kappa_GLM_fecha <- kappa2(cbind(predGLM_fecha, testData_fecha$Y)) -->
<!-- #  -->
<!-- # # Ver el valor de Kappa -->
<!-- # print(paste("El índice Kappa es:", round(kappa_GLM_fecha$value, 3))) -->
<!-- # kappa_GLM_fecha<-round(kappa_GLM_fecha$value, 3) -->
<!-- library(caret) -->
<!-- accuracyGLM_fecha<- GLM_metrics_fecha$overall["Accuracy"] -->
<!-- kappaGLM_fecha<- GLM_metrics_fecha$overall["Kappa"] -->
<!-- # Métricas por clase -->
<!-- sensitivityGLM_fecha <- GLM_metrics_fecha$byClass["Sensitivity"] -->
<!-- specificityGLM_fecha <- GLM_metrics_fecha$byClass["Specificity"] -->
<!-- precisionGLM_fecha <- GLM_metrics_fecha$byClass["Pos Pred Value"] -->
<!-- recallGLM_fecha <- GLM_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity -->
<!-- f1_scoreGLM_fecha <- GLM_metrics_fecha$byClass["F1"] -->
<!-- npvGLM_fecha <- GLM_metrics_fecha$byClass["Neg Pred Value"] -->
<!-- prevalenceGLM_fecha <- GLM_metrics_fecha$byClass["Prevalence"] -->
<!-- detection_rateGLM_fecha <- GLM_metrics_fecha$byClass["Detection Rate"] -->
<!-- balanced_accuracyGLM_fecha <- GLM_metrics_fecha$byClass["Balanced Accuracy"] -->

<!-- # Calcular LR+ y LR- que no vienen directamente en confusionMatrix -->
<!-- LR_plusGLM_fecha <- sensitivityGLM_fecha / (1 - specificityGLM_fecha) -->
<!-- LR_minusGLM_fecha <- (1 - sensitivityGLM_fecha) / specificityGLM_fecha -->

<!-- # Para manejar valores especiales -->
<!-- LR_plusGLM_fecha <- ifelse(is.nan(LR_plusGLM_fecha) | is.infinite(LR_plusGLM_fecha), NA, LR_plusGLM_fecha) -->
<!-- LR_minusGLM_fecha <- ifelse(is.nan(LR_minusGLM_fecha) | is.infinite(LR_minusGLM_fecha), NA, LR_minusGLM_fecha) -->

<!-- # Crear un dataframe con todas las métricas -->
<!-- metrics_glm_fecha <- data.frame( -->
<!--   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision",  -->
<!--              "F1_Score", "NPV", "Prevalence", "Detection_Rate",  -->
<!--              "Balanced_Accuracy", "LR+", "LR-", "AUC"), -->
<!--   glm_fecha_tot = c(accuracyGLM_fecha, kappaGLM_fecha, sensitivityGLM_fecha, specificityGLM_fecha, precisionGLM_fecha,  -->
<!--             f1_scoreGLM_fecha, npvGLM_fecha, prevalenceGLM_fecha, detection_rateGLM_fecha,  -->
<!--             balanced_accuracyGLM_fecha, LR_plusGLM_fecha, LR_minusGLM_fecha, AUC_GLM_fecha)) -->

<!-- # Mostrar los resultados -->
<!-- print(metrics_glm_fecha) -->

<!-- ``` -->




