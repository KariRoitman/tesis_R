---
title: "SIN CORRECCION"
author: "Karina Roitman"
date: "2024-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Modelos sin corrección de efecto batch

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```

sin dicotomizar


```{r}
library(dplyr)

# Eliminar la columna 'label' usando dplyr
#featureMatrix <- featureMatrix %>% select(-label)


```


```{r}
featureMatrix<-as.data.frame(featureMatrix)
featureMatrix_label<-featureMatrix[, c(2:64,66)]
featureMatrix_bind<-as.data.frame(featureMatrix_label)
```


```{r}
#featureMatrix_bind<- cbind(featureMatrix_label, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos
```



```{r}
# Renombrar la columna 'label' a 'Y' y convertirla en factor
featureMatrix_bind <- featureMatrix_bind %>% rename(Y = ncol(featureMatrix_bind))

featureMatrix_bind <- as.data.frame(featureMatrix_bind)
str(featureMatrix_bind)

# Verificar estructura del dataframe después de los cambios


```




```{r}

for (i in 1:(ncol(featureMatrix_bind) - 1)) {
  featureMatrix_bind[[i]] <- as.numeric(featureMatrix_bind[[i]])
}

str(featureMatrix_bind)
```

```{r}
library(dplyr)
#featureMatrix_bind <- featureMatrix_bind %>% rename(Y = label)
featureMatrix_bind$Y<-as.factor(featureMatrix_bind$Y)
str(featureMatrix_bind$Y)
```





```{r}
# featureMatrix_bind_nodic_tot<- featureMatrix_bind[1:542,]
# ciegoMatrix_nodico_tot<-featureMatrix_bind[543:694,]
```



```{r}
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_bind, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)


# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
table(trainData$Y)
table(testData$Y)

```


```{r}
#set.seed(42)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```


```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



```{r}
# Submuestras y repeticiones
set.seed(42)
# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```


# Random forest

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(2, 5, 10)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry)
                               #min.node.size = seq(1, 30, 2),
                               #min.node.size=min.node.size,
                               #splitrule = "gini")


seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}
# genera un vector de nrow(hiperparametros) números aleatorios entre 1 y 500.
#Esto asegura que cada combinación de hiperparámetros tenga una semilla diferente en cada iteración.

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

trainData$Y <- factor(as.numeric(factor(trainData$Y)))
trainData$Y <- factor(trainData$Y, levels = c("1", "2"))
levels(trainData$Y) <- make.names(levels(trainData$Y))

```



```{r}
library(caret)
#control1 <- trainControl(method = "cv", number = 10)
#grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

# RF <- caret::train(Y ~ ., data = trainData, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación


RF_matrix_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  RF_matrix <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_rf
  )
  
  # Mostrar resumen
  print(RF_matrix)
  plot(RF_matrix)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF_matrix)
}

RF_matrix<-RF_matrix_fx(df_train=trainData, model="rf", grid=hiperparametros, metrica="Accuracy", title = "Primera_funcion" )
print(RF_matrix)
```

```{r}
RF_matrix
```

```{r}
plot(RF_matrix)
```



# GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # Método para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


# GBM_model <- train(
#   Y ~ .,                        # Reemplaza Y con tu variable dependiente
#   data = trainData,            # Tu conjunto de entrenamiento
#   method = "gbm",              # Método GBM
#   trControl = control1,         # Control de entrenamiento
#   tuneGrid = expand.grid(
#     interaction.depth = 3,     # Profundidad máxima de los árboles
#     n.trees = 100,             # Número de árboles
#     shrinkage = 0.1,           # Tasa de aprendizaje
#     n.minobsinnode = 10        # Mínimo de observaciones en cada nodo
#   ),
#   metric = "Accuracy",         # Métrica de rendimiento
#   verbose = TRUE               # Mostrar progreso
# )
# GBM_model

```



```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

# SVM


```{r}
# set.seed(42)
# SVM <- caret::train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10, prob.model = TRUE)
# # Print the best tuning parameter sigma and C that maximizes model accuracy
# SVM$bestTun    
# 
# #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
# #sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo
# 
# SVM
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```

```{r}
#plot(SVM)
```


# KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_knn <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}
# 
# KNN <- caret::train(formula, 
#                     data = trainData, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNN
KNN_matrix_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control, preProcess) {

  
  # Entrenar el modelo
  KNN_matrix <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_knn,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNN_matrix)
  plot(KNN_matrix)
  
  # Guardar resultado
  #save(KNN_matrix, file = paste0(title, ".rda"))
  
  return(KNN_matrix)
}

KNN_matrix<-KNN_matrix_fx(df_train=trainData, model="knn", grid=tuneGrid, metrica="Accuracy", preProcess = c("center","scale") )
print(KNN_matrix)

```


```{r}
plot(KNN_matrix)
```

# GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)
max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_glm <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```



```{r}

# GLM <- caret::train(formula, 
#                   data = trainData,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLM
GLM_matrix_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  GLM_matrix <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_glm
  )
  
  # Mostrar resumen
  print(GLM_matrix)
  plot(GLM_matrix)
  
  # Guardar resultado
  save(GLM_matrix, file = paste0(title, ".rda"))
  
  return(GLM_matrix)
}

GLM_matrix<-GLM_matrix_fx(df_train=trainData, model="glmnet", grid=tuneGrid )
print(GLM_matrix)
```


```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_matrix$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_matrix$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_matrix$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# Prediccion

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```


```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_matrix, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
prediccionesRF <- factor(as.character(prediccionesRF), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesRF <- factor(prediccionesRF, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))

#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
prob_predRF <- predict(RF_matrix, newdata = testData_NOID, type = "prob")
```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF[, "X2"], levels = c(1, 2))
AUC_RF <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```



```{r}
# confusion_matrixRF <- table(prediccionesRF, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRF)
RF_metrics_sincorreccion <- caret::confusionMatrix(prediccionesRF, y_test, positive = "Cov.Pos")
```

```{r}
# #metricas
# # Precisión
# accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
# print(accuracy_RF)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
# print(sensitivity_RF)
# # Precisión negativa (VN) o especificidad
# specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[ ,1 ])
# specificity_RF
# 
# # Valor predictivo positivo (VPP)
# ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
# print(ppv_RF)
# # Valor predictivo negativo (VPN)
# npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
# print(npv_RF)

library(caret)
accuracy_RF <- RF_metrics_sincorreccion$overall["Accuracy"]
kappa_RF <- RF_metrics_sincorreccion$overall["Kappa"]
# Métricas por clase
sensitivity_RF <- RF_metrics_sincorreccion$byClass["Sensitivity"]
specificity_RF <- RF_metrics_sincorreccion$byClass["Specificity"]
precision_RF <- RF_metrics_sincorreccion$byClass["Pos Pred Value"]
recall_RF <- RF_metrics_sincorreccion$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RF <- RF_metrics_sincorreccion$byClass["F1"]
npv_RF <- RF_metrics_sincorreccion$byClass["Neg Pred Value"]
prevalence_RF <- RF_metrics_sincorreccion$byClass["Prevalence"]
detection_rate_RF <- RF_metrics_sincorreccion$byClass["Detection Rate"]
balanced_accuracy_RF <- RF_metrics_sincorreccion$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RF <- sensitivity_RF / (1 - specificity_RF)
LR_minus_RF <- (1 - sensitivity_RF) / specificity_RF

# Para manejar valores especiales
LR_plus_RF <- ifelse(is.nan(LR_plus_RF) | is.infinite(LR_plus_RF), NA, LR_plus_RF)
LR_minus_RF <- ifelse(is.nan(LR_minus_RF) | is.infinite(LR_minus_RF), NA, LR_minus_RF)

# Crear un dataframe con todas las métricas
metrics_rf_sin_correccion <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  rf_matrix_total = c(accuracy_RF, kappa_RF, sensitivity_RF, specificity_RF, precision_RF, 
            f1_score_RF, npv_RF, prevalence_RF, detection_rate_RF, 
            balanced_accuracy_RF, LR_plus_RF, LR_minus_RF, AUC_RF))

# Mostrar los resultados
print(metrics_rf_sin_correccion)




```


```{r}
ciegoMatrix_rf_nodico_tot <- as.data.frame(featureMatrix_ciegos_total)

# Si viene con columna Y (conocida), separala:
if ("Y" %in% colnames(ciegoMatrix_rf_nodico_tot)) {
  Y_ciego <- ciegoMatrix_rf_nodico_tot$Y
  X_ciego <- ciegoMatrix_rf_nodico_tot[, setdiff(colnames(ciegoMatrix_rf_nodico_tot), "Y")]
} else {
  X_ciego <- ciegoMatrix_rf_nodico_tot
  Y_ciego <- NULL
}


levels(Y_ciego) <- c("Cov.Neg", "Cov.Pos")
```


```{r}
# Predicciones (clase)
predicciones_ciego_matriz_tot <- predict(RF_matrix, newdata = X_ciego)
levels(predicciones_ciego_matriz_tot) <- c("Cov.Neg", "Cov.Pos")

# Predicciones (probabilidades)
prob_ciego <- predict(RF_matrix, newdata = X_ciego, type = "prob")
#prob_ciego$Cov.Neg<-prob_ciego$X1
#prob_ciego$Cov.Pos<-prob_ciego$X2


  # Confusion matrix
  ciego_tot_equipo_rf_matrix<-caret::confusionMatrix(predicciones_ciego_matriz_tot, Y_ciego, positive = "Cov.Pos")

  # AUC
  library(pROC)
  Y_ciego_num <- as.numeric(Y_ciego)
  roc_ciego <- roc(Y_ciego_num, prob_ciego[, "X2"])
  plot(roc_ciego, main = "ROC en Datos Ciegos", col = "darkgreen", lwd = 2)
 AUC_RF_ciego<- auc_matrix_tot<-auc(roc_ciego)
  print(auc_matrix_tot)
  
  
library(caret)
accuracy_RF_ciego <- ciego_tot_equipo_rf_matrix$overall["Accuracy"]
kappa_RF_ciego <- ciego_tot_equipo_rf_matrix$overall["Kappa"]
# Métricas por clase
sensitivity_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Sensitivity"]
specificity_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Specificity"]
precision_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Pos Pred Value"]
recall_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["F1"]
npv_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Neg Pred Value"]
prevalence_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Prevalence"]
detection_rate_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Detection Rate"]
balanced_accuracy_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RF_ciego <- sensitivity_RF_ciego / (1 - specificity_RF_ciego)
LR_minus_RF_ciego <- (1 - sensitivity_RF_ciego) / specificity_RF_ciego

# Para manejar valores especiales
LR_plus_RF_ciego <- ifelse(is.nan(LR_plus_RF_ciego) | is.infinite(LR_plus_RF_ciego), NA, LR_plus_RF_ciego)
LR_minus_RF_ciego <- ifelse(is.nan(LR_minus_RF_ciego) | is.infinite(LR_minus_RF_ciego), NA, LR_minus_RF_ciego)

# Crear un dataframe con todas las métricas
metrics_rf_matrix_ciego_fx <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  rf_matrix_total = c(accuracy_RF_ciego, kappa_RF_ciego, sensitivity_RF_ciego, specificity_RF_ciego, precision_RF_ciego, 
            f1_score_RF_ciego, npv_RF_ciego, prevalence_RF_ciego, detection_rate_RF_ciego, 
            balanced_accuracy_RF_ciego, LR_plus_RF_ciego, LR_minus_RF_ciego, AUC_RF_ciego))

# Mostrar los resultados
print(metrics_rf_matrix_ciego_fx) 

  
  
```




```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_RF <- kappa2(cbind(prediccionesRF, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RF$value, 3)))
# kappa_RF<-round(kappa_RF$value, 3)
```

# Prediccion GBM
 
```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
# print(prediccionesGBM)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

# Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# prediccionesSVM<-predict(SVM, newdata = testData_NOID)#matriz binarizada
# print(prediccionesSVM)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
# confusion_matrixSVM <- table(prediccionesSVM, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixSVM)

```


```{r}
# #metricas
# # Precisión
# accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
# print(accuracy_SVM)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
# print(sensitivity_SVM)
# # Precisión negativa (VN) o especificidad
# specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
# specificity_SVM
# 
# # Valor predictivo positivo (VPP)
# ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
# print(ppv_SVM)
# # Valor predictivo negativo (VPN)
# npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
# print(npv_SVM)
# 
# 
# 
# 
# prob_predSVM <- predict(SVM, newdata = testData_NOID, type = "prob")
# 
# 
# # Verifica los nombres de las columnas
# print(colnames(prob_predSVM))  # Debería mostrar "1" y "2"
# 
# # Calcula el AUC usando la columna correspondiente a "2" (Cov.Pos)
# library(pROC)
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, predictor = prob_predSVM[, "2"], levels = c(1, 2))
# AUC_SVM <- auc(roc_curve)
# 
# print(paste("El valor de AUC es:", round(AUC_SVM, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM<-predict(GLM_matrix, newdata = testData_NOID)#matriz binarizada
print(predGLM)

predGLM <- factor(as.character(predGLM), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLM <- factor(predGLM, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
# confusion_matrixGLM <- table(predGLM, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLM)
GLM_sincorreccion<- caret::confusionMatrix(predGLM, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_GLM <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM)
# print(paste("La precisión (Accuracy) es:", round(accuracy_GLM,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_GLM <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_GLM,2)))
# 
# # Especificidad (TNR)
# specificity_GLM <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1])
# print(paste("la especificidad es:", round(specificity_GLM,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_GLM <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ])
# print(paste("el VPP es:", round(ppv_GLM,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_GLM <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ])
# print(paste("el VPN es: ",  round(npv_GLM,2)))


# Predicciones probabilísticas con el modelo GLM
prob_predGLM <- predict(GLM_matrix, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```


```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_GLM<- kappa2(cbind(predGLM,y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLM$value, 3)))
# kappa_GLM<-round(kappa_GLM$value, 3)

# Carga los paquetes necesarios
library(caret)
accuracy_GLM <- GLM_sincorreccion$overall["Accuracy"]
kappa_GLM <- GLM_sincorreccion$overall["Kappa"]
# Métricas por clase
sensitivity_GLM <- GLM_sincorreccion$byClass["Sensitivity"]
specificity_GLM <- GLM_sincorreccion$byClass["Specificity"]
precision_GLM <- GLM_sincorreccion$byClass["Pos Pred Value"]
recall_GLM <- GLM_sincorreccion$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_GLM <- GLM_sincorreccion$byClass["F1"]
npv_GLM <- GLM_sincorreccion$byClass["Neg Pred Value"]
prevalence_GLM <- GLM_sincorreccion$byClass["Prevalence"]
detection_rate_GLM <- GLM_sincorreccion$byClass["Detection Rate"]
balanced_accuracy_GLM <- GLM_sincorreccion$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_GLM <- sensitivity_GLM / (1 - specificity_GLM)
LR_minus_GLM <- (1 - sensitivity_GLM) / specificity_GLM

# Para manejar valores especiales
LR_plus_GLM <- ifelse(is.nan(LR_plus_GLM) | is.infinite(LR_plus_GLM), NA, LR_plus_GLM)
LR_minus_GLM <- ifelse(is.nan(LR_minus_GLM) | is.infinite(LR_minus_GLM), NA, LR_minus_GLM)

# Crear un dataframe con todas las métricas
metrics_GLM_matrix_total <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLM_matrix_total = c(accuracy_GLM, kappa_GLM, sensitivity_GLM, specificity_GLM, precision_GLM, 
            f1_score_GLM, npv_GLM, prevalence_GLM, detection_rate_GLM, 
            balanced_accuracy_GLM, LR_plus_GLM, LR_minus_GLM, AUC_GLM))

# Mostrar los resultados
print(metrics_GLM_matrix_total)

```


# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN<-predict(KNN_matrix, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN)

prediccionesKNN <- factor(as.character(prediccionesKNN), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesKNN <- factor(prediccionesKNN, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))

```


```{r}
# confusion_matrixKNN <- table(prediccionesKNN, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics <- caret::confusionMatrix(prediccionesKNN, y_test, positive = "Cov.Pos")
```

```{r}
# #metricas
# # Precisión
# accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
# print(accuracy_KNN)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[, 2])
# print(sensitivity_KNN)
# # Precisión negativa (VN) o especificidad
# specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[, 1])
# specificity_KNN

# 
# # Valor predictivo positivo (VPP)
# ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
# print(ppv_KNN)
# # Valor predictivo negativo (VPN)
# npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
# print(npv_KNN)



prob_predKNN <- predict(KNN_matrix, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNN))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNN[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```


```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_KNN <- kappa2(cbind(prediccionesKNN, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNN$value, 3)))
# kappa_KNN<-round(kappa_KNN$value, 3)
# Carga los paquetes necesarios
library(caret)
accuracy_KNN <- KNN_metrics$overall["Accuracy"]
kappa_KNN <- KNN_metrics$overall["Kappa"]
# Métricas por clase
sensitivity_KNN <- KNN_metrics$byClass["Sensitivity"]
specificity_KNN <- KNN_metrics$byClass["Specificity"]
precision_KNN <- KNN_metrics$byClass["Pos Pred Value"]
recall_KNN <- KNN_metrics$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_KNN <- KNN_metrics$byClass["F1"]
npv_KNN <- KNN_metrics$byClass["Neg Pred Value"]
prevalence_KNN <- KNN_metrics$byClass["Prevalence"]
detection_rate_KNN <- KNN_metrics$byClass["Detection Rate"]
balanced_accuracy_KNN <- KNN_metrics$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_KNN <- sensitivity_KNN / (1 - specificity_KNN)
LR_minus_KNN <- (1 - sensitivity_KNN) / specificity_KNN

# Para manejar valores especiales
LR_plus_KNN <- ifelse(is.nan(LR_plus_KNN) | is.infinite(LR_plus_KNN), NA, LR_plus_KNN)
LR_minus_KNN <- ifelse(is.nan(LR_minus_KNN) | is.infinite(LR_minus_KNN), NA, LR_minus_KNN)

# Crear un dataframe con todas las métricas
metrics_knn_matrix_total <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  knn_matrix_total = c(accuracy_KNN, kappa_KNN, sensitivity_KNN, specificity_KNN, precision_KNN, 
            f1_score_KNN, npv_KNN, prevalence_KNN, detection_rate_KNN, 
            balanced_accuracy_KNN, LR_plus_KNN, LR_minus_KNN, AUC_KNN))

# Mostrar los resultados
print(metrics_knn_matrix_total)


```



```{r}
Train.rf_matrix  <- as.data.frame(trainData) 

Test.rf_matrix <- as.data.frame(testData)
```

```{r}
y_train <- Train.rf_matrix$Y
x_train <- Train.rf_matrix[, setdiff(names(Train.rf_matrix), "Y")]
y_test <- Test.rf_matrix$Y
x_test <- Test.rf_matrix[, setdiff(names(Test.rf_matrix), "Y")]

# Crear el objeto recipe solo con las variables predictoras
objeto_recipe <- recipe(Y ~ ., data = Train.rf_matrix)

# Preparar el objeto recipe
trained_recipe <- prep(objeto_recipe, training = Train.rf_matrix)

# Aplicar bake solo a las variables predictoras
x_train_baked <- bake(trained_recipe, new_data = x_train)
x_test_baked <- bake(trained_recipe, new_data = x_test)

# Volver a juntar Y con los datos procesados
Train.rf_matrix <- cbind(x_train_baked, Y = y_train)
Test.rf_matrix <- cbind(x_test_baked, Y = y_test)

```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

x <- Train.rf_matrix[, -ncol(Train.rf_matrix)] # se incluyen todas las columnas excepto la última

# if(ncol(x) <= 7){
#   
#   mtry <- c(1, 2, 3)
#   
# } else {
#   
#   mtry <- c(1, 2, smatrix(4, ncol(x) * 0.8, 2))
#   
# }
mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = smatrix(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_ranger <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_matrix$Y <- factor(as.numeric(factor(Train.rf_matrix$Y)))
Train.rf_matrix$Y <- factor(Train.rf_matrix$Y, levels = c("1", "2"))
levels(Train.rf_matrix$Y) <- make.names(levels(Train.rf_matrix$Y))

```


```{r}
class(Train.rf_matrix)
```


```{r}
Train.rf_matrix <- as.data.frame(Train.rf_matrix)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_matrix <- as.data.frame(Train.rf_matrix)

# Convertimos Y a factor
Train.rf_matrix$Y <- as.factor(Train.rf_matrix$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_matrix <- caret::train(Y ~ .,
                data = Train.rf_matrix, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```




```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_matriz <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_matriz[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_matrix,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}
```





```{r}

lengths_list <- list(
  num_trees = length(num_trees_range),
  Accuracy = length(sapply(modelos_matriz, function(m) max(m$results$Accuracy, na.rm = TRUE))),
  Kappa = length(sapply(modelos_matriz, function(m) max(m$results$Kappa, na.rm = TRUE))),
  mtry = length(sapply(modelos_matriz, function(m) m$bestTune$mtry)),
  min.node.size = length(sapply(modelos_matriz, function(m) m$bestTune$min.node.size))
)

print(lengths_list)

  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_matriz, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_matriz, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_matriz, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_matriz, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```


```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_matriz[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```

```{r}
# ranger_matriz <- caret::train(
#   Y ~ .,
#   data = Train.rf_matrix, 
#   method = "ranger",
#   tuneGrid = data.frame(mtry = 3, min.node.size = 7, splitrule = "gini"), 
#   metric = "Accuracy",
#   importance = "impurity",
#   trControl = cross_val,  # Si querés usar validación cruzada
#   num.trees = 500  # Número de árboles fijo
# )
# print(ranger_matriz)


ranger_matrix_fx<-function(df_train, model, grid, metrica,  num.trees){

ranger_matrix <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = grid,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_matrix)
}

#

ranger_matrix<-ranger_matrix_fx(Train.rf_matrix,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)
```


```{r}
Test.rf_matrix$Y<-as.factor(Test.rf_matrix$Y)
str(Test.rf_matrix$Y)
```

```{r}
testRF_NOID_matrix <- Test.rf_matrix[, -which(names(Test.rf_matrix) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_matrix<-predict(results_matrix, newdata = testRF_NOID_matrix)#matriz binarizada
print(predRANGER_matrix)


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y

prob_predRanger_matrix <- predict(results_matrix, newdata = testRF_NOID_matrix, type = "prob")
predRANGER_matrix <- factor(as.character(predRANGER_matrix), 
                             levels = c("X1", "X2"),
                             labels = c("Cov.Neg", "Cov.Pos"))



colnames(prob_predRanger_matrix)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_matrix$Y)



```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_matrix[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_matrix <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_matrix, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

RANGER_metrics_sincorreccion <- caret::confusionMatrix(predRANGER_matrix, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_RANGER_matrix <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_matrix,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RANGER_matrix <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RANGER_matrix,2)))
# 
# # Especificidad (TNR)
# specificity_RANGER_matrix <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
# print(paste("la especificidad es:", round(specificity_RANGER_matrix,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RANGER_matrix<- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
# print(paste("el VPP es:", round(ppv_RANGER_matrix,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RANGER_matrix<- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
# print(paste("el VPN es: ",  round(npv_RANGER_matrix,2)))


# Carga los paquetes necesarios
library(caret)
accuracy_RANGER <- RANGER_metrics_sincorreccion$overall["Accuracy"]
kappa_RANGER<- RANGER_metrics_sincorreccion$overall["Kappa"]
# Métricas por clase
sensitivity_RANGER <- RANGER_metrics_sincorreccion$byClass["Sensitivity"]
specificity_RANGER <- RANGER_metrics_sincorreccion$byClass["Specificity"]
precision_RANGER <- RANGER_metrics_sincorreccion$byClass["Pos Pred Value"]
recall_RANGER <- RANGER_metrics_sincorreccion$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RANGER <- RANGER_metrics_sincorreccion$byClass["F1"]
npv_RANGER <- RANGER_metrics_sincorreccion$byClass["Neg Pred Value"]
prevalence_RANGER <- RANGER_metrics_sincorreccion$byClass["Prevalence"]
detection_rate_RANGER <- RANGER_metrics_sincorreccion$byClass["Detection Rate"]
balanced_accuracy_RANGER <- RANGER_metrics_sincorreccion$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RANGER <- sensitivity_RANGER / (1 - specificity_RANGER)
LR_minus_RANGER <- (1 - sensitivity_RANGER / specificity_RANGER)

# Para manejar valores especiales
LR_plus_RANGER <- ifelse(is.nan(LR_plus_RANGER) | is.infinite(LR_plus_RANGER), NA, LR_plus_RANGER)
LR_minus_RANGER <- ifelse(is.nan(LR_minus_RANGER) | is.infinite(LR_minus_RANGER), NA, LR_minus_RANGER)

# Crear un dataframe con todas las métricas
metrics_RANGER_matrix_tot <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RANGER_matrix_tot = c(accuracy_RANGER, kappa_RANGER, sensitivity_RANGER, specificity_RANGER, precision_RANGER, 
            f1_score_RANGER, npv_RANGER, prevalence_RANGER, detection_rate_RANGER, 
            balanced_accuracy_RANGER, LR_plus_RANGER, LR_minus_RANGER, AUC_Ranger_matrix))

# Mostrar los resultados
print(metrics_RANGER_matrix_tot)

```



```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_RANGERmatrix <- kappa2(cbind(predRANGER_matrix, Test.rf_matrix$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RANGERmatrix$value, 3)))
# kappa_RANGERmatrix<-round(kappa_RANGERmatrix$value, 3)
```

# Modelos desde matriz dicotomizada sin corregir por batch


```{r}

featureMatrix_bind<-as.data.frame(featureMatrix_bind)
featureMatrix_bind$Y <- as.factor(featureMatrix_bind$Y)
```


```{r}
# featureMatrix_bind_dic_tot<-featureMatrix_bind[1:542,]
# ciegoMatrix_dico_tot<-featureMatrix_bind[543:694,]
```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_bind, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_matriz_dico <- training(split_data)
testData_matriz_dico <- testing(split_data)

```

```{r}
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
trainData_matriz_dico$Y<-as.factor(trainData_matriz_dico$Y)
str(trainData_matriz_dico)
```

```{r}
train_subset_matriz <- trainData_matriz_dico[, 1:(ncol(trainData_matriz_dico) - 1)]
```



```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(train_subset_matriz, trainData_matriz_dico$Y)
train_subset_matriz <- dichotomize(train_subset_matriz, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA

```


```{r}
trainData_matriz_dico<- cbind(train_subset_matriz, trainData_matriz_dico$Y)#1=neg,2=pos
```



```{r}
str(trainData_matriz_dico)
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
```



```{r}
library(dplyr)
trainData_matriz_dico <- trainData_matriz_dico %>% rename(Y = ncol(trainData_matriz_dico))
str(trainData_matriz_dico$Y)
```


```{r}
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
trainData_matriz_dico$Y<-as.factor(trainData_matriz_dico$Y)
str(trainData_matriz_dico)
```



```{r}
zero_var_indices <- nearZeroVar(trainData_matriz_dico[1:(ncol(trainData_matriz_dico) - 1)])
if (length(zero_var_indices) > 0) {
    trainData_matriz_dico <- trainData_matriz_dico[, -zero_var_indices]
}

```



```{r}

thr_filtered <- thr[colnames(trainData_matriz_dico[1:(ncol(trainData_matriz_dico) - 1)])]
```


quedan 42 variables


```{r}
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)

str(testData_matriz_dico)
```



```{r}
# library(dplyr)
# testData_equipo_dico <- testData_equipo_dico %>% rename(Y = label)
# str(testData_equipo_dico$Y)
```



```{r}
test_subset_matriz <- testData_matriz_dico[, 1:(ncol(testData_matriz_dico) - 1)]
```


```{r}
library(dplyr)

str(testData_matriz_dico$Y)
```



```{r}
test_subset_matriz <- test_subset_matriz[, colnames(trainData_matriz_dico)[1:(ncol(trainData_matriz_dico) - 1)]]
#test_subset <-  test_subset[, colnames(trainData_equipo_dico)]
thr_filtered <-  thr[colnames(trainData_matriz_dico)[1:(ncol(trainData_matriz_dico) - 1)]]
#Dicotimizacion de la matriz de intensidad
test_subset_matriz <- dichotomize(test_subset_matriz, thr_filtered) 

```


```{r}
testData_matriz_dico<- cbind(test_subset_matriz, testData_matriz_dico$Y)#1=neg,2=pos
```

```{r}
library(dplyr)
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico <- testData_matriz_dico %>% rename(Y = ncol(testData_matriz_dico))
str(testData_matriz_dico$Y)
```

```{r}
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)
str(testData_matriz_dico)
str(testData_matriz_dico)
```





```{r}
# Ver la distribución de clases en ambos conjuntos
table(trainData_matriz_dico$Y)
table(testData_matriz_dico$Y)


```



```{r}
library(caret)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



# Random forest


```{r}
hiperparametros <- expand.grid(mtry = c(2, 5, 10))

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```

```{r}
trainData_matriz_dico$Y <- factor(trainData_matriz_dico$Y)  # Asegura que Y sea un factor
levels(trainData_matriz_dico$Y) <- make.names(levels(trainData_matriz_dico$Y))  # Corrige los nombres

library(caret)

# RFd <- caret::train(Y ~ ., data = trainData_matriz_dico, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación




RF_matrix_dico_fx <- function(df_train, model, grid, metrica = "Accuracy",  control) {

  
  # Entrenar el modelo
  RF_matrix_dico <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_rf
  )
  
  # Mostrar resumen
  print(RF_matrix_dico)
  plot(RF_matrix_dico)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF_matrix_dico)
}

RF_matrix_dico<-RF_matrix_dico_fx(df_train=trainData_matriz_dico, model="rf", grid=hiperparametros, metrica="Accuracy" )
print(RF_matrix_dico)
```



```{r}
RF_matrix_dico
```

```{r}
plot(RF_matrix_dico)
```


# SVM

```{r}
# SVMd <- caret::train(Y ~., data = trainData_matriz_dico, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10, prob.model = TRUE)
# # Print the best tuning parameter sigma and C that maximizes model accuracy
# SVMd$bestTun    #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

#SVMd
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```




```{r}
# plot(SVMd)
```

# KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_knn <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```

```{r}
#tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
# KNNd <- caret::train(formula, 
#                     data = trainData_matriz_dico, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNNd

KNN_matrix_dico_fx <- function(df_train, model, grid, metrica = "Accuracy", control, preProcess) {

  
  # Entrenar el modelo
  KNN_matrix_dico <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_knn,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNN_matrix_dico)
  plot(KNN_matrix_dico)
  
  # Guardar resultado
  #save(KNN_matrix_dico, file = paste0(title, ".rda"))
  
  return(KNN_matrix_dico)
}

KNN_matrix_dico<-KNN_matrix_dico_fx(df_train=trainData_matriz_dico, model="knn", grid=tuneGrid, metrica="Accuracy", preProcess = c("center","scale") )
print(KNN_matrix_dico)
```

```{r}
plot(KNN_matrix_dico)
```

# GLMNET


```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)
max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_glm <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}

#tuneGrid <- expand.grid(
#  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
#  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
#)
#set.seed(42)
# GLMd <- caret::train(formula, 
#                   data = trainData_matriz_dico,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLMd 
GLM_matrix_dico_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  GLM_matrix_dico <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_glm
  )
  
  # Mostrar resumen
  print(GLM_matrix_dico)
  plot(GLM_matrix_dico)
  
  # Guardar resultado
  #save(GLM_equipo, file = paste0(title, ".rda"))
  
  return(GLM_matrix_dico)
}

GLM_matrix_dico<-GLM_matrix_dico_fx(df_train=trainData_matriz_dico, model="glmnet", grid=tuneGrid, metrica="Accuracy", title = "Primera_funcion" )
print(GLM_matrix_dico)
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_matrix_dico$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_matrix_dico$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_matrix_dico$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# Prediccion

```{r}
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)
str(testData_matriz_dico$Y)
```

```{r}
testData_NOID_matriz <- testData_matriz_dico[, -which(names(testData_matriz_dico) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRFd<-predict(RF_matrix_dico, newdata = testData_NOID_matriz)#matriz binarizada
print(prediccionesRFd)

prediccionesRFd <- factor(as.character(prediccionesRFd), 
                             levels = c("X1", "X2"),
                             labels = c("Cov.Neg", "Cov.Pos"))




# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_matriz_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixRF <- table(prediccionesRFd, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRF)
RF_metrics_dico <- caret::confusionMatrix(prediccionesRFd, y_test, positive = "Cov.Pos")
```

```{r}
# # Precisión (Accuracy)
# accuracy_RFd <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RFd,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RFd <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RFd,2)))
# 
# # Especificidad (TNR)
# specificity_RFd <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
# print(paste("la especificidad es:", round(specificity_RFd,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RFd <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
# print(paste("el VPP es:", round(ppv_RFd,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RFd <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
# print(paste("el VPN es: ",  round(npv_RFd,2)))
# 


library(pROC)
prob_predRFd <- predict(RF_matrix_dico, newdata = testData_NOID, type = "prob")

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRFd[, "X2"], levels = c(1, 2))
AUC_RFd <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RFd, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_RFd_dic <- kappa2(cbind(prediccionesRFd, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RFd_dic$value, 3)))
# kappa_RFd_dic<-round(kappa_RFd_dic$value, 3)
library(caret)
accuracy_RF_dico <- RF_metrics_dico$overall["Accuracy"]
kappa_RF_dico <- RF_metrics_dico$overall["Kappa"]
# Métricas por clase
sensitivity_RF_dico <- RF_metrics_dico$byClass["Sensitivity"]
specificity_RF_dico <- RF_metrics_dico$byClass["Specificity"]
precision_RF_dico <- RF_metrics_dico$byClass["Pos Pred Value"]
recall_RF_dico <- RF_metrics_dico$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RF_dico <- RF_metrics_dico$byClass["F1"]
npv_RF_dico <- RF_metrics_dico$byClass["Neg Pred Value"]
prevalence_RF_dico <- RF_metrics_dico$byClass["Prevalence"]
detection_rate_RF_dico <- RF_metrics_dico$byClass["Detection Rate"]
balanced_accuracy_RF_dico <- RF_metrics_dico$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RF_dico <- sensitivity_RF_dico / (1 - specificity_RF_dico)
LR_minus_RF_dico <- (1 - sensitivity_RF_dico) / specificity_RF_dico

# Para manejar valores especiales
LR_plus_RF_dico <- ifelse(is.nan(LR_plus_RF_dico) | is.infinite(LR_plus_RF_dico), NA, LR_plus_RF_dico)
LR_minus_RF_dico <- ifelse(is.nan(LR_minus_RF_dico) | is.infinite(LR_minus_RF_dico), NA, LR_minus_RF_dico)

# Crear un dataframe con todas las métricas
metrics_rf_dico_matrix_tot <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
rf_dico_matrix_tot = c(accuracy_RF_dico, kappa_RF_dico, sensitivity_RF_dico, specificity_RF_dico, precision_RF_dico, 
            f1_score_RF_dico, npv_RF_dico, prevalence_RF_dico, detection_rate_RF_dico, 
            balanced_accuracy_RF_dico, LR_plus_RF_dico, LR_minus_RF_dico, AUC_RFd))

# Mostrar los resultados
print(metrics_rf_dico_matrix_tot)

```


# Prediccion GBM

```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
# print(prediccionesGBM)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
# confusion_matrixGBM <- table(prediccionesGBM, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGBM)

```


```{r}
# Precisión (Accuracy)
# accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
# print(paste("La precisión (Accuracy) es:", round(accuracy_GBM,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_GBM,2)))
# 
# # Especificidad (TNR)
# specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
# print(paste("la especificidad es:", round(specificity_GBM,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
# print(paste("el VPP es:", round(ppv_GBM,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
# print(paste("el VPN es: ",  round(npv_GBM,2)))

```

#20.3 Prediccion SVM

```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# prediccionesSVM<-predict(SVMd, newdata = testData_matriz_dico)#matriz binarizada
# print(prediccionesSVM)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData_matriz_dico$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
# confusion_matrixSVM <- table(prediccionesSVM, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixSVM)

```

```{r}
#metricas
# # Precisión
# accuracy_SVMd <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
# print(accuracy_SVMd)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_SVMd <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
# print(sensitivity_SVMd)
# # Precisión negativa (VN) o especificidad
# specificity_SVMd <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
# specificity_SVMd
# 
# # Valor predictivo positivo (VPP)
# ppv_SVMd <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
# print(ppv_SVMd)
# # Valor predictivo negativo (VPN)
# npv_SVMd <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
# print(npv_SVMd)

# 
# 
# prob_predSVMd <- predict(SVMd, newdata = testData_NOID, type = "prob")
# 
# 
# # Verifica los nombres de las columnas
# print(colnames(prob_predSVMd))  # Debería mostrar "1" y "2"
# 
# # Calcula el AUC usando la columna correspondiente a "2" (Cov.Pos)
# library(pROC)
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, predictor = prob_predSVMd[, "2"], levels = c(1, 2))
# AUC_SVMd <- auc(roc_curve)
# 
# print(paste("El valor de AUC es:", round(AUC_SVMd, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNNd<-predict(KNN_matrix_dico, newdata = testData_matriz_dico)#matriz binarizada
print(prediccionesKNNd)

prediccionesKNNd <- factor(as.character(prediccionesKNNd), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
prediccionesKNNd <- factor(prediccionesKNNd, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))



# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_matriz_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixKNN <- table(prediccionesKNNd, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_dico <- caret::confusionMatrix(prediccionesKNNd, y_test, positive = "Cov.Pos")
```

```{r}
# #metricas
# # Precisión
# accuracy_KNNd <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
# print(accuracy_KNNd)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_KNNd <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
# print(sensitivity_KNNd)
# # Precisión negativa (VN) o especificidad
# specificity_KNNd <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
# specificity_KNNd
# 
# # Valor predictivo positivo (VPP)
# ppv_KNNd <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
# print(ppv_KNNd)
# # Valor predictivo negativo (VPN)
# npv_KNNd <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
# print(npv_KNNd)


prob_predKNNd <- predict(KNN_matrix_dico, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNNd))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNd[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNNd <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNNd, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```


```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_KNN_dic <- kappa2(cbind(prediccionesKNNd, y_test))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNN_dic$value, 3)))
# kappa_KNN_dic<-round(kappa_KNN_dic$value, 3)
# Carga los paquetes necesarios
library(caret)
accuracy_KNN_dico <- KNN_metrics_dico$overall["Accuracy"]
kappa_KNN_dico <- KNN_metrics_dico$overall["Kappa"]
# Métricas por clase
sensitivity_KNN_dico <- KNN_metrics_dico$byClass["Sensitivity"]
specificity_KNN_dico <- KNN_metrics_dico$byClass["Specificity"]
precision_KNN_dico <- KNN_metrics_dico$byClass["Pos Pred Value"]
recall_KNN_dico <- KNN_metrics_dico$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_KNN_dico <- KNN_metrics_dico$byClass["F1"]
npv_KNN_dico <- KNN_metrics_dico$byClass["Neg Pred Value"]
prevalence_KNN_dico <- KNN_metrics_dico$byClass["Prevalence"]
detection_rate_KNN_dico <- KNN_metrics_dico$byClass["Detection Rate"]
balanced_accuracy_KNN_dico <- KNN_metrics_dico$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_KNN_dico <- sensitivity_KNN_dico / (1 - specificity_KNN_dico)
LR_minus_KNN_dico <- (1 - sensitivity_KNN_dico) / specificity_KNN_dico

# Para manejar valores especiales
LR_plus_KNN_dico <- ifelse(is.nan(LR_plus_KNN_dico) | is.infinite(LR_plus_KNN_dico), NA, LR_plus_KNN_dico)
LR_minus_KNN_dico <- ifelse(is.nan(LR_minus_KNN_dico) | is.infinite(LR_minus_KNN_dico), NA, LR_minus_KNN_dico)

# Crear un dataframe con todas las métricas
metrics_KNN_dico_matrix_tot <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNN_dico_matrix_tot = c(accuracy_KNN_dico, kappa_KNN_dico, sensitivity_KNN_dico, specificity_KNN_dico, precision_KNN_dico, 
            f1_score_KNN_dico, npv_KNN_dico, prevalence_KNN_dico, detection_rate_KNN_dico, 
            balanced_accuracy_KNN_dico, LR_plus_KNN_dico, LR_minus_KNN_dico, AUC_KNNd))

# Mostrar los resultados
print(metrics_KNN_dico_matrix_tot)

```


# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLMd<-predict(GLM_matrix_dico, newdata = testData_matriz_dico)#matriz binarizada
print(predGLMd)

predGLMd <- factor(as.character(predGLMd), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLMd <- factor(predGLMd, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))



# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_matriz_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixGLMd <- table(predGLMd, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLMd)
GLM_metrics_dico <- caret::confusionMatrix(predGLMd, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_GLMd <- sum(diag(confusion_matrixGLMd)) / sum(confusion_matrixGLMd)
# print(paste("La precisión (Accuracy) es:", round(accuracy_GLMd,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_GLMd <- confusion_matrixGLMd[2, 2] / sum(confusion_matrixGLMd[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_GLMd,2)))
# 
# # Especificidad (TNR)
# specificity_GLMd <- confusion_matrixGLMd[1, 1] / sum(confusion_matrixGLMd[, 1])
# print(paste("la especificidad es:", round(specificity_GLMd,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_GLMd <- confusion_matrixGLMd[2, 2] / sum(confusion_matrixGLMd[2, ])
# print(paste("el VPP es:", round(ppv_GLMd,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_GLMd<- confusion_matrixGLMd[1, 1] / sum(confusion_matrixGLMd[1, ])
# print(paste("el VPN es: ",  round(npv_GLMd,2)))




# Predicciones probabilísticas con el modelo GLM
prob_predGLMd <- predict(GLM_matrix_dico, newdata = testData_NOID, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLMd))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLMd[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLMd <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```


```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_GLMd <- kappa2(cbind(predGLMd, testData_fecha_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLMd$value, 3)))
# kappa_GLMd<-round(kappa_GLMd$value, 3)
library(caret)
accuracy_GLM_dico <- GLM_metrics_dico$overall["Accuracy"]
kappa_GLM_dico <- GLM_metrics_dico$overall["Kappa"]
# Métricas por clase
sensitivity_GLM_dico <- GLM_metrics_dico$byClass["Sensitivity"]
specificity_GLM_dico <- GLM_metrics_dico$byClass["Specificity"]
precision_GLM_dico <- GLM_metrics_dico$byClass["Pos Pred Value"]
recall_GLM_dico<- GLM_metrics_dico$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_GLM_dico <- GLM_metrics_dico$byClass["F1"]
npv_GLM_dico <- GLM_metrics_dico$byClass["Neg Pred Value"]
prevalence_GLM_dico <- GLM_metrics_dico$byClass["Prevalence"]
detection_rate_GLM_dico <- GLM_metrics_dico$byClass["Detection Rate"]
balanced_accuracy_GLM_dico <- GLM_metrics_dico$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_GLM_dico <- sensitivity_GLM_dico / (1 - specificity_GLM_dico)
LR_minus_GLM_dico <- (1 - sensitivity_GLM_dico) / specificity_GLM_dico

# Para manejar valores especiales
LR_plus_GLM_dico <- ifelse(is.nan(LR_plus_GLM_dico) | is.infinite(LR_plus_GLM_dico), NA, LR_plus_GLM_dico)
LR_minus_GLM_dico <- ifelse(is.nan(LR_minus_GLM_dico) | is.infinite(LR_minus_GLM_dico), NA, LR_minus_GLM_dico)

# Crear un dataframe con todas las métricas
metrics_GLM_dico_matrix_tot <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLM_dico_matrix_tot = c(accuracy_GLM_dico, kappa_GLM_dico, sensitivity_GLM_dico, specificity_GLM_dico, precision_GLM_dico, 
            f1_score_GLM_dico, npv_GLM_dico, prevalence_GLM_dico, detection_rate_GLM_dico, 
            balanced_accuracy_GLM_dico, LR_plus_GLM_dico, LR_minus_GLM_dico, AUC_GLMd))

# Mostrar los resultados
print(metrics_GLM_dico_matrix_tot)






```

#ranger


```{r}
library(keras)
library(tensorflow)
library(reticulate)
library(caret)
```


```{r}
Train.rf_matrix_dico  <- as.data.frame(trainData_matriz_dico) 

Test.rf_matrix_dico <- as.data.frame(testData_matriz_dico)
```

```{r}
# objeto_recipe <- recipe(formula = Y ~ .,
#                         data =  Train.rf_matrix_dico)
# 
# 
# trained_recipe <- prep(objeto_recipe, training = Train.rf_matrix_dico)
# 
# Train.rf_matrix_dico <- bake(trained_recipe, new_data = Train.rf_matrix_dico)
# Test.rf_matrix_dico  <- bake(trained_recipe, new_data = Test.rf_matrix_dico)
```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

x <- Train.rf_matrix_dico[, -ncol(Train.rf_matrix_dico)] # se incluyen todas las columnas excepto la última

# if(ncol(x) <= 7){
#   
#   mtry <- c(1, 2, 3)
#   
# } else {
#   
#   mtry <- c(1, 2, smatrix(4, ncol(x) * 0.8, 2))
#   
# }
mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = smatrix(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}

cross_val_ranger <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_matrix_dico$Y <- factor(as.numeric(factor(Train.rf_matrix_dico$Y)))
Train.rf_matrix_dico$Y <- factor(Train.rf_matrix_dico$Y, levels = c("1", "2"))
levels(Train.rf_matrix_dico$Y) <- make.names(levels(Train.rf_matrix_dico$Y))

```





```{r}
class(Train.rf_matrix_dico)
```


```{r}
Train.rf_matrix_dico <- as.data.frame(Train.rf_matrix_dico)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_matrix_dico <- as.data.frame(Train.rf_matrix_dico)

# Convertimos Y a factor
Train.rf_matrix_dico$Y <- as.factor(Train.rf_matrix_dico$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_matrix_dico <- caret::train(Y ~ .,
                data = Train.rf_matrix_dico, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  # Aquí estaba el error, había texto adicional

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```




```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_matrix_dico <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_matrix_dico[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_matrix_dico,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}

```



```{r}
  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_matrix_dico, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_matrix_dico, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_matrix_dico, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_matrix_dico, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```

```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_matrix_dico[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```


```{r}

ranger_matrix_dico_fx<-function(df_train, model, grid, metrica, title, num.trees){

ranger_matrix_dico <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = grid,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_matrix_dico)
}



ranger_matrix_dico<-ranger_matrix_dico_fx(Train.rf_matrix_dico,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)
```


```{r}
Test.rf_matrix_dico$Y<-as.factor(Test.rf_matrix_dico$Y)
str(Test.rf_matrix_dico$Y)
```

```{r}
testRF_NOID_matrix_dico <- Test.rf_matrix_dico[, -which(names(Test.rf_matrix_dico) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_matrix_dico<-predict(ranger_matrix_dico, newdata = testRF_NOID_matrix_dico)#matriz binarizada
print(predRANGER_matrix_dico)


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y
predRANGER_matrix_dico <- factor(as.character(predRANGER_matrix_dico), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRANGER_matrix_dico <- factor(predRANGER_matrix_dico, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


prob_predRanger <- predict(ranger_matrix_dico, newdata = testRF_NOID_matrix_dico, type = "prob")

colnames(prob_predRanger)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_matrix_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))




```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_matrix_dich <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_matrix_dich, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
# 
# confusion_matrixRANGER <- table(predRANGER_matrix_dico, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRANGER)
RANGER_metrics_dico <- caret::confusionMatrix(predRANGER_matrix_dico, y_test, positive = "Cov.Pos")
```

```{r}
# # Precisión (Accuracy)
# accuracy_RANGER_matrix_dic <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_matrix_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RANGER_matrix_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RANGER_matrix_dic,2)))
# 
# # Especificidad (TNR)
# specificity_RANGER_matrix_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
# print(paste("la especificidad es:", round(specificity_RANGER_matrix_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RANGER_matrix_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
# print(paste("el VPP es:", round(ppv_RANGER_matrix_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RANGER_matrix_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
# print(paste("el VPN es: ",  round(npv_RANGER_matrix_dic,2)))

library(caret)
accuracy_RANGER_dico <- RANGER_metrics_dico$overall["Accuracy"]
kappa_RANGER_dico <- RANGER_metrics_dico$overall["Kappa"]
# Métricas por clase
sensitivity_RANGER_dico <- RANGER_metrics_dico$byClass["Sensitivity"]
specificity_RANGER_dico <- RANGER_metrics_dico$byClass["Specificity"]
precision_RANGER_dico <- RANGER_metrics_dico$byClass["Pos Pred Value"]
recall_RANGER_dico <- RANGER_metrics_dico$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RANGER_dico <- RANGER_metrics_dico$byClass["F1"]
npv_RANGER_dico <- RANGER_metrics_dico$byClass["Neg Pred Value"]
prevalence_RANGER_dico <- RANGER_metrics_dico$byClass["Prevalence"]
detection_rate_RANGER_dico <- RANGER_metrics_dico$byClass["Detection Rate"]
balanced_accuracy_RANGER_dico <- RANGER_metrics_dico$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RANGER_dico <- sensitivity_RANGER_dico / (1 - specificity_RANGER_dico)
LR_minus_RANGER_dico <- (1 - sensitivity_RANGER_dico) / specificity_RANGER_dico

# Para manejar valores especiales
LR_plus_RANGER_dico <- ifelse(is.nan(LR_plus_RANGER_dico) | is.infinite(LR_plus_RANGER_dico), NA, LR_plus_RANGER_dico)
LR_minus_RANGER_dico <- ifelse(is.nan(LR_minus_RANGER_dico) | is.infinite(LR_minus_RANGER_dico), NA, LR_minus_RANGER_dico)

# Crear un dataframe con todas las métricas
metrics_RANGER_dico_matrix_dico <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
 RANGER_dico_matrix_dico = c(accuracy_RANGER_dico, kappa_RANGER_dico, sensitivity_RANGER_dico, specificity_RANGER_dico, precision_RANGER_dico, f1_score_RANGER_dico, npv_RANGER_dico, prevalence_RANGER_dico, detection_rate_RANGER_dico, balanced_accuracy_RANGER_dico, LR_plus_RANGER_dico, LR_minus_RANGER_dico, AUC_Ranger_matrix_dich))

# Mostrar los resultados
print(metrics_RANGER_dico_matrix_dico)


```


```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_RANGERmatrix_dic <- kappa2(cbind(predRANGER_matrix_dico, Test.rf_matrix_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RANGERmatrix_dic$value, 3)))
# kappa_RANGERmatrix_dic<-round(kappa_RANGERmatrix_dic$value, 3)
```


# COMPARACION MODELOS MATRIZ ORIGINAL

```{r}
# resultados_modelos_original <- data.frame(
#    Modelo = c("RF_dic", "GLMC_dic", "KNN_dic", "Ranger_dic",  "RF_matriz", "GLM_matriz", "KNN_matriz", "Ranger_matriz"),
#   Accuracy = c(accuracy_RFd, accuracy_GLMd, accuracy_KNNd, accuracy_RANGER_matrix_dic, accuracy_RF, accuracy_GLM ,  accuracy_KNN, accuracy_RANGER_matrix),
#   
#   Sensibilidad=c(sensitivity_RFd, sensitivity_GLMd, sensitivity_KNNd, sensitivity_RANGER_matrix_dic, sensitivity_RF, sensitivity_GLM,  sensitivity_KNN, sensitivity_RANGER_matrix),
#   
#   Especificidad = c(specificity_RFd, specificity_GLMd, specificity_KNNd, specificity_RANGER_matrix_dic, specificity_RF, specificity_GLM,  specificity_KNN, specificity_RANGER_matrix),
#  
#   AUC= c(AUC_RFd, AUC_GLMd, AUC_KNNd,  AUC_Ranger_matrix_dich, AUC_RF, AUC_GLM, AUC_KNN, AUC_Ranger_matrix),
# 
# Kappa=c(kappa_RFd_dic, kappa_GLMd, kappa_KNN_dic, kappa_RANGERmatrix_dic, kappa_RF, kappa_GLM, kappa_KNN, kappa_RANGERmatrix))
# 
# print(resultados_modelos_original)

```


```{r}

# 
# # Cargar la librería
# library(writexl)
# 
# # Exportar a Excel
# write_xlsx(resultados_modelos_original, "C:/Users/karin/Desktop/MCD/TESIS/resultados_modelos_original.xlsx")
```

