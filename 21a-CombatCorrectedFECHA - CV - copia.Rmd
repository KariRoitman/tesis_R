---
title: "fecha"
author: "Karina Roitman"
date: "2025-01-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```


# Modelos (con datos combat_corrected POR FECHA dicotomizado)

```{r}
combat_corrected_fecha_tot<- cbind(combat_corrected_fecha_tot, Y=Datos_actualizados$PCR.Cov)#1=neg,2=pos
combat_corrected_fecha_tot<-as.data.frame(combat_corrected_fecha_tot)
#combat_corrected_fecha_tot$label <- as.factor(combat_corrected_fecha_tot$label)
```

```{r}
# combat_corrected_fecha_tot_dic<-combat_corrected_fecha_tot[1:542,]
# 
# ciego_combat_corrected_fecha_tot_dic<-combat_corrected_fecha_tot[543:694,]
#combat_corrected_fecha_tot <- combat_corrected_fecha_tot %>% rename(Y = label)
```


```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_fecha_tot, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_fecha_dico <- training(split_data)
testData_fecha_dico <- testing(split_data)

```

```{r}
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
trainData_fecha_dico$Y<-as.factor(trainData_fecha_dico$Y)
str(trainData_fecha_dico)
```

```{r}
train_subset_fecha <- trainData_fecha_dico[, -ncol(trainData_fecha_dico)]
```


```{r}
library(dplyr)
#trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = label)
#trainData_fecha_dico<-trainData_fecha_dico[,-ncol(trainData_fecha_dico)]
str(trainData_fecha_dico$Y)
```


```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(train_subset_fecha, trainData_fecha_dico$Y)
train_subset_fecha <- dichotomize(train_subset_fecha, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA

```


```{r}
trainData_fecha_dico<- cbind(train_subset_fecha, trainData_fecha_dico$Y)#1=neg,2=pos
```



```{r}
str(trainData_fecha_dico)
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
```





```{r}
library(dplyr)
trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = ncol(trainData_fecha_dico))
str(trainData_fecha_dico$Y)
```


```{r}
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
trainData_fecha_dico$Y<-as.factor(trainData_fecha_dico$Y)
str(trainData_fecha_dico)
```



```{r}
zero_var_indices <- caret::nearZeroVar(trainData_fecha_dico[,-ncol(train_subset_fecha)])
#train_subset_fecha <- train_subset_fecha
if (length(zero_var_indices) > 0) {
    trainData_fecha_dico <- trainData_fecha_dico[, -zero_var_indices]
}


```
QUEDAN 41 VARIABLES

```{r}

thr_filtered <- thr[colnames(trainData_fecha_dico[,-ncol(trainData_fecha_dico)])]

```


```{r}
#trainData_fecha_dico <- cbind(train_subset_fecha, trainData_fecha_dico$Y)
```




```{r}
#library(dplyr)
#trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
#trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = V42)
#str(trainData_fecha_dico$Y)
```



```{r}
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
testData_fecha_dico$Y<-as.factor(testData_fecha_dico$Y)

str(testData_fecha_dico)
```



```{r}
library(dplyr)
#testData_fecha_dico <- testData_fecha_dico %>% rename(Y = label)
str(testData_fecha_dico$Y)
```


```{r}
test_subset_fecha <- testData_fecha_dico[, -ncol(testData_fecha_dico)]
```



```{r}
test_subset_fecha <- test_subset_fecha[, colnames(trainData_fecha_dico[,-ncol(trainData_fecha_dico)])]


#test_subset <-  test_subset[, colnames(trainData_equipo_dico)]

#Dicotimizacion de la matriz de intensidad
test_subset_fecha <- dichotomize(test_subset_fecha, thr_filtered) 


```


```{r}
testData_fecha_dico<- cbind(test_subset_fecha, testData_fecha_dico$Y)#1=neg,2=pos
```

```{r}
library(dplyr)
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
testData_fecha_dico <- testData_fecha_dico %>% rename(Y = ncol(testData_fecha_dico))
str(testData_fecha_dico$Y)
```


```{r}
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
testData_fecha_dico$Y<-as.factor(testData_fecha_dico$Y)
str(testData_fecha_dico)
```




```{r}
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
trainData_fecha_dico$Y<-as.factor(trainData_fecha_dico$Y)
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
# Ver la distribución de clases en ambos conjuntos
table(trainData_fecha_dico$Y)
table(testData_fecha_dico$Y)


```






```{r}
library(caret)
#set.seed(42)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```




```{r}
# Submuestras y repeticiones
set.seed(42)

particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(2, 5, 10)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry)


seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}
# genera un vector de nrow(hiperparametros) números aleatorios entre 1 y 500.
#Esto asegura que cada combinación de hiperparámetros tenga una semilla diferente en cada iteración.

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

trainData_fecha_dico$Y <- factor(as.numeric(factor(trainData_fecha_dico$Y)))
trainData_fecha_dico$Y <- factor(trainData_fecha_dico$Y, levels = c("1", "2"))
levels(trainData_fecha_dico$Y) <- make.names(levels(trainData_fecha_dico$Y))

```



# Random forest

```{r}
class(trainData_fecha_dico)
```


```{r}
trainData_fecha_dico$Y <- factor(as.numeric(factor(trainData_fecha_dico$Y)))
trainData_fecha_dico$Y <- factor(trainData_fecha_dico$Y, levels = c("1", "2"))
levels(trainData_fecha_dico$Y) <- make.names(levels(trainData_fecha_dico$Y))
library(caret)
# #control1 <- trainControl(method = "cv", number = 10)
# #grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.
# #set.seed(42)
# rf_fecha_dic <- caret::train(Y ~ ., data = trainData_fecha_dico, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación


RF_fecha_dic_tot_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  RF_fecha_dic_tot <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_rf
  )
  
  # Mostrar resumen
  print(RF_fecha_dic_tot)
  plot(RF_fecha_dic_tot, main = title)
  
  # Guardar resultado
  save(RF_fecha_dic_tot, file = paste0(title, ".rda"))
  
  return(RF_fecha_dic_tot)
}

RF_fecha_dic_tot<-RF_fecha_dic_tot_fx(df_train=trainData_fecha_dico, model="rf", grid=hiperparametros, metrica="Accuracy")
print(RF_fecha_dic_tot)
```

```{r}
RF_fecha_dic_tot$results
```

```{r}
plot(RF_fecha_dic_tot)
```


# ranger


combat_corrected_fecha_tot_DICHO

```{r}
Train.rf_fecha_dico  <- as.data.frame(trainData_fecha_dico) 

Test.rf_fecha_dico <- as.data.frame(testData_fecha_dico)
```

```{r}
y_train <- Train.rf_fecha_dico$Y
x_train <- Train.rf_fecha_dico[, setdiff(names(Train.rf_fecha_dico), "Y")]
y_test <- Test.rf_fecha_dico$Y
x_test <- Test.rf_fecha_dico[, setdiff(names(Test.rf_fecha_dico), "Y")]

# Crear el objeto recipe solo con las variables predictoras
objeto_recipe <- recipe(Y ~ ., data = Train.rf_fecha_dico)

# Preparar el objeto recipe
trained_recipe <- prep(objeto_recipe, training = Train.rf_fecha_dico)

# Aplicar bake solo a las variables predictoras
x_train_baked <- bake(trained_recipe, new_data = x_train)
x_test_baked <- bake(trained_recipe, new_data = x_test)

# Volver a juntar Y con los datos procesados
Train.rf_fecha_dico <- cbind(x_train_baked, Y = y_train)
Test.rf_fecha_dico <- cbind(x_test_baked, Y = y_test)

```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

x <- Train.rf_fecha_dico[, -ncol(Train.rf_fecha_dico)] # se incluyen todas las columnas excepto la última


mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = sfecha(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")


seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_ranger <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_fecha_dico$Y <- factor(as.numeric(factor(Train.rf_fecha_dico$Y)))
Train.rf_fecha_dico$Y <- factor(Train.rf_fecha_dico$Y, levels = c("1", "2"))
levels(Train.rf_fecha_dico$Y) <- make.names(levels(Train.rf_fecha_dico$Y))

```





```{r}
class(Train.rf_fecha_dico)
```


```{r}
Train.rf_fecha_dico <- as.data.frame(Train.rf_fecha_dico)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_fecha_dico <- as.data.frame(Train.rf_fecha_dico)

# Convertimos Y a factor
Train.rf_fecha_dico$Y <- as.factor(Train.rf_fecha_dico$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_ranger_fecha_dic <- caret::train(Y ~ .,
                data = Train.rf_fecha_dico, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  # Aquí estaba el error, había texto adicional

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```


```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_fecha_dico <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_fecha_dico[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_fecha_dico,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}
```





```{r}
  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_fecha_dico, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_fecha_dico, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_fecha_dico, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_fecha_dico, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```


```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_fecha_dico[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```

```{r}
# ranger_fecha_dico <- caret::train(
#   Y ~ .,
#   data = Train.rf_fecha_dico, 
#   method = "ranger",
#   tuneGrid = data.frame(mtry = 2, min.node.size = 3, splitrule = "gini"), 
#   metric = "Accuracy",
#   importance = "impurity",
#   trControl = cross_val,  # Si querés usar validación cruzada
#   num.trees = 500  # Número de árboles fijo
# )
# print(ranger_fecha_dico)




ranger_fecha_dic_fx<-function(df_train, model, grid, metrica,  num.trees){

ranger_fecha_dic <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = grid,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_fecha_dic)
}

#

ranger_fecha_dic<-ranger_fecha_dic_fx(Train.rf_fecha_dico,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)
```



```{r}
Test.rf_fecha_dico$Y<-as.factor(Test.rf_fecha_dico$Y)
str(Test.rf_fecha_dico$Y)
```

```{r}
testRF_NOID_fecha_dico <- Test.rf_fecha_dico[, -which(names(Test.rf_fecha_dico) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_fecha_dic<-predict(ranger_fecha_dic, newdata = testRF_NOID_fecha_dico)#matriz binarizada
print(predRANGER_fecha_dic)


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y
predRANGER_fecha_dic <- factor(predRANGER_fecha_dic, levels = c("X1", "X2"), labels = c("Cov.Neg", "Cov.Pos"))
prob_predRanger_fecha_dic <- predict(ranger_fecha_dic, newdata = testRF_NOID_fecha_dico, type = "prob")

colnames(prob_predRanger_fecha_dic)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_fecha_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))



```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_fecha_dic[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_FECHA_dich <- auc(roc_curve)


print(paste("El valor de AUC es:", round(AUC_Ranger_FECHA_dich, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

Ran_metrics_fecha_dic <- caret::confusionMatrix(predRANGER_fecha_dic, y_test, positive = "Cov.Pos")
# Imprimir la matriz de confusión
print(Ran_metrics_fecha_dic)
```


```{r}
# # Precisión (Accuracy)
# accuracy_RANGER_fecha_dic <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_fecha_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RANGER_fecha_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RANGER_fecha_dic,2)))
# 
# # Especificidad (TNR)
# specificity_RANGER_fecha_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
# print(paste("la especificidad es:", round(specificity_RANGER_fecha_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RANGER_fecha_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
# print(paste("el VPP es:", round(ppv_RANGER_fecha_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RANGER_fecha_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
# print(paste("el VPN es: ",  round(npv_RANGER_fecha_dic,2)))


```



```{r}
# Carga los paquetes necesarios
library(caret)

accuracy_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$overall["Accuracy"]
kappa_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$overall["Kappa"]
# Métricas por clase
sensitivity_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Sensitivity"]
specificity_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Specificity"]
precision_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Pos Pred Value"]
recall_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["F1"]
npv_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Neg Pred Value"]
prevalence_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Prevalence"]
detection_rate_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Detection Rate"]
balanced_accuracy_RAN_FECHA_DIC <- Ran_metrics_fecha_dic$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RAN_FECHA_DIC <- sensitivity_RAN_FECHA_DIC / (1 - specificity_RAN_FECHA_DIC)
LR_minus_RAN_FECHA_DIC <- (1 - sensitivity_RAN_FECHA_DIC) / specificity_RAN_FECHA_DIC

# Para manejar valores especiales
LR_plus_RAN_FECHA_DIC <- ifelse(is.nan(LR_plus_RAN_FECHA_DIC) | is.infinite(LR_plus_RAN_FECHA_DIC), NA, LR_plus_RAN_FECHA_DIC)
LR_minus_RAN_FECHA_DIC <- ifelse(is.nan(LR_minus_RAN_FECHA_DIC) | is.infinite(LR_minus_RAN_FECHA_DIC), NA, LR_minus_RAN_FECHA_DIC)

# Crear un dataframe con todas las métricas
metrics_ran_fecha_dic <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  ran_fecha_dic_tot = c(accuracy_RAN_FECHA_DIC , kappa_RAN_FECHA_DIC , sensitivity_RAN_FECHA_DIC , specificity_RAN_FECHA_DIC , precision_RAN_FECHA_DIC , 
            f1_score_RAN_FECHA_DIC , npv_RAN_FECHA_DIC , prevalence_RAN_FECHA_DIC , detection_rate_RAN_FECHA_DIC , 
            balanced_accuracy_RAN_FECHA_DIC , LR_plus_RAN_FECHA_DIC , LR_minus_RAN_FECHA_DIC , AUC_Ranger_FECHA_dich ))

# Mostrar los resultados
print(metrics_ran_fecha_dic)

```




# KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_knn <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}

# #set.seed(42)
# # Ajustar el modelo KNN
# KNN_fecha_dic <- caret::train(formula, 
#                     data = trainData_fecha_dico, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNN_fecha_dic

KNN_fecha_dico_fx <- function(df_train, model, grid, metrica = "Accuracy", control, preProcess) {

  
  # Entrenar el modelo
  KNN_fecha_dico_tot <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_knn,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNN_fecha_dico_tot)
  plot(KNN_fecha_dico_tot)
  
  # Guardar resultado
  #save(KNN_fecha_dico_tot, file = paste0(title, ".rda"))
  
  return(KNN_fecha_dico_tot)
}

KNN_fecha_dico_tot<-KNN_fecha_dico_fx(df_train=trainData_fecha_dico, model="knn", grid=tuneGrid, metrica="Accuracy", preProcess = c("center","scale") )
print(KNN_fecha_dico_tot)
```

```{r}
plot(KNN_fecha_dico_tot)
```

# GLMNET


```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)
max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_glm <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```



```{r}
# 
# #set.seed(42)
# GLM_fecha_dic<- caret::train(formula, 
#                   data = trainData_fecha_dico,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLM_fecha_dic 
GLM_fecha_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {
  # Entrenar el modelo
  GLM_fecha_dic <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_glm
  )
  
  # Mostrar resumen
  print(GLM_fecha_dic)
  plot(GLM_fecha_dic, main = title)
  
  # Guardar resultado
  #save(GLM_equipo, file = paste0(title, ".rda"))
  
  return(GLM_fecha_dic)
}

GLM_fecha_dic<-GLM_fecha_fx(df_train=trainData_fecha_dico, model="glmnet", grid=tuneGrid, metrica="Accuracy", title = "Primera_funcion" )
print(GLM_fecha_dic)

```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_fecha_dic$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_fecha_dic$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_fecha_dic$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# Predicciones

```{r}
testData_fecha_dico$Y<-as.factor(testData_fecha_dico$Y)
str(testData_fecha_dico$Y)
```

```{r}
testData_NOID_fecha_dico <- testData_fecha_dico[, -which(names(testData_fecha_dico) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRF_fecha_dic<-predict(RF_fecha_dic_tot, newdata = testData_NOID_fecha_dico)#matriz binarizada
print(predRF_fecha_dic)

predRF_fecha_dic <- factor(as.character(predRF_fecha_dic), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRF_fecha_dic <- factor(predRF_fecha_dic, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y

prob_predRF_fechadic <- predict(RF_fecha_dic_tot, newdata = testData_NOID_fecha_dico, type = "prob")

colnames(prob_predRF_fechadic)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(testData_fecha_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))

#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF_fechadic[, "X2"], levels = c(1, 2))
AUC_RF_fecha_dic <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_fecha_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```


```{r}
# confusion_matrixRF <- table(predRF_fecha_dic, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRF)
RF_metrics_fecha_dic <- caret::confusionMatrix(predRF_fecha_dic, y_test, positive = "Cov.Pos")
```

```{r}
# # Precisión (Accuracy)
# accuracyRF_fecha_dic <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
# print(paste("La precisión (Accuracy) es:", round(accuracyRF_fecha_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivityRF_fecha_dic <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
# print(paste("La sensibilidad es:", round(sensitivityRF_fecha_dic,2)))
# 
# # Especificidad (TNR)
# specificityRF_fecha_dic <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
# print(paste("la especificidad es:", round(specificityRF_fecha_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppvRF_fecha_dic <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
# print(paste("el VPP es:", round(ppvRF_fecha_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npvRF_fecha_dic <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
# print(paste("el VPN es: ",  round(npvRF_fecha_dic,2)))


```
```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_RFfecha_dic <- kappa2(cbind(predRF_fecha_dic, testData_fecha_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RFfecha_dic$value, 3)))
# kappa_RFfecha_dic<-round(kappa_RFfecha_dic$value, 3)
```



```{r}
# Carga los paquetes necesarios
library(caret)

accuracy_RF_fecha_dic <- RF_metrics_fecha_dic$overall["Accuracy"]
kappa_RF_fecha_dic <- RF_metrics_fecha_dic$overall["Kappa"]
# Métricas por clase
sensitivity_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Sensitivity"]
specificity_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Specificity"]
precision_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Pos Pred Value"]
recall_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["F1"]
npv_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Neg Pred Value"]
prevalence_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Prevalence"]
detection_rate_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Detection Rate"]
balanced_accuracy_RF_fecha_dic <- RF_metrics_fecha_dic$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RF_fecha_dic<- sensitivity_RF_fecha_dic / (1 - specificity_RF_fecha_dic)
LR_minus_RF_fecha_dic <- (1 - sensitivity_RF_fecha_dic) / specificity_RF_fecha_dic

# Para manejar valores especiales
LR_plus_RF_fecha_dic <- ifelse(is.nan(LR_plus_RF_fecha_dic) | is.infinite(LR_plus_RF_fecha_dic), NA, LR_plus_RF_fecha_dic)
LR_minus_RF_fecha_dic <- ifelse(is.nan(LR_minus_RF_fecha_dic) | is.infinite(LR_minus_RF_fecha_dic), NA, LR_minus_RF_fecha_dic)

# Crear un dataframe con todas las métricas
metrics_RF_fecha_dic <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RF_fecha_dic_tot = c(accuracy_RF_fecha_dic, kappa_RF_fecha_dic, sensitivity_RF_fecha_dic, specificity_RF_fecha_dic, precision_RF_fecha_dic, f1_score_RF_fecha_dic, npv_RF_fecha_dic, prevalence_RF_fecha_dic, detection_rate_RF_fecha_dic, 
            balanced_accuracy_RF_fecha_dic, LR_plus_RF_fecha_dic, LR_minus_RF_fecha_dic, AUC_RF_fecha_dic))

# Mostrar los resultados
print(metrics_RF_fecha_dic)


```


# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_fecha_dic<-predict(GLM_fecha_dic, newdata = testData_NOID_fecha_dico)#matriz binarizada
print(predGLM_fecha_dic)

predGLM_fecha_dic <- factor(as.character(predGLM_fecha_dic), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLM_fecha_dic <- factor(predGLM_fecha_dic, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))

```

```{r}
# confusion_matrixGLM <- table(predGLM_fecha_dic, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLM)
GLM_metrics_fecha <- caret::confusionMatrix(predGLM_fecha_dic, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_GLM_fecha_dic <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM)
# print(paste("La precisión (Accuracy) es:", round(accuracy_GLM_fecha_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_GLM_fecha_dic <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_GLM_fecha_dic,2)))
# 
# # Especificidad (TNR)
# specificity_GLM_fecha_dic <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1])
# print(paste("la especificidad es:", round(specificity_GLM_fecha_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_GLM_fecha_dic <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ])
# print(paste("el VPP es:", round(ppv_GLM_fecha_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_GLM_fecha_dic <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ])
# print(paste("el VPN es: ",  round(npv_GLM_fecha_dic,2)))




# Predicciones probabilísticas con el modelo GLM
prob_predGLM <- predict(GLM_fecha_dic, newdata = testData_NOID_fecha_dico, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha_dico$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_fecha_dic <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_fecha_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```
```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_GLM_fecha_dic <- kappa2(cbind(predGLM_fecha_dic, testData_fecha_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLM_fecha_dic$value, 3)))
# kappa_GLM_fecha_dic<-round(kappa_GLM_fecha_dic$value, 3)
```

```{r}
# Carga los paquetes necesarios
library(caret)
accuracyGLM_fecha_dic <- GLM_metrics_fecha$overall["Accuracy"]
kappaGLM_fecha_dic <- GLM_metrics_fecha$overall["Kappa"]
# Métricas por clase
sensitivityGLM_fecha_dic <- GLM_metrics_fecha$byClass["Sensitivity"]
specificityGLM_fecha_dic <- GLM_metrics_fecha$byClass["Specificity"]
precisionGLM_fecha_dic <- GLM_metrics_fecha$byClass["Pos Pred Value"]
recallGLM_fecha_dic <- GLM_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreGLM_fecha_dic <- GLM_metrics_fecha$byClass["F1"]
npvGLM_fecha_dic <- GLM_metrics_fecha$byClass["Neg Pred Value"]
prevalenceGLM_fecha_dic <- GLM_metrics_fecha$byClass["Prevalence"]
detection_rateGLM_fecha_dic <- GLM_metrics_fecha$byClass["Detection Rate"]
balanced_accuracyGLM_fecha_dic <- GLM_metrics_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusGLM_fecha_dic <- sensitivityGLM_fecha_dic / (1 - specificityGLM_fecha_dic)
LR_minusGLM_fecha_dic <- (1 - sensitivityGLM_fecha_dic) / specificityGLM_fecha_dic

# Para manejar valores especiales
LR_plusGLM_fecha_dic <- ifelse(is.nan(LR_plusGLM_fecha_dic) | is.infinite(LR_plusGLM_fecha_dic), NA, LR_plusGLM_fecha_dic)
LR_minusGLM_fecha_dic <- ifelse(is.nan(LR_minusGLM_fecha_dic) | is.infinite(LR_minusGLM_fecha_dic), NA, LR_minusGLM_fecha_dic)

# Crear un dataframe con todas las métricas
metrics_GLM_fecha_dic <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLM_fecha_dic_tot = c(accuracyGLM_fecha_dic , kappaGLM_fecha_dic , sensitivityGLM_fecha_dic , specificityGLM_fecha_dic , precisionGLM_fecha_dic , f1_scoreGLM_fecha_dic , npvGLM_fecha_dic , prevalenceGLM_fecha_dic , detection_rateGLM_fecha_dic , 
            balanced_accuracyGLM_fecha_dic , LR_plusGLM_fecha_dic , LR_minusGLM_fecha_dic , AUC_GLM_fecha_dic))

# Mostrar los resultados
print(metrics_GLM_fecha_dic)


```


# Prediccion SVM

```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# predSVMC<-predict(SVM_c, newdata = testData_NOID)#matriz binarizada
# print(predSVMC)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada


```

```{r}

# 
# # Predicción de probabilidades
# prob_predSVMC <- predict(SVM_c, newdata = testData_NOID, type = "prob")
# 
# # Verificar el resultado
# print(prob_predSVMC)

```


```{r}
# confusion_matrixSVM <- table(predSVMC, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
# accuracy_SVMC <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
# print(accuracy_SVMC)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_SVMC <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
# print(sensitivity_SVMC)
# # Precisión negativa (VN) o especificidad
# specificity_SVMC <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
# specificity_SVMC
# 
# # Valor predictivo positivo (VPP)
# ppv_SVMC <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
# print(ppv_SVMC)
# # Valor predictivo negativo (VPN)
# npv_SVMC <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
# print(npv_SVMC)
# 
# 
# library(pROC)
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, predictor = prob_predSVMC[, "2"], levels = c(1, 2))
# AUC_SVMC <- auc(roc_curve)
# 
# print(paste("El valor de AUC es:", round(AUC_SVMC, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predKNN_fecha_dic<-predict(KNN_fecha_dico_tot, newdata = testData_NOID_fecha_dico)#matriz binarizada
print(predKNN_fecha_dic)

predKNN_fecha_dic <- factor(as.character(predKNN_fecha_dic), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predKNN_fecha_dic <- factor(predKNN_fecha_dic, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Crea la matriz de confusión usando caret


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixKNN <- table(predKNN_fecha_dic, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_fecha_dic <- caret::confusionMatrix(predKNN_fecha_dic, y_test, positive = "Cov.Pos")

```

```{r}

prob_predKNN_fecha <- predict(KNN_fecha_dico_tot, newdata = testData_NOID_fecha_dico, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNN_fecha))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha_dico$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNN_fecha[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_fecha_dic <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_fecha_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```


```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_KNN_fecha_dic <- kappa2(cbind(predKNN_fecha_dic, testData_fecha_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNN_fecha_dic$value, 3)))
# kappa_KNN_fecha_dic<-round(kappa_KNN_fecha_dic$value, 3)
```


```{r}
# Carga los paquetes necesarios
library(caret)
accuracy_KNN_fecha_dic<- KNN_metrics_fecha_dic$overall["Accuracy"]
kappa_KNN_fecha_dic <- KNN_metrics_fecha_dic$overall["Kappa"]
# Métricas por clase
sensitivity_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Sensitivity"]
specificity_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Specificity"]
precision_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Pos Pred Value"]
recall_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["F1"]
npv_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Neg Pred Value"]
prevalence_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Prevalence"]
detection_rate_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Detection Rate"]
balanced_accuracy_KNN_fecha_dic <- KNN_metrics_fecha_dic$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_KNN_fecha_dic <- sensitivity_KNN_fecha_dic / (1 - specificity_KNN_fecha_dic)
LR_minus_KNN_fecha_dic <- (1 - sensitivity_KNN_fecha_dic) / specificity_KNN_fecha_dic

# Para manejar valores especiales
LR_plus_KNN_fecha_dic <- ifelse(is.nan(LR_plus_KNN_fecha_dic) | is.infinite(LR_plus_KNN_fecha_dic), NA, LR_plus_KNN_fecha_dic)
LR_minus_KNN_fecha_dic <- ifelse(is.nan(LR_minus_KNN_fecha_dic) | is.infinite(LR_minus_KNN_fecha_dic), NA, LR_minus_KNN_fecha_dic)

# Crear un dataframe con todas las métricas
metrics_KNN_fecha_dic <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNN_fecha_dic_tot = c(accuracy_KNN_fecha_dic, kappa_KNN_fecha_dic, sensitivity_KNN_fecha_dic, specificity_KNN_fecha_dic, precision_KNN_fecha_dic, f1_score_KNN_fecha_dic, npv_KNN_fecha_dic, prevalence_KNN_fecha_dic, detection_rate_KNN_fecha_dic,balanced_accuracy_KNN_fecha_dic, LR_plus_KNN_fecha_dic, LR_minus_KNN_fecha_dic, AUC_KNN_fecha_dic))

# Mostrar los resultados
print(metrics_KNN_fecha_dic)






```


# Modelos (con datos combat_corrected pero sin dictomizar)


<!-- # ```{r} -->
<!-- # combat_corrected_fecha_tot<- cbind(combat_corrected_fecha_tot, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos -->
<!-- # ``` -->

```{r}
combat_corrected_fecha_tot<-as.data.frame(combat_corrected_fecha_tot)
combat_corrected_fecha_tot$Y<-as.factor(combat_corrected_fecha_tot$Y)
str(combat_corrected_fecha_tot)
```



```{r}
library(dplyr)
#combat_corrected_fecha_tot<-combat_corrected_fecha_tot[,-ncol(combat_corrected_fecha_tot)]
#combat_corrected_fecha_tot <- combat_corrected_fecha_tot %>% rename(Y = label)
combat_corrected_fecha_tot$Y<-as.factor(combat_corrected_fecha_tot$Y)
str(combat_corrected_fecha_tot$Y)
```
```{r}
# combat_corrected_fecha_tot_nodic<-combat_corrected_fecha_tot[1:542,]
# 
# ciego_combat_corrected_fecha_tot_nodic<-combat_corrected_fecha_tot[543:694,]
```


```{r}
# zero_var_indices <- nearZeroVar(combat_corrected_fecha_tot)
# combat_corrected_fecha_tot <- combat_corrected_fecha_tot
# if (length(zero_var_indices) > 0) {
#     combat_corrected_fecha_tot <- combat_corrected_fecha_tot[, -zero_var_indices$Position]
# }

```


```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_fecha_tot, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_fecha <- training(split_data)
testData_fecha <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData_fecha$Y)
table(testData_fecha$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
#trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
#testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
#colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
#table(trainData$Y)
#table(testData$Y)

```

```{r}
#set.seed(42)
library(caret)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



# Random forest


```{r}
hiperparametros <- expand.grid(mtry = c(2, 5, 10))

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```



```{r}
trainData_fecha$Y <- factor(trainData_fecha$Y)  # Asegura que Y sea un factor
levels(trainData_fecha$Y) <- make.names(levels(trainData_fecha$Y))  # Corrige los nombres
library(caret)

# RF_fecha <- caret::train(Y ~ ., data = trainData_fecha, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación
RF_fecha_tot_fx <- function(df_train, model, grid, metrica = "Accuracy",  control) {

  # Entrenar el modelo
  RF_fecha_tot <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(RF_fecha_tot)
  plot(RF_fecha_tot, main = title)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF_fecha_tot)
}

RF_fecha_tot<-RF_fecha_tot_fx(df_train=trainData_fecha, model="rf", grid=hiperparametros, metrica="Accuracy", control=cross_val_rf)
print(RF_fecha_tot)
```

```{r}
RF_fecha_tot
```

```{r}
plot(RF_fecha_tot)
```


# ranger


```{r}
Train.rf_FECHA  <- as.data.frame(trainData_fecha) 

Test.rf_FECHA <- as.data.frame(testData_fecha)
```


```{r}
library(keras)
library(tensorflow)
library(reticulate)
library(caret)
```


```{r}

# objeto_recipe <- recipe(formula = Y ~ .,
#                         data =  Train.rf_FECHA)
# 
# objeto_recipe <- objeto_recipe %>% 
#   step_nzv(all_predictors())
# 
# trained_recipe <- prep(objeto_recipe, training = Train.rf_FECHA)
# 
# Train.rf_FECHA <- bake(trained_recipe, new_data = Train.rf_FECHA)
# Test.rf_FECHA  <- bake(trained_recipe, new_data = Test.rf_FECHA)
```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = sfecha(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")


seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_FECHA$Y <- factor(as.numeric(factor(Train.rf_FECHA$Y)))
Train.rf_FECHA$Y <- factor(Train.rf_FECHA$Y, levels = c("1", "2"))
levels(Train.rf_FECHA$Y) <- make.names(levels(Train.rf_FECHA$Y))

```





```{r}
class(Train.rf_FECHA)
```


```{r}
Train.rf_FECHA <- as.data.frame(Train.rf_FECHA)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_FECHA <- as.data.frame(Train.rf_FECHA)

# Convertimos Y a factor
Train.rf_FECHA$Y <- as.factor(Train.rf_FECHA$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_FECHA <- caret::train(Y ~ .,
                data = Train.rf_FECHA, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```

```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_fecha <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_fecha[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_FECHA,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}


```





```{r}
  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_fecha, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_fecha, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_fecha, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_fecha, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```


```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_fecha_dico[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```

```{r}
# ranger_fecha <- caret::train(
#   Y ~ .,
#   data = Train.rf_fecha_dico, 
#   method = "ranger",
#   tuneGrid = data.frame(mtry = 3, min.node.size = 1, splitrule = "gini"), 
#   metric = "Accuracy",
#   importance = "impurity",
#   trControl = cross_val,  # Si querés usar validación cruzada
#   num.trees = 500  # Número de árboles fijo
# )
# print(ranger_fecha_dico)

ranger_fecha_tot_fx<-function(df_train, model, grid, metrica,  num.trees){

ranger_fecha_tot <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = grid,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_fecha_tot)
}

#

ranger_fecha_tot<-ranger_fecha_tot_fx(Train.rf_fecha_dico,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)


```


```{r}
Test.rf_FECHA$Y<-as.factor(Test.rf_FECHA$Y)
str(Test.rf_FECHA$Y)
```

```{r}
testRF_NOID_FECHA <- Test.rf_FECHA[, -which(names(Test.rf_FECHA) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_FECHA<-predict(ranger_fecha_tot, newdata = testRF_NOID_FECHA)#matriz binarizada
print(predRANGER_FECHA)
predRANGER_FECHA <- factor(as.character(predRANGER_FECHA), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRANGER_FECHA <- factor(predRANGER_FECHA, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))




# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- Test.rf_FECHA$Y

prob_predRanger_FECHA <- predict(ranger_fecha_tot, newdata = testRF_NOID_FECHA, type = "prob")

colnames(prob_predRanger_FECHA)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_FECHA$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))



```



```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_FECHA[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_FECHA <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_FECHA, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

# confusion_matrixRANGER <- table(predRANGER_FECHA, y_test)
# Imprimir la matriz de confusión
# print(confusion_matrixRANGER)
Ranger_metrics_fecha <- caret::confusionMatrix(predRANGER_FECHA, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_RANGER_fecha <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_fecha,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RANGER_fecha <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RANGER_fecha,2)))
# 
# # Especificidad (TNR)
# specificity_RANGER_fecha <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
# print(paste("la especificidad es:", round(specificity_RANGER_fecha,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RANGER_fecha<- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
# print(paste("el VPP es:", round(ppv_RANGER_fecha,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RANGER_fecha<- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
# print(paste("el VPN es: ",  round(npv_RANGER_fecha,2)))


```



```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_RANGERfecha <- kappa2(cbind(predRANGER_FECHA, Test.rf_FECHA$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RANGERfecha$value, 3)))
# kappa_RANGERfecha<-round(kappa_RANGERfecha$value, 3)
# Carga los paquetes necesarios
library(caret)
accuracyRANGER_FECHA <- Ranger_metrics_fecha$overall["Accuracy"]
kappaRANGER_FECHA <- Ranger_metrics_fecha$overall["Kappa"]
# Métricas por clase
sensitivityRANGER_FECHA <- Ranger_metrics_fecha$byClass["Sensitivity"]
specificityRANGER_FECHA <- Ranger_metrics_fecha$byClass["Specificity"]
precisionRANGER_FECHA <- Ranger_metrics_fecha$byClass["Pos Pred Value"]
recallRANGER_FECHA <- Ranger_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRANGER_FECHA <- Ranger_metrics_fecha$byClass["F1"]
npvRANGER_FECHA <- Ranger_metrics_fecha$byClass["Neg Pred Value"]
prevalenceRANGER_FECHA <- Ranger_metrics_fecha$byClass["Prevalence"]
detection_rateRANGER_FECHA <- Ranger_metrics_fecha$byClass["Detection Rate"]
balanced_accuracyRANGER_FECHA <- Ranger_metrics_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRANGER_FECHA <- sensitivityRANGER_FECHA / (1 - specificityRANGER_FECHA)
LR_minusRANGER_FECHA <- (1 - sensitivityRANGER_FECHA) / specificityRANGER_FECHA

# Para manejar valores especiales
LR_plusRANGER_FECHA <- ifelse(is.nan(LR_plusRANGER_FECHA) | is.infinite(LR_plusRANGER_FECHA), NA, LR_plusRANGER_FECHA)
LR_minusRANGER_FECHA <- ifelse(is.nan(LR_minusRANGER_FECHA) | is.infinite(LR_minusRANGER_FECHA), NA, LR_minusRANGER_FECHA)

# Crear un dataframe con todas las métricas
metrics_ranger_fecha <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  ranger_fecha_tot = c(accuracyRANGER_FECHA, kappaRANGER_FECHA, sensitivityRANGER_FECHA, specificityRANGER_FECHA, precisionRANGER_FECHA, f1_scoreRANGER_FECHA, npvRANGER_FECHA, prevalenceRANGER_FECHA, detection_rateRANGER_FECHA, balanced_accuracyRANGER_FECHA, LR_plusRANGER_FECHA, LR_minusRANGER_FECHA, AUC_Ranger_FECHA))

# Mostrar los resultados
print(metrics_ranger_fecha)

```



# KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_knn <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}

# KNN_fecha <- caret::train(formula, 
#                     data = trainData_fecha, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNN_fecha
KNN_fecha_tot_fx <- function(df_train, model, grid, metrica = "Accuracy",  control, preProcess) {

  
  # Entrenar el modelo
  KNN_fecha_tot <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_knn,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNN_fecha_tot)
  plot(KNN_fecha_tot, main = title)
  
  # Guardar resultado
 # save(KNN_fecha_tot_fx, file = paste0(title, ".rda"))
  
  return(KNN_fecha_tot)
}

KNN_fecha_tot<-KNN_fecha_tot_fx(df_train=trainData_fecha, model="knn", grid=tuneGrid, metrica="Accuracy",  preProcess = c("center","scale") )
print(KNN_fecha_tot)

```

```{r}
plot(KNN_fecha_tot)
```

# GLMNET


```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)
max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_glm <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}

#set.seed(42)
# GLM_fecha <- caret::train(formula, 
#                   data = trainData_fecha,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLM_fecha 
GLM_fecha_tot_fx <- function(df_train, model, grid, metrica = "Accuracy", control) {

  
  # Entrenar el modelo
  GLM_fecha_tot <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_glm
  )
  
  # Mostrar resumen
  print(GLM_fecha_tot)
  plot(GLM_fecha_tot, main = title)
  
  # Guardar resultado
 # save(GLM_fecha_tot, file = paste0(title, ".rda"))
  
  return(GLM_fecha_tot)
}

GLM_fecha_tot<-GLM_fecha_tot_fx(df_train=trainData_fecha, model="glmnet", grid=tuneGrid, metrica="Accuracy")
print(GLM_fecha_tot)
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_fecha_tot$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_fecha_tot$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_fecha_tot$results[best_model_index, ]



# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# Prediccion

```{r}
testData_fecha$Y<-as.factor(testData_fecha$Y)
str(testData_fecha$Y)
```

```{r}
testData_NOID_fecha <- testData_fecha[, -which(names(testData_fecha) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRF_fecha<-predict(RF_fecha_tot, newdata = testData_NOID_fecha)#matriz binarizada
print(predRF_fecha)

predRF_fecha <- factor(as.character(predRF_fecha), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRF_fecha <- factor(predRF_fecha, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))



# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixRF <- table(predRF_fecha, y_test)
# Imprimir la matriz de confusión
# print(confusion_matrixRF)
RF_metrics_fecha <- caret::confusionMatrix(predRF_fecha, y_test, positive = "Cov.Pos")


```

```{r}

library(pROC)

prob_predRF_fecha <- predict(RF_fecha_tot, newdata = testData_NOID_fecha, type = "prob")

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF_fecha[, "X2"], levels = c(1, 2))
AUC_RF_fecha <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_fecha, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

```


```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_RFfecha <- kappa2(cbind(predRF_fecha, testData_fecha$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RFfecha$value, 3)))
# kappa_RFfecha<-round(kappa_RFfecha$value, 3)
# Carga los paquetes necesarios
library(caret)
accuracyRF_fecha <- RF_metrics_fecha$overall["Accuracy"]
kappaRF_fecha <- RF_metrics_fecha$overall["Kappa"]
# Métricas por clase
sensitivityRF_fecha <- RF_metrics_fecha$byClass["Sensitivity"]
specificityRF_fecha <- RF_metrics_fecha$byClass["Specificity"]
precisionRF_fecha <- RF_metrics_fecha$byClass["Pos Pred Value"]
recallRF_fecha <- RF_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRF_fecha <- RF_metrics_fecha$byClass["F1"]
npvRF_fecha <- RF_metrics_fecha$byClass["Neg Pred Value"]
prevalenceRF_fecha <- RF_metrics_fecha$byClass["Prevalence"]
detection_rateRF_fecha <- RF_metrics_fecha$byClass["Detection Rate"]
balanced_accuracyRF_fecha <- RF_metrics_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRF_fecha <- sensitivityRF_fecha / (1 - specificityRF_fecha)
LR_minusRF_fecha <- (1 - sensitivityRF_fecha) / specificityRF_fecha

# Para manejar valores especiales
LR_plusRF_fecha <- ifelse(is.nan(LR_plusRF_fecha) | is.infinite(LR_plusRF_fecha), NA, LR_plusRF_fecha)
LR_minusRF_fecha <- ifelse(is.nan(LR_minusRF_fecha) | is.infinite(LR_minusRF_fecha), NA, LR_minusRF_fecha)

# Crear un dataframe con todas las métricas
metrics_rf_fecha <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  rf_fecha_tot = c(accuracyRF_fecha, kappaRF_fecha, sensitivityRF_fecha, specificityRF_fecha, precisionRF_fecha, 
            f1_scoreRF_fecha, npvRF_fecha, prevalenceRF_fecha, detection_rateRF_fecha, 
            balanced_accuracyRF_fecha, LR_plusRF_fecha, LR_minusRF_fecha, AUC_RF_fecha))

# Mostrar los resultados
print(metrics_rf_fecha)

```



```{r}
ciego_combat_corrected_fecha_tot_nodic <- as.data.frame(featureMatrix_ciegos_total)




# Si viene con columna Y (conocida), separala:
if ("Y" %in% colnames(ciego_combat_corrected_fecha_tot_nodic)) {
  Y_ciego <- ciego_combat_corrected_fecha_tot_nodic$Y
  X_ciego <- ciego_combat_corrected_fecha_tot_nodic[, setdiff(colnames(ciego_combat_corrected_fecha_tot_nodic), "Y")]
} else {
  X_ciego <- ciego_combat_corrected_fecha_tot_nodic
  Y_ciego <- NULL
}

X_ciego<-as.data.frame(X_ciego)
# Renombrar niveles
levels(Y_ciego) <- c("Cov.Neg", "Cov.Pos")



```



```{r}



# Predicciones (clase)
predicciones_ciego_fecha_tot <- predict(RF_fecha_tot, newdata = X_ciego)
levels(predicciones_ciego_fecha_tot) <- c("Cov.Neg", "Cov.Pos")

# Predicciones (probabilidades)
prob_ciego <- predict(RF_fecha_tot, newdata = X_ciego, type = "prob")
#prob_ciego$Cov.Neg<-prob_ciego$X1
#prob_ciego$Cov.Pos<-prob_ciego$X2


  # Confusion matrix
  ciego_tot_fecha<-caret::confusionMatrix(predicciones_ciego_fecha_tot, Y_ciego, positive = "Cov.Pos")

  # AUC
  library(pROC)
  Y_ciego_num <- as.numeric(Y_ciego)
  roc_ciego <- roc(Y_ciego_num, prob_ciego[, "X2"])
  plot(roc_ciego, main = "ROC en Datos Ciegos", col = "darkgreen", lwd = 2)
  auc_fecha_tot_rf_ciego<-auc(roc_ciego)
  AUC_RF_fecha_ciego<-print(auc_fecha_tot_rf_ciego)
  
  
  
  library(caret)
accuracyRF_fecha_ciego <- ciego_tot_fecha$overall["Accuracy"]
kappaRF_fecha_ciego <- ciego_tot_fecha$overall["Kappa"]
# Métricas por clase
sensitivityRF_fecha_ciego <- ciego_tot_fecha$byClass["Sensitivity"]
specificityRF_fecha_ciego <- ciego_tot_fecha$byClass["Specificity"]
precisionRF_fecha_ciego <- ciego_tot_fecha$byClass["Pos Pred Value"]
recallRF_fecha_ciego <- ciego_tot_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRF_fecha_ciego <- ciego_tot_fecha$byClass["F1"]
npvRF_fecha_ciego <- ciego_tot_fecha$byClass["Neg Pred Value"]
prevalenceRF_fecha_ciego <- ciego_tot_fecha$byClass["Prevalence"]
detection_rateRF_fecha_ciego <- ciego_tot_fecha$byClass["Detection Rate"]
balanced_accuracyRF_fecha_ciego <- ciego_tot_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRF_fecha_ciego <- sensitivityRF_fecha_ciego / (1 - specificityRF_fecha_ciego)
LR_minusRF_fecha_ciego <- (1 - sensitivityRF_fecha_ciego) / specificityRF_fecha_ciego

# Para manejar valores especiales
LR_plusRF_fecha_ciego <- ifelse(is.nan(LR_plusRF_fecha_ciego) | is.infinite(LR_plusRF_fecha_ciego), NA, LR_plusRF_fecha_ciego)
LR_minusRF_fecha_ciego <- ifelse(is.nan(LR_minusRF_fecha_ciego) | is.infinite(LR_minusRF_fecha_ciego), NA, LR_minusRF_fecha_ciego)

# Crear un dataframe con todas las métricas
metrics_rf_fecha_ciego_fx <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  rf_fecha_tot = c(accuracyRF_fecha_ciego, kappaRF_fecha_ciego, sensitivityRF_fecha_ciego, specificityRF_fecha_ciego, precisionRF_fecha_ciego, 
            f1_scoreRF_fecha_ciego, npvRF_fecha_ciego, prevalenceRF_fecha_ciego, detection_rateRF_fecha_ciego,
            balanced_accuracyRF_fecha_ciego, LR_plusRF_fecha_ciego, LR_minusRF_fecha_ciego, AUC_RF_fecha_ciego))

# Mostrar los resultados
print(metrics_rf_fecha_ciego_fx)
```





# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predKNN_fecha<-predict(KNN_fecha_tot, newdata = testData_NOID_fecha)#matriz binarizada
print(predKNN_fecha)

predKNN_fecha <- factor(as.character(predKNN_fecha), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predKNN_fecha <- factor(predKNN_fecha, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))



# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixKNN <- table(predKNN_fecha, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_fecha <- caret::confusionMatrix(predKNN_fecha, y_test, positive = "Cov.Pos")
```

```{r}



prob_predKNNfecha <- predict(KNN_fecha_tot, newdata = testData_NOID_fecha, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNNfecha))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNfecha[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_fecha <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_fecha, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```



```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_KNNfecha <- kappa2(cbind(predKNN_fecha, testData_fecha$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNNfecha$value, 3)))
# kappa_KNNfecha<-round(kappa_KNNfecha$value, 3)
# Carga los paquetes necesarios
library(caret)
accuracyKNN_fecha <- KNN_metrics_fecha$overall["Accuracy"]
kappaKNN_fecha <- KNN_metrics_fecha$overall["Kappa"]
# Métricas por clase
sensitivityKNN_fecha <- KNN_metrics_fecha$byClass["Sensitivity"]
specificityKNN_fecha <- KNN_metrics_fecha$byClass["Specificity"]
precisionKNN_fecha <- KNN_metrics_fecha$byClass["Pos Pred Value"]
recallKNN_fecha <- KNN_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreKNN_fecha <- KNN_metrics_fecha$byClass["F1"]
npvKNN_fecha <- KNN_metrics_fecha$byClass["Neg Pred Value"]
prevalenceKNN_fecha <- KNN_metrics_fecha$byClass["Prevalence"]
detection_rateKNN_fecha <- KNN_metrics_fecha$byClass["Detection Rate"]
balanced_accuracyKNN_fecha <- KNN_metrics_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusKNN_fecha <- sensitivityKNN_fecha / (1 - specificityKNN_fecha)
LR_minusKNN_fecha <- (1 - sensitivityKNN_fecha) / specificityKNN_fecha

# Para manejar valores especiales
LR_plusKNN_fecha <- ifelse(is.nan(LR_plusKNN_fecha) | is.infinite(LR_plusKNN_fecha), NA, LR_plusKNN_fecha)
LR_minusKNN_fecha <- ifelse(is.nan(LR_minusKNN_fecha) | is.infinite(LR_minusKNN_fecha), NA, LR_minusKNN_fecha)

# Crear un dataframe con todas las métricas
metrics_KNN_fecha <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNN_fecha_tot = c(accuracyKNN_fecha, kappaKNN_fecha, sensitivityKNN_fecha, specificityKNN_fecha, precisionKNN_fecha, 
            f1_scoreKNN_fecha, npvKNN_fecha, prevalenceKNN_fecha, detection_rateKNN_fecha, 
            balanced_accuracyKNN_fecha, LR_plusKNN_fecha, LR_minusKNN_fecha, AUC_KNN_fecha))

# Mostrar los resultados
print(metrics_KNN_fecha)

```




```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_fecha<-predict(GLM_fecha_tot, newdata = testData_NOID_fecha)#matriz binarizada
print(predGLM_fecha)

predGLM_fecha <- factor(as.character(predGLM_fecha), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLM_fecha <- factor(predGLM_fecha, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixGLM <- table(predGLM_fecha, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLM)
GLM_metrics_fecha <- caret::confusionMatrix(predGLM_fecha, y_test, positive = "Cov.Pos")
```


```{r}

# Predicciones probabilísticas con el modelo GLM
prob_predGLM_fecha <- predict(GLM_fecha_tot, newdata = testData_NOID_fecha, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM_fecha))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM_fecha[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_fecha <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_fecha, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```

```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_GLM_fecha <- kappa2(cbind(predGLM_fecha, testData_fecha$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLM_fecha$value, 3)))
# kappa_GLM_fecha<-round(kappa_GLM_fecha$value, 3)
library(caret)
accuracyGLM_fecha<- GLM_metrics_fecha$overall["Accuracy"]
kappaGLM_fecha<- GLM_metrics_fecha$overall["Kappa"]
# Métricas por clase
sensitivityGLM_fecha <- GLM_metrics_fecha$byClass["Sensitivity"]
specificityGLM_fecha <- GLM_metrics_fecha$byClass["Specificity"]
precisionGLM_fecha <- GLM_metrics_fecha$byClass["Pos Pred Value"]
recallGLM_fecha <- GLM_metrics_fecha$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreGLM_fecha <- GLM_metrics_fecha$byClass["F1"]
npvGLM_fecha <- GLM_metrics_fecha$byClass["Neg Pred Value"]
prevalenceGLM_fecha <- GLM_metrics_fecha$byClass["Prevalence"]
detection_rateGLM_fecha <- GLM_metrics_fecha$byClass["Detection Rate"]
balanced_accuracyGLM_fecha <- GLM_metrics_fecha$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusGLM_fecha <- sensitivityGLM_fecha / (1 - specificityGLM_fecha)
LR_minusGLM_fecha <- (1 - sensitivityGLM_fecha) / specificityGLM_fecha

# Para manejar valores especiales
LR_plusGLM_fecha <- ifelse(is.nan(LR_plusGLM_fecha) | is.infinite(LR_plusGLM_fecha), NA, LR_plusGLM_fecha)
LR_minusGLM_fecha <- ifelse(is.nan(LR_minusGLM_fecha) | is.infinite(LR_minusGLM_fecha), NA, LR_minusGLM_fecha)

# Crear un dataframe con todas las métricas
metrics_glm_fecha <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  glm_fecha_tot = c(accuracyGLM_fecha, kappaGLM_fecha, sensitivityGLM_fecha, specificityGLM_fecha, precisionGLM_fecha, 
            f1_scoreGLM_fecha, npvGLM_fecha, prevalenceGLM_fecha, detection_rateGLM_fecha, 
            balanced_accuracyGLM_fecha, LR_plusGLM_fecha, LR_minusGLM_fecha, AUC_GLM_fecha))

# Mostrar los resultados
print(metrics_glm_fecha)

```




