---
title: "SIN CORRECCION"
author: "Karina Roitman"
date: "2024-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Modelos sin corrección de efecto batch

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```



```{r}
library(dplyr)

# Eliminar la columna 'label' usando dplyr
#featureMatrix <- featureMatrix %>% select(-label)


```


```{r}
#featureMatrix_label<-featureMatrix[, c(2:60)]
```


```{r}
#featureMatrix_bind<- cbind(featureMatrix_label, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos
```



```{r}
# # Renombrar la columna 'label' a 'Y' y convertirla en factor
# names(featureMatrix_bind)[names(featureMatrix_bind) == "label"] <- "Y"
# 
# featureMatrix_bind <- as.data.frame(featureMatrix_bind)
# str(featureMatrix_bind)

# Verificar estructura del dataframe después de los cambios


```




```{r}

for (i in 1:(ncol(featureMatrix_bind) - 1)) {
  featureMatrix_bind[[i]] <- as.numeric(featureMatrix_bind[[i]])
}

str(featureMatrix_bind)
```

```{r}
# library(dplyr)
# featureMatrix_bind <- featureMatrix_bind %>% rename(Y = label)
# featureMatrix_bind$Y<-as.factor(featureMatrix_bind$Y)
# str(featureMatrix_bind$Y)
```



```{r}
str(featureMatrix_bind)
```

 Modelos desde matriz dicotomizada sin corregir por batch


```{r}

featureMatrix_bind<-as.data.frame(featureMatrix_bind)
featureMatrix_bind$Y <- as.factor(featureMatrix_bind$Y)
```


```{r}
# featureMatrix_bind_dic_tot<-featureMatrix_bind[1:542,]
# ciegoMatrix_dico_tot<-featureMatrix_bind[543:694,]
```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_bind, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_matriz_dico <- training(split_data)
testData_matriz_dico <- testing(split_data)

```

```{r}
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
trainData_matriz_dico$Y<-as.factor(trainData_matriz_dico$Y)
str(trainData_matriz_dico)
```

```{r}
train_subset_matriz <- trainData_matriz_dico[, 1:(ncol(trainData_matriz_dico) - 1)]
```




```{r}
labels <- trainData_matriz_dico$Y

#Definir número de bootstraps
n_boot <- 10
set.seed(123)

#Inicializar lista para guardar umbrales
thresholds_list <- vector("list", n_boot)

#  Ejecutar bootstrap
for (i in 1:n_boot) {
  sample_indices <- sample(1:nrow(train_subset_matriz), replace = TRUE)
  sampled_data <- train_subset_matriz[sample_indices, ]
  sampled_labels <- labels[sample_indices]

  thresholds <- optimizeThreshold(sampled_data, sampled_labels)
  thresholds_list[[i]] <- thresholds
}

# Unir todos los umbrales en un data frame
thresholds_df <- do.call(rbind, thresholds_list)

# Calcular el CV de cada pico
cv_thresholds <- apply(thresholds_df, 2, function(x) {
  media <- mean(x, na.rm = TRUE)
  sd <- sd(x, na.rm = TRUE)
  if (media == 0) return(Inf) else return((sd / media) * 100)
})

#Seleccionar los picos con CV < 40 (estables)
picos_estables <- names(cv_thresholds[cv_thresholds < 40])

#  Filtrar la matriz original para quedarte solo con esos picos
train_subset_matriz <- train_subset_matriz[, picos_estables]

#  Volver a combinar con la variable Y
#train_subset_fecha$Y <- labels
```




```{r}
# #Dicotimizacion de la matriz de intensidad
#  #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
# thr <- optimizeThreshold(train_subset_matriz, trainData_matriz_dico$Y)
# train_subset_matriz <- dichotomize(train_subset_matriz, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA

```


```{r}
trainData_matriz_dico<- cbind(train_subset_matriz, trainData_matriz_dico$Y)#1=neg,2=pos
```



```{r}
str(trainData_matriz_dico)
trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
```



```{r}
library(dplyr)
trainData_matriz_dico <- trainData_matriz_dico %>% rename(Y = ncol(trainData_matriz_dico))
str(trainData_matriz_dico$Y)
```

# 
# ```{r}
# trainData_matriz_dico<-as.data.frame(trainData_matriz_dico)
# trainData_matriz_dico$Y<-as.factor(trainData_matriz_dico$Y)
# str(trainData_matriz_dico)
# ```
# 
# 
# 
# ```{r}
# zero_var_indices <- nearZeroVar(trainData_matriz_dico[1:(ncol(trainData_matriz_dico) - 1)])
# if (length(zero_var_indices) > 0) {
#     trainData_matriz_dico <- trainData_matriz_dico[, -zero_var_indices]
# }
# 
# ```
# 
# 
# 
# ```{r}
# 
# thr_filtered <- thr[colnames(trainData_matriz_dico[1:(ncol(trainData_matriz_dico) - 1)])]
#```


quedan 42 variables


```{r}
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)

str(testData_matriz_dico)
```



```{r}
# library(dplyr)
# testData_equipo_dico <- testData_equipo_dico %>% rename(Y = label)
# str(testData_equipo_dico$Y)
```



```{r}
test_subset_matriz <- testData_matriz_dico[, 1:(ncol(testData_matriz_dico) - 1)]
```


```{r}
library(dplyr)

str(testData_matriz_dico$Y)
```



```{r}
test_subset_matriz <- test_subset_matriz[, colnames(trainData_matriz_dico)[1:(ncol(trainData_matriz_dico) - 1)]]
#test_subset <-  test_subset[, colnames(trainData_equipo_dico)]
# thr_filtered <-  thr[colnames(trainData_matriz_dico)[1:(ncol(trainData_matriz_dico) - 1)]]
# #Dicotimizacion de la matriz de intensidad
# test_subset_matriz <- dichotomize(test_subset_matriz, thr_filtered) 

```


```{r}
testData_matriz_dico<- cbind(test_subset_matriz, testData_matriz_dico$Y)#1=neg,2=pos
```

```{r}
library(dplyr)
testData_matriz_dico<-as.data.frame(testData_matriz_dico)
testData_matriz_dico <- testData_matriz_dico %>% rename(Y = ncol(testData_matriz_dico))
str(testData_matriz_dico$Y)
```

```{r}
testData_matriz_dico$Y<-as.factor(testData_matriz_dico$Y)
str(testData_matriz_dico)

```



```{r}
# Ver la distribución de clases en ambos conjuntos
table(trainData_matriz_dico$Y)
table(testData_matriz_dico$Y)

trainData_matriz<-trainData_matriz_dico
testData_matriz<-testData_matriz_dico
```



```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



```{r}
# Submuestras y repeticiones
set.seed(42)
# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```


# Random forest

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry)
                               #min.node.size = seq(1, 30, 2),
                               #min.node.size=min.node.size,
                               #splitrule = "gini")


seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}
# genera un vector de nrow(hiperparametros) números aleatorios entre 1 y 500.
#Esto asegura que cada combinación de hiperparámetros tenga una semilla diferente en cada iteración.

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

trainData$Y <- factor(as.numeric(factor(trainData$Y)))
trainData$Y <- factor(trainData$Y, levels = c("1", "2"))
levels(trainData$Y) <- make.names(levels(trainData$Y))

```



```{r}
library(caret)
#control1 <- trainControl(method = "cv", number = 10)
#grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

# RF <- caret::train(Y ~ ., data = trainData, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación


RF_matrix_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  RF_matrix <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = cross_val_rf
  )
  
  # Mostrar resumen
  print(RF_matrix)
  plot(RF_matrix)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF_matrix)
}

RF_matrix<-RF_matrix_fx(df_train=trainData, model="rf", grid=hiperparametros, metrica="Accuracy", title = "Primera_funcion" )
print(RF_matrix)
```

```{r}
RF_matrix
```

```{r}
plot(RF_matrix)
```





```{r}
# Predicciones (clase)
predicciones_ciego_matriz_tot <- predict(RF_matrix, newdata = X_ciego)
levels(predicciones_ciego_matriz_tot) <- c("Cov.Neg", "Cov.Pos")

# Predicciones (probabilidades)
prob_ciego <- predict(RF_matrix, newdata = X_ciego, type = "prob")
#prob_ciego$Cov.Neg<-prob_ciego$X1
#prob_ciego$Cov.Pos<-prob_ciego$X2


  # Confusion matrix
  ciego_tot_equipo_rf_matrix<-caret::confusionMatrix(predicciones_ciego_matriz_tot, Y_ciego, positive = "Cov.Pos")

  # AUC
  library(pROC)
  Y_ciego_num <- as.numeric(Y_ciego)
  roc_ciego <- roc(Y_ciego_num, prob_ciego[, "X2"])
  plot(roc_ciego, main = "ROC en Datos Ciegos", col = "darkgreen", lwd = 2)
 AUC_RF_ciego<- auc_matrix_tot<-auc(roc_ciego)
  print(auc_matrix_tot)
  
  
library(caret)
accuracy_RF_ciego <- ciego_tot_equipo_rf_matrix$overall["Accuracy"]
kappa_RF_ciego <- ciego_tot_equipo_rf_matrix$overall["Kappa"]
# Métricas por clase
sensitivity_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Sensitivity"]
specificity_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Specificity"]
precision_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Pos Pred Value"]
recall_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Sensitivity"]  # Igual a sensitivity
f1_score_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["F1"]
npv_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Neg Pred Value"]
prevalence_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Prevalence"]
detection_rate_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Detection Rate"]
balanced_accuracy_RF_ciego <- ciego_tot_equipo_rf_matrix$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plus_RF_ciego <- sensitivity_RF_ciego / (1 - specificity_RF_ciego)
LR_minus_RF_ciego <- (1 - sensitivity_RF_ciego) / specificity_RF_ciego

# Para manejar valores especiales
LR_plus_RF_ciego <- ifelse(is.nan(LR_plus_RF_ciego) | is.infinite(LR_plus_RF_ciego), NA, LR_plus_RF_ciego)
LR_minus_RF_ciego <- ifelse(is.nan(LR_minus_RF_ciego) | is.infinite(LR_minus_RF_ciego), NA, LR_minus_RF_ciego)

# Crear un dataframe con todas las métricas
metrics_rf_matrix_ciego_cv <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  rf_matrix_total_cv = c(accuracy_RF_ciego, kappa_RF_ciego, sensitivity_RF_ciego, specificity_RF_ciego, precision_RF_ciego, 
            f1_score_RF_ciego, npv_RF_ciego, prevalence_RF_ciego, detection_rate_RF_ciego, 
            balanced_accuracy_RF_ciego, LR_plus_RF_ciego, LR_minus_RF_ciego, AUC_RF_ciego))

# Mostrar los resultados
print(metrics_rf_matrix_ciego_cv) 

  
  
```



<!-- # Prediccion GLM -->

<!-- ```{r} -->
<!-- #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test) -->
<!-- predGLM<-predict(GLM_matrix, newdata = testData_NOID)#matriz binarizada -->
<!-- print(predGLM) -->

<!-- predGLM <- factor(as.character(predGLM),  -->
<!--                              levels = c("X1", "X2"), -->
<!--                              labels = c("1", "2")) -->
<!-- predGLM <- factor(predGLM, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->


<!-- # Obtener las etiquetas reales del conjunto de datos de prueba -->
<!-- y_test <- testData$Y -->
<!-- y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->
<!-- #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # confusion_matrixGLM <- table(predGLM, y_test) -->
<!-- # # Imprimir la matriz de confusión -->
<!-- # print(confusion_matrixGLM) -->
<!-- GLM_sincorreccion<- caret::confusionMatrix(predGLM, y_test, positive = "Cov.Pos") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # # Precisión (Accuracy) -->
<!-- # accuracy_GLM <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM) -->
<!-- # print(paste("La precisión (Accuracy) es:", round(accuracy_GLM,2))) -->
<!-- #  -->
<!-- # # Sensibilidad (Recall o TPR) -->
<!-- # sensitivity_GLM <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2]) -->
<!-- # print(paste("La sensibilidad es:", round(sensitivity_GLM,2))) -->
<!-- #  -->
<!-- # # Especificidad (TNR) -->
<!-- # specificity_GLM <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1]) -->
<!-- # print(paste("la especificidad es:", round(specificity_GLM,2))) -->
<!-- #  -->
<!-- # # Valor Predictivo Positivo (PPV o Precision) -->
<!-- # ppv_GLM <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ]) -->
<!-- # print(paste("el VPP es:", round(ppv_GLM,2))) -->
<!-- #  -->
<!-- # # Valor Predictivo Negativo (VPN) -->
<!-- # npv_GLM <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ]) -->
<!-- # print(paste("el VPN es: ",  round(npv_GLM,2))) -->


<!-- # Predicciones probabilísticas con el modelo GLM -->
<!-- prob_predGLM <- predict(GLM_matrix, newdata = testData_NOID, type = "prob") -->

<!-- # Verifica los nombres de las columnas (clases) -->
<!-- print(colnames(prob_predGLM))  # Deberían ser "1" y "2" o los nombres de las clases -->

<!-- # Asegúrate de que y_test sea numérico con niveles 1 y 2 -->
<!-- y_test_numeric <- as.numeric(testData$Y) -->

<!-- # Calcula la curva ROC usando la probabilidad de la clase positiva ("2") -->
<!-- roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM[, "X2"], levels = c(1, 2)) -->

<!-- # Calcula el AUC -->
<!-- AUC_GLM <- auc(roc_curve_glm) -->
<!-- print(paste("El valor de AUC es:", round(AUC_GLM, 3))) -->

<!-- # Visualiza la curva ROC -->
<!-- plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- #  -->
<!-- # library(irr) -->
<!-- #  -->
<!-- # # Calcular Kappa -->
<!-- # kappa_GLM<- kappa2(cbind(predGLM,y_test)) -->
<!-- #  -->
<!-- # # Ver el valor de Kappa -->
<!-- # print(paste("El índice Kappa es:", round(kappa_GLM$value, 3))) -->
<!-- # kappa_GLM<-round(kappa_GLM$value, 3) -->

<!-- # Carga los paquetes necesarios -->
<!-- library(caret) -->
<!-- accuracy_GLM <- GLM_sincorreccion$overall["Accuracy"] -->
<!-- kappa_GLM <- GLM_sincorreccion$overall["Kappa"] -->
<!-- # Métricas por clase -->
<!-- sensitivity_GLM <- GLM_sincorreccion$byClass["Sensitivity"] -->
<!-- specificity_GLM <- GLM_sincorreccion$byClass["Specificity"] -->
<!-- precision_GLM <- GLM_sincorreccion$byClass["Pos Pred Value"] -->
<!-- recall_GLM <- GLM_sincorreccion$byClass["Sensitivity"]  # Igual a sensitivity -->
<!-- f1_score_GLM <- GLM_sincorreccion$byClass["F1"] -->
<!-- npv_GLM <- GLM_sincorreccion$byClass["Neg Pred Value"] -->
<!-- prevalence_GLM <- GLM_sincorreccion$byClass["Prevalence"] -->
<!-- detection_rate_GLM <- GLM_sincorreccion$byClass["Detection Rate"] -->
<!-- balanced_accuracy_GLM <- GLM_sincorreccion$byClass["Balanced Accuracy"] -->

<!-- # Calcular LR+ y LR- que no vienen directamente en confusionMatrix -->
<!-- LR_plus_GLM <- sensitivity_GLM / (1 - specificity_GLM) -->
<!-- LR_minus_GLM <- (1 - sensitivity_GLM) / specificity_GLM -->

<!-- # Para manejar valores especiales -->
<!-- LR_plus_GLM <- ifelse(is.nan(LR_plus_GLM) | is.infinite(LR_plus_GLM), NA, LR_plus_GLM) -->
<!-- LR_minus_GLM <- ifelse(is.nan(LR_minus_GLM) | is.infinite(LR_minus_GLM), NA, LR_minus_GLM) -->

<!-- # Crear un dataframe con todas las métricas -->
<!-- metrics_GLM_matrix_total <- data.frame( -->
<!--   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision",  -->
<!--              "F1_Score", "NPV", "Prevalence", "Detection_Rate",  -->
<!--              "Balanced_Accuracy", "LR+", "LR-", "AUC"), -->
<!--   GLM_matrix_total = c(accuracy_GLM, kappa_GLM, sensitivity_GLM, specificity_GLM, precision_GLM,  -->
<!--             f1_score_GLM, npv_GLM, prevalence_GLM, detection_rate_GLM,  -->
<!--             balanced_accuracy_GLM, LR_plus_GLM, LR_minus_GLM, AUC_GLM)) -->

<!-- # Mostrar los resultados -->
<!-- print(metrics_GLM_matrix_total) -->

<!-- ``` -->


<!-- # Prediccion KNN -->

<!-- ```{r} -->
<!-- #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test) -->
<!-- prediccionesKNN<-predict(KNN_matrix, newdata = testData_NOID)#matriz binarizada -->
<!-- print(prediccionesKNN) -->

<!-- prediccionesKNN <- factor(as.character(prediccionesKNN),  -->
<!--                              levels = c("X1", "X2"), -->
<!--                              labels = c("1", "2")) -->
<!-- prediccionesKNN <- factor(prediccionesKNN, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->


<!-- # Obtener las etiquetas reales del conjunto de datos de prueba -->
<!-- y_test <- testData$Y -->
<!-- #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada -->
<!-- y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos")) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # confusion_matrixKNN <- table(prediccionesKNN, y_test) -->
<!-- # # Imprimir la matriz de confusión -->
<!-- # print(confusion_matrixKNN) -->
<!-- KNN_metrics <- caret::confusionMatrix(prediccionesKNN, y_test, positive = "Cov.Pos") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # #metricas -->
<!-- # # Precisión -->
<!-- # accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN) -->
<!-- # print(accuracy_KNN) -->
<!-- # # Precisión positiva (VP) o sensibilidad -->
<!-- # sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[, 2]) -->
<!-- # print(sensitivity_KNN) -->
<!-- # # Precisión negativa (VN) o especificidad -->
<!-- # specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[, 1]) -->
<!-- # specificity_KNN -->

<!-- #  -->
<!-- # # Valor predictivo positivo (VPP) -->
<!-- # ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ]) -->
<!-- # print(ppv_KNN) -->
<!-- # # Valor predictivo negativo (VPN) -->
<!-- # npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ]) -->
<!-- # print(npv_KNN) -->



<!-- prob_predKNN <- predict(KNN_matrix, newdata = testData_NOID, type = "prob") -->

<!-- # Verifica los nombres de las columnas (clases) -->
<!-- print(colnames(prob_predKNN))  # Deberían ser "1" y "2" o los nombres de las clases -->

<!-- # Asegúrate de que y_test sea numérico con niveles 1 y 2 -->
<!-- y_test_numeric <- as.numeric(testData$Y) -->

<!-- # Calcula la curva ROC usando la probabilidad de la clase positiva ("2") -->
<!-- roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNN[, "X2"], levels = c(1, 2)) -->

<!-- # Calcula el AUC -->
<!-- AUC_KNN <- auc(roc_curve_knn) -->
<!-- print(paste("El valor de AUC es:", round(AUC_KNN, 3))) -->

<!-- # Visualiza la curva ROC -->
<!-- plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- #  -->
<!-- # library(irr) -->
<!-- #  -->
<!-- # # Calcular Kappa -->
<!-- # kappa_KNN <- kappa2(cbind(prediccionesKNN, y_test)) -->
<!-- #  -->
<!-- # # Ver el valor de Kappa -->
<!-- # print(paste("El índice Kappa es:", round(kappa_KNN$value, 3))) -->
<!-- # kappa_KNN<-round(kappa_KNN$value, 3) -->
<!-- # Carga los paquetes necesarios -->
<!-- library(caret) -->
<!-- accuracy_KNN <- KNN_metrics$overall["Accuracy"] -->
<!-- kappa_KNN <- KNN_metrics$overall["Kappa"] -->
<!-- # Métricas por clase -->
<!-- sensitivity_KNN <- KNN_metrics$byClass["Sensitivity"] -->
<!-- specificity_KNN <- KNN_metrics$byClass["Specificity"] -->
<!-- precision_KNN <- KNN_metrics$byClass["Pos Pred Value"] -->
<!-- recall_KNN <- KNN_metrics$byClass["Sensitivity"]  # Igual a sensitivity -->
<!-- f1_score_KNN <- KNN_metrics$byClass["F1"] -->
<!-- npv_KNN <- KNN_metrics$byClass["Neg Pred Value"] -->
<!-- prevalence_KNN <- KNN_metrics$byClass["Prevalence"] -->
<!-- detection_rate_KNN <- KNN_metrics$byClass["Detection Rate"] -->
<!-- balanced_accuracy_KNN <- KNN_metrics$byClass["Balanced Accuracy"] -->

<!-- # Calcular LR+ y LR- que no vienen directamente en confusionMatrix -->
<!-- LR_plus_KNN <- sensitivity_KNN / (1 - specificity_KNN) -->
<!-- LR_minus_KNN <- (1 - sensitivity_KNN) / specificity_KNN -->

<!-- # Para manejar valores especiales -->
<!-- LR_plus_KNN <- ifelse(is.nan(LR_plus_KNN) | is.infinite(LR_plus_KNN), NA, LR_plus_KNN) -->
<!-- LR_minus_KNN <- ifelse(is.nan(LR_minus_KNN) | is.infinite(LR_minus_KNN), NA, LR_minus_KNN) -->

<!-- # Crear un dataframe con todas las métricas -->
<!-- metrics_knn_matrix_total <- data.frame( -->
<!--   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision",  -->
<!--              "F1_Score", "NPV", "Prevalence", "Detection_Rate",  -->
<!--              "Balanced_Accuracy", "LR+", "LR-", "AUC"), -->
<!--   knn_matrix_total = c(accuracy_KNN, kappa_KNN, sensitivity_KNN, specificity_KNN, precision_KNN,  -->
<!--             f1_score_KNN, npv_KNN, prevalence_KNN, detection_rate_KNN,  -->
<!--             balanced_accuracy_KNN, LR_plus_KNN, LR_minus_KNN, AUC_KNN)) -->

<!-- # Mostrar los resultados -->
<!-- print(metrics_knn_matrix_total) -->


<!-- ``` -->
