---
title: "COMBAT_CORRECTED"
author: "Karina Roitman"
date: "2024-12-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```


# Modelos (con datos combat_corrected POR EQUIPO pero sin dictomizar)



```{r}
combat_corrected_equipo_high<-as.data.frame(combat_corrected_equipo_high)
combat_corrected_equipo_high<- cbind(combat_corrected_equipo_high, Y=Datos_actualizados$PCR.Cov)#1=neg,2=pos
```

```{r}
combat_corrected_equipo_high<-as.data.frame(combat_corrected_equipo_high)
combat_corrected_equipo_high$Y<-as.factor(combat_corrected_equipo_high$Y)
str(combat_corrected_equipo_high)
```

```{r}
#combat_corrected_equipo_high<-combat_corrected_equipo_high[,c(1:60)]
```


```{r}
library(dplyr)
#combat_corrected_equipo_high_tot <- combat_corrected_equipo_high %>% rename(Y = label)
str(combat_corrected_equipo_high$Y)
```

```{r}
combat_corrected_equipo_high_carga<-combat_corrected_equipo_high[1:238,]

ciego_combat_corrected_equipo_high<-combat_corrected_equipo_high[239:327,]
```


```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_equipo_high_carga, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_equipo <- training(split_data)
testData_equipo <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData_equipo$Y)
table(testData_equipo$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


```

```{r}
# set.seed(42)
# library(caret)
# control1 <- trainControl(method = "cv", number = 5)
# #en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```



```{r}
# Submuestras y repeticiones
set.seed(42)
# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(2, 5, 10)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry)
                               #min.node.size = seq(1, 30, 2),
                               #min.node.size=min.node.size,
                               #splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}
# genera un vector de nrow(hiperparametros) números aleatorios entre 1 y 500.
#Esto asegura que cada combinación de hiperparámetros tenga una semilla diferente en cada iteración.

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_rf <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

trainData_equipo$Y <- factor(as.numeric(factor(trainData_equipo$Y)))
trainData_equipo$Y <- factor(trainData_equipo$Y, levels = c("1", "2"))
levels(trainData_equipo$Y) <- make.names(levels(trainData_equipo$Y))

```

# Random forest


```{r}

library(caret)


# RF_equipo <- caret::train(Y ~ ., data = trainData_equipo, 
#                       method = "rf",   # Método para random forest
#                       #trControl = control1, 
#                       tuneGrid = hiperparametros,  # Parámetro mtry
#                       metric = "Accuracy",
#                       trControl = cross_val)  # Métrica para clasificación
RF_equipo_carga_fx <- function(df_train, model, grid, metrica = "Accuracy", title = "modelo", control) {

  
  # Entrenar el modelo
  RF_equipo <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = hiperparametros,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(RF_equipo)
  plot(RF_equipo, main = title)
  
  # Guardar resultado
  #save(RF_equipo, file = paste0(title, ".rda"))
  
  return(RF_equipo)
}

RF_equipo<-RF_equipo_carga_fx(df_train=trainData_equipo, model="rf", grid=tuneGrid, metrica="Accuracy", control=cross_val_rf )
print(RF_equipo)
```

```{r}
RF_equipo
```

```{r}
plot(RF_equipo)
```


KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_knn <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}

#set.seed(42)
# Ajustar el modelo KNN
# KNN_equipo <- caret::train(formula, 
#                     data = trainData_equipo, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNN_equipo

KNN_fecha_carga_fx <- function(df_train, model, grid, metrica = "Accuracy",  control, preProcess) {

  
  # Entrenar el modelo
  KNN_equipo <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control,
    preProcess = preProcess
  )
  
  # Mostrar resumen
  print(KNN_equipo)
  plot(KNN_equipo, main = title)
  
  # Guardar resultado
  #save(KNN_fecha_tot_fx, file = paste0(title, ".rda"))
  
  return(KNN_equipo)
}

KNN_equipo<-KNN_fecha_carga_fx(df_train=trainData_equipo, model="knn", grid=tuneGrid, metrica="Accuracy",   control= cross_val_knn, preProcess = c("center","scale") )
print(KNN_equipo)
```

```{r}
plot(KNN_equipo)
```

# GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
# Seeds
seed.rf <- 42
set.seed(seed.rf)

max_sample <- max(500, nrow(tuneGrid) * 2)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(max_sample, nrow(tuneGrid)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```


```{r}
# Training control

cross_val_glm <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)
```


```{r}

#set.seed(42)
# GLM_equipo <- caret::train(formula, 
#                   data = trainData_equipo,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    #trControl = control1,
#                    trControl= cross_val,
#                    tuneGrid=tuneGrid)
# 
# GLM_equipo 

GLM_equipo_carga_fx <- function(df_train, model, grid, metrica = "Accuracy",  control) {

  
  # Entrenar el modelo
  GLM_equipo <- caret::train(
    Y ~ .,
    data = df_train,
    method = model,
    tuneGrid = grid,
    metric = metrica,
    trControl = control
  )
  
  # Mostrar resumen
  print(GLM_equipo)
  plot(GLM_equipo, main = title)
  
  # Guardar resultado
  #save(GLM_equipo, file = paste0(title, ".rda"))
  
  return(GLM_equipo)
}

GLM_equipo<-GLM_equipo_carga_fx(df_train=trainData_equipo, model="glmnet", grid=tuneGrid, metrica="Accuracy", control=cross_val_glm)
print(GLM_equipo)
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_equipo$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_equipo$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_equipo$results[best_model_index, ]



# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

#  Prediccion

```{r}
testData_equipo$Y<-as.factor(testData_equipo$Y)
str(testData_equipo$Y)
```

```{r}
testData_NOID_equipo <- testData_equipo[, -which(names(testData_equipo) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRF_equipo_carga<-predict(RF_equipo, newdata = testData_NOID_equipo)#matriz binarizada
print(predRF_equipo_carga)

predRF_equipo_carga <- factor(as.character(predRF_equipo_carga), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRF_equipo_carga <- factor(predRF_equipo_carga, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_equipo$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixRF <- table(predRF_equipo, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRF)

RF_metrics_equipo_carga <- caret::confusionMatrix(predRF_equipo_carga, y_test, positive = "Cov.Pos")

```

```{r}
# # Precisión (Accuracy)
# accuracy_RF_equipo <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RF_equipo,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RF_equipo <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RF_equipo,2)))
# 
# # Especificidad (TNR)
# specificityRF_equipo <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
# print(paste("la especificidad es:", round(specificityRF_equipo,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RF_equipo <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
# print(paste("el VPP es:", round(ppv_RF_equipo,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RF_equipo <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
# print(paste("el VPN es: ",  round(npv_RF_equipo,2)))


library(pROC)

prob_predRF_equipo <- predict(RF_equipo, newdata = testData_NOID_equipo, type = "prob")

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF_equipo[, "X2"], levels = c(1, 2))
AUC_RF_equipo_carga <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_equipo_carga, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

```


```{r}
library(caret)
accuracyRF_equipo_carga <- RF_metrics_equipo_carga$overall["Accuracy"]
kappaRF_equipo_carga <- RF_metrics_equipo_carga$overall["Kappa"]
# Métricas por clase
sensitivityRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Sensitivity"]
specificityRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Specificity"]
precisionRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Pos Pred Value"]
recallRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRF_equipo_carga <- RF_metrics_equipo_carga$byClass["F1"]
npvRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Neg Pred Value"]
prevalenceRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Prevalence"]
detection_rateRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Detection Rate"]
balanced_accuracyRF_equipo_carga <- RF_metrics_equipo_carga$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRF_equipo_carga <- sensitivityRF_equipo_carga / (1 - specificityRF_equipo_carga)
LR_minusRF_equipo_carga <- (1 - sensitivityRF_equipo_carga) / specificityRF_equipo_carga

# Para manejar valores especiales
LR_plusRF_equipo_carga <- ifelse(is.nan(LR_plusRF_equipo_carga) | is.infinite(LR_plusRF_equipo_carga), NA, LR_plusRF_equipo_carga)
LR_minusRF_equipo_carga <- ifelse(is.nan(LR_minusRF_equipo_carga) | is.infinite(LR_minusRF_equipo_carga), NA, LR_minusRF_equipo_carga)

# Crear un dataframe con todas las métricas
metricsRF_equipo_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RF_equipo_carga = c(accuracyRF_equipo_carga, kappaRF_equipo_carga, sensitivityRF_equipo_carga, specificityRF_equipo_carga, precisionRF_equipo_carga, 
            f1_scoreRF_equipo_carga, npvRF_equipo_carga, prevalenceRF_equipo_carga, detection_rateRF_equipo_carga, 
            balanced_accuracyRF_equipo_carga, LR_plusRF_equipo_carga, LR_minusRF_equipo_carga, AUC_RF_equipo_carga))

# Mostrar los resultados
print(metricsRF_equipo_carga)

# 
# library(irr)
# 
# # Calcular Kappa
# kappa_RFeq <- kappa2(cbind(predRF_equipo, testData_equipo$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RFeq$value, 3)))
# kappa_RFeq<-round(kappa_RFeq$value, 3)
```





# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predKNN_equipo<-predict(KNN_equipo, newdata = testData_NOID_equipo)#matriz binarizada
print(predKNN_equipo)

predKNN_equipo <- factor(as.character(predKNN_equipo), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predKNN_equipo <- factor(predKNN_equipo, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_equipo$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixKNN <- table(predKNN_equipo, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_equipo <- caret::confusionMatrix(predKNN_equipo, y_test, positive = "Cov.Pos")
```

```{r}
# #metricas
# # Precisión
# accuracy_KNN_equipo <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
# print(accuracy_KNN_equipo)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_KNN_equipo<- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
# print(sensitivity_KNN_equipo)
# # Precisión negativa (VN) o especificidad
# specificity_KNN_equipo <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
# specificity_KNN_equipo
# 
# # Valor predictivo positivo (VPP)
# ppv_KNN_equipo <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
# print(ppv_KNN_equipo)
# # Valor predictivo negativo (VPN)
# npv_KNN_equipo <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
# print(npv_KNN_equipo)


prob_predKNNequipo <- predict(KNN_equipo, newdata = testData_NOID_equipo, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNNequipo))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_equipo$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNequipo[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_equipo_carga <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_equipo_carga, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```



```{r}
accuracyKNN_equipo_carga <- KNN_metrics_equipo$overall["Accuracy"]
kappaKNN_equipo_carga <- KNN_metrics_equipo$overall["Kappa"]

# Métricas por clase
sensitivityKNN_equipo_carga <- KNN_metrics_equipo$byClass["Sensitivity"]
specificityKNN_equipo_carga <- KNN_metrics_equipo$byClass["Specificity"]
precisionKNN_equipo_carga <- KNN_metrics_equipo$byClass["Pos Pred Value"]
recallKNN_equipo_carga <- KNN_metrics_equipo$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreKNN_equipo_carga <- KNN_metrics_equipo$byClass["F1"]
npvKNN_equipo_carga <- KNN_metrics_equipo$byClass["Neg Pred Value"]
prevalenceKNN_equipo_carga <- KNN_metrics_equipo$byClass["Prevalence"]
detection_rateKNN_equipo_carga <- KNN_metrics_equipo$byClass["Detection Rate"]
balanced_accuracyKNN_equipo_carga <- KNN_metrics_equipo$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusKNN_equipo_carga <- sensitivityKNN_equipo_carga / (1 - specificityKNN_equipo_carga)
LR_minusKNN_equipo_carga <- (1 - sensitivityKNN_equipo_carga) / specificityKNN_equipo_carga

# Para manejar valores especiales
LR_plusKNN_equipo_carga <- ifelse(is.nan(LR_plusKNN_equipo_carga) | is.infinite(LR_plusKNN_equipo_carga), NA, LR_plusKNN_equipo_carga)
LR_minusKNN_equipo_carga <- ifelse(is.nan(LR_minusKNN_equipo_carga) | is.infinite(LR_minusKNN_equipo_carga), NA, LR_minusKNN_equipo_carga)

# Crear un dataframe con todas las métricas
metricsKNN_equipo_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNN_equipo_carga = c(accuracyKNN_equipo_carga, kappaKNN_equipo_carga, sensitivityKNN_equipo_carga, specificityKNN_equipo_carga, precisionKNN_equipo_carga, 
            f1_scoreKNN_equipo_carga, npvKNN_equipo_carga, prevalenceKNN_equipo_carga, detection_rateKNN_equipo_carga, 
            balanced_accuracyKNN_equipo_carga, LR_plusKNN_equipo_carga, LR_minusKNN_equipo_carga, AUC_KNN_equipo_carga))

# Mostrar los resultados
print(metricsKNN_equipo_carga)

# library(irr)
# 
# # Calcular Kappa
# kappa_KNNeq <- kappa2(cbind(predKNN_equipo, testData_equipo$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNNeq$value, 3)))
# kappa_KNNeq<-round(kappa_KNNeq$value, 3)
```


# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_equipo<-predict(GLM_equipo, newdata = testData_NOID_equipo)#matriz binarizada
print(predGLM_equipo)

predGLM_equipo <- factor(as.character(predGLM_equipo), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLM_equipo <- factor(predGLM_equipo, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_equipo$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
#y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixGLM <- table(predGLM_equipo, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLM)
GLM_metrics_equipocarga <- caret::confusionMatrix(predGLM_equipo, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_GLM_equipo <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM)
# print(paste("La precisión (Accuracy) es:", round(accuracy_GLM_equipo,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_GLM_equipo <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_GLM_equipo,2)))
# 
# # Especificidad (TNR)
# specificity_GLM_equipo <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1])
# print(paste("la especificidad es:", round(specificity_GLM_equipo,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_GLM_equipo <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ])
# print(paste("el VPP es:", round(ppv_GLM_equipo,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_GLM_equipo <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ])
# print(paste("el VPN es: ",  round(npv_GLM_equipo,2)))


# Predicciones probabilísticas con el modelo GLM
prob_predGLM_equipo <- predict(GLM_equipo, newdata = testData_NOID_equipo, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM_equipo))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_equipo$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM_equipo[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_equipo_carga <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_equipo_carga, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```

```{r}
# Carga los paquetes necesarios
library(caret)
accuracyGLM_equipo_carga  <- GLM_metrics_equipocarga$overall["Accuracy"]
kappaGLM_equipo_carga <- GLM_metrics_equipocarga$overall["Kappa"]
# Métricas por clase
sensitivityGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Sensitivity"]
specificityGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Specificity"]
precisionGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Pos Pred Value"]
recallGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["F1"]
npvGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Neg Pred Value"]
prevalenceGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Prevalence"]
detection_rateGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Detection Rate"]
balanced_accuracyGLM_equipo_carga  <- GLM_metrics_equipocarga$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusGLM_equipo_carga  <- sensitivityGLM_equipo_carga  / (1 - specificityGLM_equipo_carga )
LR_minusGLM_equipo_carga  <- (1 - sensitivityGLM_equipo_carga) / specificityGLM_equipo_carga 

# Para manejar valores especiales
LR_plusGLM_equipo_carga  <- ifelse(is.nan(LR_plusGLM_equipo_carga ) | is.infinite(LR_plusGLM_equipo_carga ), NA, LR_plusGLM_equipo_carga )
LR_minusGLM_equipo_carga  <- ifelse(is.nan(LR_minusGLM_equipo_carga ) | is.infinite(LR_minusGLM_equipo_carga ), NA, LR_minusGLM_equipo_carga )

# Crear un dataframe con todas las métricas
metricsGLM_equipo_carga  <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLM_equipo_carga = c(accuracyGLM_equipo_carga , kappaGLM_equipo_carga , sensitivityGLM_equipo_carga , specificityGLM_equipo_carga , precisionGLM_equipo_carga , 
            f1_scoreGLM_equipo_carga , npvGLM_equipo_carga , prevalenceGLM_equipo_carga , detection_rateGLM_equipo_carga , 
            balanced_accuracyGLM_equipo_carga , LR_plusGLM_equipo_carga , LR_minusGLM_equipo_carga , AUC_GLM_equipo_carga ))

# Mostrar los resultados
print(metricsGLM_equipo_carga)
# library(irr)
# 
# # Calcular Kappa
# kappa_GLM_equipo <- kappa2(cbind(predGLM_equipo, testData_equipo$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLM_equipo$value, 3)))
# 
# kappa_GLM_equipo<-kappa_GLM_equipo$value
```

# Prediccion RANGER

```{r}
Train.rf_equipo  <- as.data.frame(trainData_equipo) 

Test.rf_equipo <- as.data.frame(testData_equipo)
```

```{r}
# objeto_recipe <- recipe(formula = Y ~ .,
#                         data =  Train.rf_equipo)
# 
# objeto_recipe <- objeto_recipe %>% 
#   step_nzv(all_predictors())
# 
# trained_recipe <- prep(objeto_recipe, training = Train.rf_equipo)
# 
# Train.rf_equipo <- bake(trained_recipe, new_data = Train.rf_equipo)
# Test.rf_equipo  <- bake(trained_recipe, new_data = Test.rf_equipo)
```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = seq(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val_ranger <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_equipo$Y <- factor(as.numeric(factor(Train.rf_equipo$Y)))
Train.rf_equipo$Y <- factor(Train.rf_equipo$Y, levels = c("1", "2"))
levels(Train.rf_equipo$Y) <- make.names(levels(Train.rf_equipo$Y))

```





```{r}
class(Train.rf_equipo)
```


```{r}
Train.rf_equipo <- as.data.frame(Train.rf_equipo)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_equipo <- as.data.frame(Train.rf_equipo)

# Convertimos Y a factor
Train.rf_equipo$Y <- as.factor(Train.rf_equipo$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_equipo <- caret::train(Y ~ .,
                data = Train.rf_equipo, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val_ranger,
                num.trees = n_trees)
               #allowParallel=FALSE)  

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```


```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_equipo <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_equipo[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_equipo,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val_ranger,
    num.trees = nt
  )
}

```





```{r}
  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_equipo, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_equipo, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_equipo, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_equipo, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```


```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_equipo[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```

```{r}
# ranger_equipo <- caret::train(
#   Y ~ .,
#   data = Train.rf_equipo, 
#   method = "ranger",
#   tuneGrid = data.frame(mtry = 2, min.node.size = 1, splitrule = "gini"), 
#   metric = "Accuracy",
#   importance = "impurity",
#   trControl = cross_val,  # Si querés usar validación cruzada
#   num.trees = 200  # Número de árboles fijo
# )
# print(ranger_equipo)
ranger_carga_equipo_fx<-function(df_train, model, grid, metrica, title, num.trees){

ranger_equipo <- caret::train(
  Y ~ .,
  data = df_train,
  method = "ranger",
  tuneGrid = hiperparametros,
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val_ranger,  # Si querés usar validación cruzada
  num.trees=num.trees  # Número de árboles fijo
)

return(ranger_equipo)
}


ranger_equipo<-ranger_carga_equipo_fx(Train.rf_equipo,"ranger",hiperparametros,"Accuracy", num.trees=mejor_num_trees)

```



```{r}
Test.rf_equipo$Y<-as.factor(Test.rf_equipo$Y)
str(Test.rf_equipo$Y)
```

```{r}
testRF_NOID_equipo <- Test.rf_equipo[, -which(names(Test.rf_equipo) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_equipo<-predict(ranger_equipo, newdata = testRF_NOID_equipo)#matriz binarizada
print(predRANGER_equipo)

predRANGER_equipo <- factor(as.character(predRANGER_equipo), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRANGER_equipo <- factor(predRANGER_equipo, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- Test.rf_equipo$Y

prob_predRanger_equipo <- predict(ranger_equipo, newdata = testRF_NOID_equipo, type = "prob")

colnames(prob_predRanger_equipo)
# Asegúrate de que y_test sea un factor con los niveles correctos
#y_test <- factor(Test.rf_equipo$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))




```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_equipo[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_equipo_carga <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_equipo_carga, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

# confusion_matrixRANGER <- table(predRANGER_equipo, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRANGER)
Ranger_metrics_equipo_carga <- caret::confusionMatrix(predRANGER_equipo, y_test, positive = "Cov.Pos")
```

```{r}

library(caret)
accuracyRanger_equipo_carga <- Ranger_metrics_equipo_carga$overall["Accuracy"]
kappaRanger_equipo_carga <- Ranger_metrics_equipo_carga$overall["Kappa"]
# Métricas por clase
sensitivityRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Sensitivity"]
specificityRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Specificity"]
precisionRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Pos Pred Value"]
recallRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["F1"]
npvRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Neg Pred Value"]
prevalenceRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Prevalence"]
detection_rateRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Detection Rate"]
balanced_accuracyRanger_equipo_carga <- Ranger_metrics_equipo_carga$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRanger_equipo_carga <- sensitivityRanger_equipo_carga / (1 - specificityRanger_equipo_carga)
LR_minusRanger_equipo_carga <- (1 - sensitivityRanger_equipo_carga) / specificityRanger_equipo_carga

# Para manejar valores especiales
LR_plusRanger_equipo_carga <- ifelse(is.nan(LR_plusRanger_equipo_carga) | is.infinite(LR_plusRanger_equipo_carga), NA, LR_plusRanger_equipo_carga)
LR_minusRanger_equipo_carga <- ifelse(is.nan(LR_minusRanger_equipo_carga) | is.infinite(LR_minusRanger_equipo_carga), NA, LR_minusRanger_equipo_carga)

# Crear un dataframe con todas las métricas
metricsRanger_equipo_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  Ranger_equipo_carga = c(accuracyRanger_equipo_carga, kappaRanger_equipo_carga, sensitivityRanger_equipo_carga, specificityRanger_equipo_carga, precisionRanger_equipo_carga, 
            f1_scoreRanger_equipo_carga, npvRanger_equipo_carga, prevalenceRanger_equipo_carga, detection_rateRanger_equipo_carga, 
            balanced_accuracyRanger_equipo_carga, LR_plusRanger_equipo_carga, LR_minusRanger_equipo_carga, AUC_Ranger_equipo_carga))

# Mostrar los resultados
print(metricsRanger_equipo_carga)


```




# Modelos (con datos combat_corrected POR EQUIPO dicotomizado)



```{r}
combat_corrected_equipo_high_dic<-combat_corrected_equipo_high[1:238,]

ciego_combat_corrected_equipo_high_dic<-combat_corrected_equipo_high[239:327,]
```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_equipo_high_dic, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_equipo_dico <- training(split_data)
testData_equipo_dico <- testing(split_data)

```

```{r}
trainData_equipo_dico<-as.data.frame(trainData_equipo_dico)
trainData_equipo_dico$Y<-as.factor(trainData_equipo_dico$Y)
str(trainData_equipo_dico)
```

```{r}
#trainData_equipo_dico <- trainData_equipo_dico[, 1:(ncol(trainData_equipo_dico))-1]
train_subset <- trainData_equipo_dico[, 1:(ncol(trainData_equipo_dico))-1]
```


```{r}
library(dplyr)
#trainData_equipo_dico <- trainData_equipo_dico %>% rename(Y = label)
#trainData_equipo_dico<-trainData_equipo_dico[,-ncol(trainData_equipo_dico)]
str(trainData_equipo_dico$Y)
```


```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(train_subset, trainData_equipo_dico$Y)
train_subset <- dichotomize(train_subset, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA

```


```{r}
trainData_equipo_dico<- cbind(train_subset, trainData_equipo_dico$Y)#1=neg,2=pos
```



```{r}
str(trainData_equipo_dico)
trainData_equipo_dico<-as.data.frame(trainData_equipo_dico)
```





```{r}
library(dplyr)
trainData_equipo_dico<-as.data.frame(trainData_equipo_dico)
trainData_equipo_dico <- trainData_equipo_dico %>% rename(Y = ncol(trainData_equipo_dico))
str(trainData_equipo_dico$Y)
```


```{r}
trainData_equipo_dico<-as.data.frame(trainData_equipo_dico)
trainData_equipo_dico$Y<-as.factor(trainData_equipo_dico$Y)
str(trainData_equipo_dico)
```



```{r}
zero_var_indices <- caret::nearZeroVar(trainData_equipo_dico[,1:ncol(trainData_equipo_dico)-1])

if (length(zero_var_indices) > 0) {
    trainData_equipo_dico <- trainData_equipo_dico[, -zero_var_indices]
}

```



```{r}
train_subset<-trainData_equipo_dico[,-ncol(trainData_equipo_dico)]
thr_filtered <- thr[colnames(trainData_equipo_dico)[1:(ncol(trainData_equipo_dico) - 1)]]

```


quedan 37 variables


```{r}
testData_equipo_dico<-as.data.frame(testData_equipo_dico)
testData_equipo_dico$Y<-as.factor(testData_equipo_dico$Y)
#testData_equipo_dico<-testData_equipo_dico[,-ncol(testData_equipo_dico)]
str(testData_equipo_dico)
```



```{r}
# library(dplyr)
# testData_equipo_dico <- testData_equipo_dico %>% rename(Y = label)
# str(testData_equipo_dico$Y)
```



```{r}
test_subset <- testData_equipo_dico[, 1:ncol(testData_equipo_dico)-1]
```


```{r}
library(dplyr)

str(testData_equipo_dico$Y)
levels(testData_equipo_dico$Y)
```




```{r}
test_subset <- test_subset[colnames(trainData_equipo_dico)[1:ncol(trainData_equipo_dico)-1]]
#test_subset <-  test_subset[, colnames(trainData_equipo_dico)]
#thr_filtered <-  thr(trainData_equipo_dico)[colnames(1:ncol(trainData_equipo_dico)-1)]
#Dicotimizacion de la matriz de intensidad
test_subset <- dichotomize(test_subset, thr_filtered) 
length(thr_filtered)

```


```{r}
testData_equipo_dico<- cbind(test_subset, testData_equipo_dico$Y)#1=neg,2=pos

```

```{r}
library(dplyr)
testData_equipo_dico<-as.data.frame(testData_equipo_dico)
testData_equipo_dico <- testData_equipo_dico %>% rename(Y = ncol(testData_equipo_dico))
str(testData_equipo_dico$Y)
```

```{r}
str(testData_equipo_dico)
```




```{r}
testData_equipo_dico<-as.data.frame(testData_equipo_dico)
testData_equipo_dico$Y<-as.factor(testData_equipo_dico$Y)
str(testData_equipo_dico)
```




```{r}
# Ver la distribución de clases en ambos conjuntos
table(trainData_equipo_dico$Y)
table(testData_equipo_dico$Y)


```



```{r}
library(caret)
#set.seed(42)
#control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

# Random forest

```{r}
class(trainData_equipo_dico)
```


```{r}
trainData_equipo_dico$Y <- factor(trainData_equipo_dico$Y)  # Asegura que Y sea un factor
levels(trainData_equipo_dico$Y) <- make.names(levels(trainData_equipo_dico$Y))  # Corrige los nombres
```



```{r}
library(caret)
#control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.
#set.seed(42)
# rf_equipo_dic <- caret::train(Y ~ ., data = trainData_equipo_dico, 
#                       method = "rf",   # Método para random forest
#                       trControl = cross_val, 
#                       tuneGrid = grid_rf,  # Parámetro mtry
#                       metric = "Accuracy")  # Métrica para clasificación

```



```{r}
entrenar_modelo <- function(data, formula, metodo, tuneGrid, preProcess = NULL, cross_val, metric = "Accuracy") {
  modelo <- caret::train(
    formula,
    data = data,
    method = metodo,
    tuneGrid = tuneGrid,
    trControl = cross_val,
    metric = metric,
    preProcess = preProcess
  )
  
  # Mostrar resultados
  print(modelo)
  plot(modelo)
  
  # Extraer mejor combinación de hiperparámetros
  best_params <- modelo$bestTune
  best_row <- apply(modelo$results, 1, function(row) {
    all(row[1:length(best_params)] == as.numeric(best_params))
  })
  best_metrics <- modelo$results[best_row, c("Accuracy", "Kappa")]
  
  cat("\n>> Métricas del mejor modelo (", metodo, "):\n", sep = "")
  print(best_params)
  print(best_metrics)

  return(modelo)
}

```



```{r}
# Asegurar que Y sea un factor con nombres válidos
trainData_equipo_dico$Y <- factor(trainData_equipo_dico$Y)
levels(trainData_equipo_dico$Y) <- make.names(levels(trainData_equipo_dico$Y))

# Fórmula
formula <- Y ~ .

# RANDOM FOREST
grid_rf <- expand.grid(mtry = c(2, 5, 10))
rf_equipo_dic <- entrenar_modelo(
  data = trainData_equipo_dico,
  formula = formula,
  metodo = "rf",
  tuneGrid = grid_rf,
  cross_val = cross_val_rf
)

plot(rf_equipo_dic)
```

```{r}
rf_equipo_dic$results
```

```{r}
plot(rf_equipo_dic)
```






# KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)
#set.seed(42)
# Ajustar el modelo KNN
# KNN_equipo_dic <- caret::train(formula, 
#                     data = trainData_equipo_dico, 
#                     trControl = cross_val, 
#                     method = "knn", 
#                     metric = "Accuracy", 
#                     preProcess = c("center","scale"),
#                     tuneGrid = tuneGrid)
# 
# KNN_equipo_dic


# KNN
grid_knn <- expand.grid(k = 1:15)
KNN_equipo_dic <- entrenar_modelo(
  data = trainData_equipo_dico,
  formula = formula,
  metodo = "knn",
  tuneGrid = grid_knn,
  preProcess = c("center", "scale"),
  cross_val = cross_val_knn
)

plot(KNN_equipo_dic)

```



# GLMNET

```{r}

# tuneGrid <- expand.grid(
#   alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
#   lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
# )
# #set.seed(42)
# GLM_equipo_dic<- caret::train(formula, 
#                   data = trainData_equipo_dico,  
#                    method = "glmnet",
#                    metric = "Accuracy",
#                    tuneLength = 3,
#                    trControl = cross_val,
#                   tuneGrid=tuneGrid)
# 
# GLM_equipo_dic 
grid_glmnet <- expand.grid(
  alpha = seq(0, 1, by = 0.1),
  lambda = 10^seq(-3, 3, length = 100)
)
GLM_equipo_dic <- entrenar_modelo(
  data = trainData_equipo_dico,
  formula = formula,
  metodo = "glmnet",
  tuneGrid = grid_glmnet,
  cross_val = cross_val_glm
)





library(ggplot2)

# Convertir resultados a data.frame
res <- GLM_equipo_dic$results

# Plot con ggplot y escala logarítmica en lambda
ggplot(res, aes(x = lambda, y = Accuracy, color = as.factor(alpha))) +
  geom_point() +
  scale_x_log10() +
  labs(
    x = "Lambda (log10 scale)",
    y = "Accuracy (CV)",
    color = "Alpha",
    title = "GLMNET: Accuracy vs Lambda & Alpha"
  ) +
  theme_minimal()

```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_equipo_dic$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_equipo_dic$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_equipo_dic$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

#  Predicciones combat por equipo dicotomizado

```{r}
testData_equipo_dico$Y<-as.factor(testData_equipo_dico$Y)
str(testData_equipo_dico$Y)
```

```{r}
testData_NOID_equipo_dico <- testData_equipo_dico[, -which(names(testData_equipo_dico) == "Y")]

```

# RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRF_equipo_dico_carga<-predict(rf_equipo_dic, newdata = testData_NOID_equipo_dico)#matriz binarizada
print(predRF_equipo_dico_carga)

predRF_equipo_dico_carga <- factor(as.character(predRF_equipo_dico_carga), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predRF_equipo_dico_carga <- factor(predRF_equipo_dico_carga, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y

prob_predRFC <- predict(rf_equipo_dic, newdata = testData_NOID_equipo_dico, type = "prob")

colnames(prob_predRFC)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(testData_equipo_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))




```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRFC[, "X2"], levels = c(1, 2))
AUC_RF_equipo_dic_carga <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_equipo_dic_carga, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```


```{r}
# confusion_matrixRF <- table(predRF_equipo_dico, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixRF)
RF_metrics_equipo_carga <- caret::confusionMatrix(predRF_equipo_dico_carga, y_test, positive = "Cov.Pos")
```

```{r}
library(caret)
accuracyRF_equipo_dico_carga <- RF_metrics_equipo_carga$overall["Accuracy"]
kappaRF_equipo_dico_carga <- RF_metrics_equipo_carga$overall["Kappa"]
# Métricas por clase
sensitivityRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Sensitivity"]
specificityRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Specificity"]
precisionRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Pos Pred Value"]
recallRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["F1"]
npvRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Neg Pred Value"]
prevalenceRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Prevalence"]
detection_rateRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Detection Rate"]
balanced_accuracyRF_equipo_dico_carga <- RF_metrics_equipo_carga$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusRF_equipo_dico_carga <- sensitivityRF_equipo_dico_carga / (1 - specificityRF_equipo_dico_carga)
LR_minusRF_equipo_dico_carga <- (1 - sensitivityRF_equipo_dico_carga) / specificityRF_equipo_dico_carga

# Para manejar valores especiales
LR_plusRF_equipo_dico_carga <- ifelse(is.nan(LR_plusRF_equipo_dico_carga) | is.infinite(LR_plusRF_equipo_dico_carga), NA, LR_plusRF_equipo_dico_carga)
LR_minusRF_equipo_dico_carga <- ifelse(is.nan(LR_minusRF_equipo_dico_carga) | is.infinite(LR_minusRF_equipo_dico_carga), NA, LR_minusRF_equipo_dico_carga)

# Crear un dataframe con todas las métricas
metricsRF_equipo_dico_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  RF_equipo_dico_carga = c(accuracyRF_equipo_dico_carga, kappaRF_equipo_dico_carga, sensitivityRF_equipo_dico_carga, specificityRF_equipo_dico_carga, precisionRF_equipo_dico_carga, 
            f1_scoreRF_equipo_dico_carga, npvRF_equipo_dico_carga, prevalenceRF_equipo_dico_carga, detection_rateRF_equipo_dico_carga, 
            balanced_accuracyRF_equipo_dico_carga, LR_plusRF_equipo_dico_carga, LR_minusRF_equipo_dico_carga, AUC_RF_equipo_dic_carga))

# Mostrar los resultados
print(metricsRF_equipo_dico_carga)

# # Precisión (Accuracy)
# accuracyRF_equipo_dic <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
# print(paste("La precisión (Accuracy) es:", round(accuracyRF_equipo_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivityRF_equipo_dic <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
# print(paste("La sensibilidad es:", round(sensitivityRF_equipo_dic,2)))
# 
# # Especificidad (TNR)
# specificityRF_equipo_dic <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
# print(paste("la especificidad es:", round(specificityRF_equipo_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppvRF_equipo_dic <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
# print(paste("el VPP es:", round(ppvRF_equipo_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npvRF_equipo_dic <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
# print(paste("el VPN es: ",  round(npvRF_equipo_dic,2)))


```
```{r}

# library(irr)
# 
# # Calcular Kappa
# kappa_RFeq_dic <- kappa2(cbind(predRF_equipo_dico, testData_equipo_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RFeq_dic$value, 3)))
# kappa_RFeq_dic<-round(kappa_RFeq_dic$value, 3)
```



# Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_equipo_dic<-predict(GLM_equipo_dic, newdata = testData_NOID_equipo_dico)#matriz binarizada
print(predGLM_equipo_dic)

predGLM_equipo_dic <- factor(as.character(predGLM_equipo_dic), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predGLM_equipo_dic <- factor(predGLM_equipo_dic, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))

# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_equipo_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))

```



```{r}
# confusion_matrixGLM <- table(predGLM_equipo_dic, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixGLM)
GLM_metrics_equipo_carga <- caret::confusionMatrix(predGLM_equipo_dic, y_test, positive = "Cov.Pos")
```


```{r}
# # Precisión (Accuracy)
# accuracy_GLM_equipo_dic <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM)
# print(paste("La precisión (Accuracy) es:", round(accuracy_GLM_equipo_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_GLM_equipo_dic <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_GLM_equipo_dic,2)))
# 
# # Especificidad (TNR)
# specificity_GLM_equipo_dic <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1])
# print(paste("la especificidad es:", round(specificity_GLM_equipo_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_GLM_equipo_dic <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ])
# print(paste("el VPP es:", round(ppv_GLM_equipo_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_GLM_equipo_dic <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ])
# print(paste("el VPN es: ",  round(npv_GLM_equipo_dic,2)))


# Predicciones probabilísticas con el modelo GLM
prob_predGLM <- predict(GLM_equipo_dic, newdata = testData_NOID_equipo_dico, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_equipo_dico$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_equipo_dic_carga <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_equipo_dic_carga, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```
```{r}
library(caret)
accuracyGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$overall["Accuracy"]
kappaGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$overall["Kappa"]
# Métricas por clase
sensitivityGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Sensitivity"]
specificityGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Specificity"]
precisionGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Pos Pred Value"]
recallGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["F1"]
npvGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Neg Pred Value"]
prevalenceGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Prevalence"]
detection_rateGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Detection Rate"]
balanced_accuracyGLM_equipo_dic_carga <- GLM_metrics_equipo_carga$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusGLM_equipo_dic_carga <- sensitivityGLM_equipo_dic_carga / (1 - specificityGLM_equipo_dic_carga)
LR_minusGLM_equipo_dic_carga <- (1 - sensitivityGLM_equipo_dic_carga) / specificityGLM_equipo_dic_carga

# Para manejar valores especiales
LR_plusGLM_equipo_dic_carga <- ifelse(is.nan(LR_plusGLM_equipo_dic_carga) | is.infinite(LR_plusGLM_equipo_dic_carga), NA, LR_plusGLM_equipo_dic_carga)
LR_minusGLM_equipo_dic_carga <- ifelse(is.nan(LR_minusGLM_equipo_dic_carga) | is.infinite(LR_minusGLM_equipo_dic_carga), NA, LR_minusGLM_equipo_dic_carga)

# Crear un dataframe con todas las métricas
metricsGLM_equipo_dic_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  GLM_equipo_dic_carga = c(accuracyGLM_equipo_dic_carga, kappaGLM_equipo_dic_carga, sensitivityGLM_equipo_dic_carga, specificityGLM_equipo_dic_carga, precisionGLM_equipo_dic_carga, 
            f1_scoreGLM_equipo_dic_carga, npvGLM_equipo_dic_carga, prevalenceGLM_equipo_dic_carga, detection_rateGLM_equipo_dic_carga, 
            balanced_accuracyGLM_equipo_dic_carga, LR_plusGLM_equipo_dic_carga, LR_minusGLM_equipo_dic_carga, AUC_GLM_equipo_dic_carga))

# Mostrar los resultados
print(metricsGLM_equipo_dic_carga)


# library(irr)
# 
# # Calcular Kappa
# kappa_GLM_eq_dic <- kappa2(cbind(predGLM_equipo_dic, testData_equipo_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_GLM_eq_dic$value, 3)))
# kappa_GLM_eq_dic<-round(kappa_GLM_eq_dic$value, 3)
```



# Prediccion SVM

```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# predSVMC<-predict(SVM_c, newdata = testData_NOID)#matriz binarizada
# print(predSVMC)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada


```

```{r}

# 
# # Predicción de probabilidades
# prob_predSVMC <- predict(SVM_c, newdata = testData_NOID, type = "prob")
# 
# # Verificar el resultado
# print(prob_predSVMC)

```


```{r}
# confusion_matrixSVM <- table(predSVMC, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
# accuracy_SVMC <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
# print(accuracy_SVMC)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_SVMC <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
# print(sensitivity_SVMC)
# # Precisión negativa (VN) o especificidad
# specificity_SVMC <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
# specificity_SVMC
# 
# # Valor predictivo positivo (VPP)
# ppv_SVMC <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
# print(ppv_SVMC)
# # Valor predictivo negativo (VPN)
# npv_SVMC <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
# print(npv_SVMC)
# 
# 
# library(pROC)
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, predictor = prob_predSVMC[, "2"], levels = c(1, 2))
# AUC_SVMC <- auc(roc_curve)
# 
# print(paste("El valor de AUC es:", round(AUC_SVMC, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

# Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predKNN_equipo_dic<-predict(KNN_equipo_dic, newdata = testData_NOID_equipo_dico)#matriz binarizada
print(predKNN_equipo_dic)
predKNN_equipo_dic <- factor(as.character(predKNN_equipo_dic), 
                             levels = c("X1", "X2"),
                             labels = c("1", "2"))
predKNN_equipo_dic <- factor(predKNN_equipo_dic, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))




# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_equipo_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada
y_test <- factor(y_test, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
```

```{r}
# confusion_matrixKNN <- table(predKNN_equipo_dic, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixKNN)
KNN_metrics_equipo_carga <- caret::confusionMatrix(predKNN_equipo_dic, y_test, positive = "Cov.Pos")
```

```{r}
# #metricas
# # Precisión
# accuracy_KNN_equipo_dic <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
# print(accuracy_KNN_equipo_dic)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_KNN_equipo_dic <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
# print(sensitivity_KNN_equipo_dic)
# # Precisión negativa (VN) o especificidad
# specificity_KNN_equipo_dic <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
# specificity_KNN_equipo_dic
# 
# # Valor predictivo positivo (VPP)
# ppv_KNN_equipo_dic <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
# print(ppv_KNN_equipo_dic)
# # Valor predictivo negativo (VPN)
# npv_KNN_equipo_dic <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
# print(npv_KNN_equipo_dic)

prob_predKNNC <- predict(KNN_equipo_dic, newdata = testData_NOID_equipo_dico, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNNC))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_equipo_dico$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNC[, "X2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_equipo_dic <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_equipo_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```
```{r}
library(caret)
accuracyKNN_equipo_dic <- KNN_metrics_equipo_carga$overall["Accuracy"]
kappaKNN_equipo_dic <- KNN_metrics_equipo_carga$overall["Kappa"]
# Métricas por clase
sensitivityKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Sensitivity"]
specificityKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Specificity"]
precisionKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Pos Pred Value"]
recallKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Sensitivity"]  # Igual a sensitivity
f1_scoreKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["F1"]
npvKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Neg Pred Value"]
prevalenceKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Prevalence"]
detection_rateKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Detection Rate"]
balanced_accuracyKNN_equipo_dic <- KNN_metrics_equipo_carga$byClass["Balanced Accuracy"]

# Calcular LR+ y LR- que no vienen directamente en confusionMatrix
LR_plusKNN_equipo_dic <- sensitivityKNN_equipo_dic / (1 - specificityKNN_equipo_dic)
LR_minusKNN_equipo_dic <- (1 - sensitivityKNN_equipo_dic) / specificityKNN_equipo_dic

# Para manejar valores especiales
LR_plusKNN_equipo_dic <- ifelse(is.nan(LR_plusKNN_equipo_dic) | is.infinite(LR_plusKNN_equipo_dic), NA, LR_plusKNN_equipo_dic)
LR_minusKNN_equipo_dic <- ifelse(is.nan(LR_minusKNN_equipo_dic) | is.infinite(LR_minusKNN_equipo_dic), NA, LR_minusKNN_equipo_dic)

# Crear un dataframe con todas las métricas
metricsKNN_equipo_dic_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
             "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  KNN_equipo_dic = c(accuracyKNN_equipo_dic, kappaKNN_equipo_dic, sensitivityKNN_equipo_dic, specificityKNN_equipo_dic, precisionKNN_equipo_dic, 
            f1_scoreKNN_equipo_dic, npvKNN_equipo_dic, prevalenceKNN_equipo_dic, detection_rateKNN_equipo_dic, 
            balanced_accuracyKNN_equipo_dic, LR_plusKNN_equipo_dic, LR_minusKNN_equipo_dic, AUC_KNN_equipo_dic))

# Mostrar los resultados
print(metricsKNN_equipo_dic_carga)

# library(irr)
# 
# # Calcular Kappa
# kappa_KNN_eq_dic <- kappa2(cbind(predKNN_equipo_dic, testData_equipo_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_KNN_eq_dic$value, 3)))
# kappa_KNN_eq_dic<-round(kappa_KNN_eq_dic$value, 3)
```




#RANGER
```{r}
Train.rf_equipo_dico  <- as.data.frame(trainData_equipo_dico) 

Test.rf_equipo_dico <- as.data.frame(testData_equipo_dico)
```

```{r}
y_train <- trainData_equipo_dico$Y
x_train <- trainData_equipo_dico[, setdiff(names(trainData_equipo_dico), "Y")]
y_test <- testData_equipo_dico$Y
x_test <- testData_equipo_dico[, setdiff(names(testData_equipo_dico), "Y")]

# Crear el objeto recipe solo con las variables predictoras
objeto_recipe <- recipe(Y ~ ., data = trainData_equipo_dico)

# Preparar el objeto recipe
trained_recipe <- prep(objeto_recipe, training = trainData_equipo_dico)

# Aplicar bake solo a las variables predictoras
x_train_baked <- bake(trained_recipe, new_data = x_train)
x_test_baked <- bake(trained_recipe, new_data = x_test)

# Volver a juntar Y con los datos procesados
trainData_equipo_dico <- cbind(x_train_baked, Y = y_train)
testData_equipo_dico <- cbind(x_test_baked, Y = y_test)

```

```{r}
# Submuestras y repeticiones
set.seed(42)
# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 


mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = seq(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_equipo_dico$Y <- factor(as.numeric(factor(Train.rf_equipo_dico$Y)))
Train.rf_equipo_dico$Y <- factor(Train.rf_equipo_dico$Y, levels = c("1", "2"))
levels(Train.rf_equipo_dico$Y) <- make.names(levels(Train.rf_equipo_dico$Y))

```





```{r}
class(Train.rf_equipo_dico)
```


```{r}
Train.rf_equipo_dico <- as.data.frame(Train.rf_equipo_dico)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_equipo_dico <- as.data.frame(Train.rf_equipo_dico)

# Convertimos Y a factor
Train.rf_equipo_dico$Y <- as.factor(Train.rf_equipo_dico$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-100
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_equipo_dico <- caret::train(Y ~ .,
                data = Train.rf_equipo_dico, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val,
                num.trees = n_trees)
               #allowParallel=FALSE)

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)
results_equipo_dico
```

```{r}
# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_equipo_dico <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_equipo_dico[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_equipo_dico,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val,
    num.trees = nt
  )
}

```





```{r}
  resultados_modelos <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_equipo_dico, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_equipo_dico, function(m) max(m$results$Kappa, na.rm = TRUE)),
  mtry = sapply(modelos_equipo_dico, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_equipo_dico, function(m) m$bestTune$min.node.size)
)

# Ordenamos los modelos de mejor a peor Accuracy
resultados_modelos <- resultados_modelos[order(-resultados_modelos$Accuracy), ]

print(head(resultados_modelos, 20)) 

```


```{r}
# Encontrar el índice del modelo con mejor Accuracy en la tabla de resultados
mejor_idx <- which.max(resultados_modelos$Accuracy)

# Extraer la mejor combinación de hiperparámetros
mejor_num_trees <- resultados_modelos$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos$min.node.size[mejor_idx]

# Extraer el modelo correspondiente en la lista
mejor_modelo <- modelos_equipo_dico[[as.character(mejor_num_trees)]]

# Mostrar los hiperparámetros seleccionados
cat("Mejor modelo seleccionado:\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")

# Mostrar detalles del mejor modelo
print(mejor_modelo)

```


```{r}
ranger_equipo_dico <- caret::train(
  Y ~ .,
  data = Train.rf_equipo_dico, 
  method = "ranger",
  tuneGrid = data.frame(mtry = 3, min.node.size = 9, splitrule = "gini"), 
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val,  # Si querés usar validación cruzada
  num.trees = 100  # Número de árboles fijo
)
print(ranger_equipo_dico)
```


```{r}
Test.rf_equipo_dico$Y<-as.factor(Test.rf_equipo_dico$Y)
str(Test.rf_equipo_dico$Y)
```





```{r}
# ============================================================================
# CÓDIGO RANGER IDEAL - VERSIÓN COMPLETA Y OPTIMIZADA
# ============================================================================

# 1. PREPARACIÓN INICIAL DE DATOS
# ============================================================================

# Convertir a data.frame
Train.rf_equipo_dico <- as.data.frame(trainData_equipo_dico) 
Test.rf_equipo_dico <- as.data.frame(testData_equipo_dico)

# 2. PREPROCESAMIENTO CON RECIPES (MANTENER - ES MUY BUENO)
# ============================================================================

# Separar Y de las variables predictoras para el preprocesamiento
y_train <- trainData_equipo_dico$Y
x_train <- trainData_equipo_dico[, setdiff(names(trainData_equipo_dico), "Y")]
y_test <- testData_equipo_dico$Y
x_test <- testData_equipo_dico[, setdiff(names(testData_equipo_dico), "Y")]

# Crear el objeto recipe
objeto_recipe <- recipe(Y ~ ., data = trainData_equipo_dico)

# Preparar el objeto recipe
trained_recipe <- prep(objeto_recipe, training = trainData_equipo_dico)

# Aplicar bake a las variables predictoras
x_train_baked <- bake(trained_recipe, new_data = x_train)
x_test_baked <- bake(trained_recipe, new_data = x_test)

# Volver a juntar Y con los datos procesados
trainData_equipo_dico <- cbind(x_train_baked, Y = y_train)
testData_equipo_dico <- cbind(x_test_baked, Y = y_test)

# Actualizar Train.rf después del preprocesamiento
Train.rf_equipo_dico <- as.data.frame(trainData_equipo_dico)
Test.rf_equipo_dico <- as.data.frame(testData_equipo_dico)

# 3. CONFIGURACIÓN DE VALIDACIÓN CRUZADA
# ============================================================================

# Parámetros de validación cruzada
set.seed(42)
particiones <- 3
repeticiones <- 5

# 4. CONFIGURACIÓN DE HIPERPARÁMETROS
# ============================================================================

# Configuración de hiperparámetros
seed.rf <- 42
set.seed(seed.rf) 

x <- Train.rf_equipo_dico[, -ncol(Train.rf_equipo_dico)]

# Hiperparámetros a probar
mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)
hiperparametros <- expand.grid(
  mtry = mtry,
  min.node.size = min.node.size,
  splitrule = "gini"
)

# 5. CONFIGURACIÓN DE SEMILLAS PARA REPRODUCIBILIDAD
# ============================================================================

set.seed(seed.rf)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# 6. CONFIGURACIÓN DE CONTROL DE ENTRENAMIENTO
# ============================================================================

cross_val <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds
)

# 7. PREPARACIÓN DE LA VARIABLE OBJETIVO
# ============================================================================

# Convertir Y a factor con niveles apropiados
Train.rf_equipo_dico$Y <- factor(as.numeric(factor(Train.rf_equipo_dico$Y)))
Train.rf_equipo_dico$Y <- factor(Train.rf_equipo_dico$Y, levels = c("1", "2"))
levels(Train.rf_equipo_dico$Y) <- make.names(levels(Train.rf_equipo_dico$Y))

# Asegurar que es data.frame
Train.rf_equipo_dico <- as.data.frame(Train.rf_equipo_dico)
Train.rf_equipo_dico$Y <- as.factor(Train.rf_equipo_dico$Y)

# 8. ENTRENAMIENTO CON DIFERENTES NÚMEROS DE ÁRBOLES
# ============================================================================

# Definir los valores de num.trees a probar
num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

# Lista para almacenar los modelos
modelos_equipo_dico <- list()

# Establecer semilla para reproducibilidad
set.seed(80)

cat("=== INICIANDO ENTRENAMIENTO DE MODELOS ===\n")

# Iterar sobre cada cantidad de árboles
for (nt in num_trees_range) {
  cat("Entrenando modelo con", nt, "árboles...\n")
  
  modelos_equipo_dico[[as.character(nt)]] <- caret::train(
    Y ~ .,
    data = Train.rf_equipo_dico,
    method = "ranger",
    tuneGrid = hiperparametros,
    metric = "Accuracy",
    importance = "impurity",
    trControl = cross_val,
    num.trees = nt
  )
  
  # Mostrar progreso
  cat("✓ Modelo con", nt, "árboles completado\n")
}

cat("=== ENTRENAMIENTO COMPLETADO ===\n\n")

# 9. ANÁLISIS COMPLETO DE RESULTADOS
# ============================================================================

cat("=== ANÁLISIS DE RESULTADOS ===\n")

# Crear tabla de resultados
resultados_modelos_dico <- data.frame(
  num_trees = num_trees_range,
  Accuracy = sapply(modelos_equipo_dico, function(m) max(m$results$Accuracy, na.rm = TRUE)),
  Kappa = sapply(modelos_equipo_dico, function(m) max(m$results$Kappa, na.rm = TRUE)),
  AccuracySD = sapply(modelos_equipo_dico, function(m) {
    idx <- which.max(m$results$Accuracy)
    m$results$AccuracySD[idx]
  }),
  KappaSD = sapply(modelos_equipo_dico, function(m) {
    idx <- which.max(m$results$Accuracy)
    m$results$KappaSD[idx]
  }),
  mtry = sapply(modelos_equipo_dico, function(m) m$bestTune$mtry),
  min.node.size = sapply(modelos_equipo_dico, function(m) m$bestTune$min.node.size)
)

# Ordenar los modelos de mejor a peor Accuracy
resultados_modelos_dico <- resultados_modelos_dico[order(-resultados_modelos_dico$Accuracy), ]

cat("Top 10 modelos por Accuracy:\n")
print(head(resultados_modelos_dico, 10))

# 10. SELECCIÓN DEL MEJOR MODELO
# ============================================================================

# Encontrar el mejor modelo
mejor_idx <- which.max(resultados_modelos_dico$Accuracy)
mejor_num_trees <- resultados_modelos_dico$num_trees[mejor_idx]
mejor_mtry <- resultados_modelos_dico$mtry[mejor_idx]
mejor_min_node_size <- resultados_modelos_dico$min.node.size[mejor_idx]
mejor_accuracy <- resultados_modelos_dico$Accuracy[mejor_idx]
mejor_kappa <- resultados_modelos_dico$Kappa[mejor_idx]

# Extraer el modelo correspondiente
mejor_modelo_dico <- modelos_equipo_dico[[as.character(mejor_num_trees)]]

# Mostrar resultados del mejor modelo
cat("\n=== MEJOR MODELO SELECCIONADO ===\n")
cat("Número de árboles:", mejor_num_trees, "\n")
cat("mtry:", mejor_mtry, "\n")
cat("min.node.size:", mejor_min_node_size, "\n")
cat("Accuracy:", round(mejor_accuracy, 4), "\n")
cat("Kappa:", round(mejor_kappa, 4), "\n")
cat("=====================================\n\n")

# 11. MODELO FINAL CON HIPERPARÁMETROS OPTIMIZADOS
# ============================================================================

cat("=== ENTRENANDO MODELO FINAL OPTIMIZADO ===\n")

# Entrenar modelo final con los mejores hiperparámetros
ranger_equipo_dico_final <- caret::train(
  Y ~ .,
  data = Train.rf_equipo_dico, 
  method = "ranger",
  tuneGrid = data.frame(
    mtry = mejor_mtry, 
    min.node.size = mejor_min_node_size, 
    splitrule = "gini"
  ), 
  metric = "Accuracy",
  importance = "impurity",
  trControl = cross_val,
  num.trees = mejor_num_trees
)

cat("Modelo final optimizado completado!\n")
print(ranger_equipo_dico_final)

# 12. RESUMEN FINAL Y EXPORTACIÓN DE RESULTADOS
# ============================================================================

cat("\n=== RESUMEN FINAL ===\n")
cat("Total de modelos evaluados:", length(num_trees_range), "\n")
cat("Mejor configuración encontrada:\n")
cat("- Árboles:", mejor_num_trees, "\n")
cat("- mtry:", mejor_mtry, "\n") 
cat("- min.node.size:", mejor_min_node_size, "\n")
cat("- Accuracy final:", round(mejor_accuracy, 4), "±", 
    round(resultados_modelos_dico$AccuracySD[mejor_idx], 4), "\n")
cat("- Kappa final:", round(mejor_kappa, 4), "±", 
    round(resultados_modelos_dico$KappaSD[mejor_idx], 4), "\n")

# Guardar tabla de resultados (opcional)
# write.csv(resultados_modelos_dico, "resultados_ranger_equipo_dico.csv", row.names = FALSE)

cat("\n¡PROCESO COMPLETADO EXITOSAMENTE!\n")

# ============================================================================
# VARIABLES DISPONIBLES AL FINAL:
# - mejor_modelo_dico: El mejor modelo de la búsqueda
# - ranger_equipo_dico_final: Modelo final reentrenado
# - resultados_modelos_dico: Tabla completa de resultados
# - Train.rf_equipo_dico: Datos de entrenamiento procesados
# - Test.rf_equipo_dico: Datos de prueba procesados
# ============================================================================
```





```{r}
# 1. Preparar datos de test (sin la variable Y)
testRF_NOID_equipo_dico <- Test.rf_equipo_dico[, -which(names(Test.rf_equipo_dico) == "Y")]

# 2. Realizar predicciones
# Predicciones de clase
predRANGER_equipo_dico <- predict(ranger_equipo_dico_final, newdata = testRF_NOID_equipo_dico)
print("Predicciones originales:")
print(predRANGER_equipo_dico)

# Predicciones de probabilidad
prob_predRanger <- predict(ranger_equipo_dico_final, newdata = testRF_NOID_equipo_dico, type = "prob")
print("Columnas de probabilidades:")
print(colnames(prob_predRanger))

# 3. Convertir predicciones a formato interpretable (una sola conversión)
predRANGER_equipo_dico <- factor(predRANGER_equipo_dico, 
                                levels = c("X1", "X2"), 
                                labels = c("Cov.Neg", "Cov.Pos"))

# 4. Preparar etiquetas reales
y_test <- factor(Test.rf_equipo_dico$Y, 
                levels = c(1, 2), 
                labels = c("Cov.Neg", "Cov.Pos"))

# 5. Calcular AUC y curva ROC
library(pROC)

# Convertir a numérico para ROC (1 = Cov.Neg, 2 = Cov.Pos)
y_test_numeric <- as.numeric(y_test)

# Crear curva ROC
roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger[, "X2"], 
                 levels = c(1, 2))

AUC_Ranger_equipo_dich_carga <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_equipo_dich_carga, 3)))

# Visualizar curva ROC
plot(roc_curve, 
     main = "Curva ROC - Ranger Model", 
     col = "blue", 
     lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")  # Línea de referencia
legend("bottomright", 
       legend = paste("AUC =", round(AUC_Ranger_equipo_dich_carga, 3)),
       col = "blue", lwd = 2)

# 6. Matriz de confusión y métricas
library(caret)

RANGER_metrics_equipo_carga <- caret::confusionMatrix(predRANGER_equipo_dico, 
                                              y_test, 
                                              positive = "Cov.Pos")

print("Matriz de Confusión:")
print(RANGER_metrics_equipo_carga)

# 7. Extraer métricas individuales
accuracyRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$overall["Accuracy"]
kappaRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$overall["Kappa"]

# Métricas por clase
sensitivityRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Sensitivity"]
specificityRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Specificity"]
precisionRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Pos Pred Value"]
f1_scoreRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["F1"]
npvRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Neg Pred Value"]
balanced_accuracyRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Balanced Accuracy"]

# 8. Calcular Likelihood Ratios con manejo de casos especiales
LR_plusRanger_equipo_dich_carga <- ifelse(
  specificityRanger_equipo_dich_carga == 1, 
  Inf,  # Si especificidad = 1, LR+ = Infinito
  sensitivityRanger_equipo_dich_carga / (1 - specificityRanger_equipo_dich_carga)
)

LR_minusRanger_equipo_dich_carga <- ifelse(
  specificityRanger_equipo_dich_carga == 0,
  Inf,  # Si especificidad = 0, LR- = Infinito
  (1 - sensitivityRanger_equipo_dich_carga) / specificityRanger_equipo_dich_carga
)

# 9. Crear tabla resumen de métricas
metricsRanger_equipo_dich_carga <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
             "F1_Score", "NPV", "Balanced_Accuracy", "LR+", "LR-", "AUC"),
  Ranger_equipo_dich_carga  = c(
    round(accuracyRanger_equipo_dich_carga, 4),
    round(kappaRanger_equipo_dich_carga, 4),
    round(sensitivityRanger_equipo_dich_carga, 4),
    round(specificityRanger_equipo_dich_carga, 4),
    round(precisionRanger_equipo_dich_carga, 4),
    round(f1_scoreRanger_equipo_dich_carga, 4),
    round(npvRanger_equipo_dich_carga, 4),
    round(balanced_accuracyRanger_equipo_dich_carga, 4),
    round(LR_plusRanger_equipo_dich_carga, 4),
    round(LR_minusRanger_equipo_dich_carga, 4),
    round(AUC_Ranger_equipo_dich_carga, 4)
  )
)

# Mostrar resultados finales
print("=== RESUMEN DE MÉTRICAS ===")
print(metricsRanger_equipo_dich_carga)

# 10. Verificaciones adicionales
print("=== VERIFICACIONES ===")
print(paste("Número de predicciones:", length(predRANGER_equipo_dico)))
print(paste("Número de valores reales:", length(y_test)))
print(paste("Distribución de predicciones:"))
print(table(predRANGER_equipo_dico))
print(paste("Distribución de valores reales:"))
print(table(y_test))
```




# 
# 
# 
# ```{r}
# testRF_NOID_equipo_dico <- Test.rf_equipo_dico[, -which(names(Test.rf_equipo_dico) == "Y")]
# 
# ```
# 
# 
# 
# ```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# predRANGER_equipo_dico<-predict(ranger_equipo_dico, newdata = testRF_NOID_equipo_dico)#matriz binarizada
# print(predRANGER_equipo_dico)
# 
# predRANGER_equipo_dico <- factor(as.character(predRANGER_equipo_dico), 
#                              levels = c("X1", "X2"),
#                              labels = c("1", "2"))
# predRANGER_equipo_dico <- factor(predRANGER_equipo_dico, levels = c("1", "2"), labels = c("Cov.Neg", "Cov.Pos"))
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# #y_test <- testData$Y
# 
# prob_predRanger <- predict(results_equipo_dico, newdata = testRF_NOID_equipo_dico, type = "prob")
# 
# colnames(prob_predRanger)
# # Asegúrate de que y_test sea un factor con los niveles correctos
# y_test <- factor(Test.rf_equipo_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))
# 
# ```
# 
# 
# ```{r}
# library(pROC)
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, 
#                  predictor = prob_predRanger[, "X2"], 
#                  levels = c(1, 2))
# AUC_Ranger_equipo_dich_carga <- auc(roc_curve)
# print(paste("El valor de AUC es:", round(AUC_Ranger_equipo_dich_carga, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
# 
# # confusion_matrixRANGER <- table(predRANGER_equipo_dico, y_test)
# # # Imprimir la matriz de confusión
# # print(confusion_matrixRANGER)
# RANGER_metrics_equipo_carga <- caret::confusionMatrix(predRANGER_equipo_dico, y_test, positive = "Cov.Pos")
# ```
# 
# ```{r}
# library(caret)
# accuracyRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$overall["Accuracy"]
# kappaRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$overall["Kappa"]
# # Métricas por clase
# sensitivityRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Sensitivity"]
# specificityRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Specificity"]
# precisionRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Pos Pred Value"]
# recallRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Sensitivity"]  # Igual a sensitivity
# f1_scoreRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["F1"]
# npvRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Neg Pred Value"]
# prevalenceRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Prevalence"]
# detection_rateRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Detection Rate"]
# balanced_accuracyRanger_equipo_dich_carga <- RANGER_metrics_equipo_carga$byClass["Balanced Accuracy"]
# 
# # Calcular LR+ y LR- que no vienen directamente en confusionMatrix
# LR_plusRanger_equipo_dich_carga <- sensitivityRanger_equipo_dich_carga / (1 - specificityRanger_equipo_dich_carga)
# LR_minusRanger_equipo_dich_carga <- (1 - sensitivityRanger_equipo_dich_carga) / specificityRanger_equipo_dich_carga
# 
# # Para manejar valores especiales
# LR_plusRanger_equipo_dich_carga <- ifelse(is.nan(LR_plusRanger_equipo_dich_carga) | is.infinite(LR_plusRanger_equipo_dich_carga), NA, LR_plusRanger_equipo_dich_carga)
# LR_minusRanger_equipo_dich_carga <- ifelse(is.nan(LR_minusRanger_equipo_dich_carga) | is.infinite(LR_minusRanger_equipo_dich_carga), NA, LR_minusRanger_equipo_dich_carga)
# 
# # Crear un dataframe con todas las métricas
# metricsRanger_equipo_dich_carga <- data.frame(
#   Metric = c("Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", 
#              "F1_Score", "NPV", "Prevalence", "Detection_Rate", 
#              "Balanced_Accuracy", "LR+", "LR-", "AUC"),
#   Ranger_equipo_dich_carga = c(accuracyRanger_equipo_dich_carga, kappaRanger_equipo_dich_carga, sensitivityRanger_equipo_dich_carga, specificityRanger_equipo_dich_carga, precisionRanger_equipo_dich_carga, 
#             f1_scoreRanger_equipo_dich_carga, npvRanger_equipo_dich_carga, prevalenceRanger_equipo_dich_carga, detection_rateRanger_equipo_dich_carga, 
#             balanced_accuracyRanger_equipo_dich_carga, LR_plusRanger_equipo_dich_carga, LR_minusRanger_equipo_dich_carga, AUC_Ranger_equipo_dich_carga))
# 
# # Mostrar los resultados
# print(metricsRanger_equipo_dich_carga)


# # Precisión (Accuracy)
# accuracy_RANGER_eq_dic <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
# print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_eq_dic,2)))
# 
# # Sensibilidad (Recall o TPR)
# sensitivity_RANGER_eq_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
# print(paste("La sensibilidad es:", round(sensitivity_RANGER_eq_dic,2)))
# 
# # Especificidad (TNR)
# specificity_RANGER_eq_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
# print(paste("la especificidad es:", round(specificity_RANGER_eq_dic,2)))
# 
# # Valor Predictivo Positivo (PPV o Precision)
# ppv_RANGER_eq_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
# print(paste("el VPP es:", round(ppv_RANGER_eq_dic,2)))
# 
# # Valor Predictivo Negativo (VPN)
# npv_RANGER_eq_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
# print(paste("el VPN es: ",  round(npv_RANGER_eq_dic,2)))


```


```{r}
# 
# library(irr)
# 
# # Calcular Kappa
# kappa_RANGEReq_dic <- kappa2(cbind(predRANGER_equipo_dico, Test.rf_equipo_dico$Y))
# 
# # Ver el valor de Kappa
# print(paste("El índice Kappa es:", round(kappa_RANGEReq_dic$value, 3)))
# kappa_RANGEReq_dic<-round(kappa_RANGEReq_dic$value, 3)
```




# COMPARACION MODELOS

```{r}
# resultados_modelos_equipo <- data.frame(
#   Modelo = c("RF_equipodic", "GLMC_equipodic", "KNN_equipodic", "Ranger_equipodic",  "RF_equipo", "GLM_equipo", "KNN_equipo", "Ranger_equipo"),
#   Accuracy = c(accuracyRF_equipo_dic, accuracy_GLM_equipo_dic , accuracy_KNN_equipo_dic, accuracy_RANGER_eq_dic, accuracy_RF_equipo, accuracy_GLM_equipo , accuracy_KNN_equipo, accuracy_RANGER_eq),
#   Sensibilidad=c(sensitivityRF_equipo_dic, sensitivity_GLM_equipo_dic, sensitivity_KNN_equipo_dic, sensitivity_RANGER_eq_dic, sensitivity_RF_equipo, sensitivity_GLM_equipo, sensitivity_KNN_equipo, sensitivity_RANGER_eq),
#   Especificidad = c(specificityRF_equipo_dic, specificity_GLM_equipo_dic, specificity_KNN_equipo_dic, specificity_RANGER_eq_dic, specificityRF_equipo, specificity_GLM_equipo, specificity_KNN_equipo, specificity_RANGER_eq),
#   AUC= c(AUC_RF_equipo_dic, AUC_GLM_equipo_dic, AUC_KNN_equipo, AUC_Ranger_equipo_dich, AUC_RF_equipo, AUC_GLM_equipo, AUC_KNN_equipo, AUC_Ranger_equipo),
# kappa= c(kappa_RFeq_dic, kappa_GLM_equipo, kappa_KNN_eq_dic, kappa_RANGEReq_dic, kappa_RFeq, kappa_GLM_equipo, kappa_KNNeq, kappa_RANGEReq))
# 
# 
# print(resultados_modelos_equipo)

```




```{r}


# Cargar la librería
# library(writexl)
# 
# # Exportar a Excel
# write_xlsx(resultados_modelos_equipo, "C:/Users/karin/Desktop/MCD/TESIS/resultados_modelos_equipo_cv.xlsx")
```






