---
title: "hc"
author: "Karina Roitman"
date: "2024-12-29"
output: html_document
---

---
title: "Untitled"
author: "Karina Roitman"
date: "2024-12-27"
output: html_document
---


COMBAT SIN DICOTOMIZAR


```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```




```{r}
combat_corrected_filtrado<- cbind(combat_corrected_filtrado, label=Datos_filtrados$PCR.Cov)#1=neg,2=pos
```

```{r}
combat_corrected_filtrado<-as.data.frame(combat_corrected_filtrado)
combat_corrected_filtrado$label<-as.factor(combat_corrected_filtrado$label)
str(combat_corrected_filtrado)
```

```{r}


library(dplyr)
combat_corrected_filtrado <- combat_corrected_filtrado %>% rename(Y = label)
str(combat_corrected_filtrado$Y)
```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_filtrado, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData$Y)
table(testData$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
#trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
#testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
#colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
#table(trainData$Y)
#table(testData$Y)

```

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

#19.1 Random forest

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

RF_filtrado_hc <- train(Y ~ ., data = trainData, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
RF_filtrado_hc
```

```{r}
plot(RF_filtrado_hc)
```


#con ranger

```{r}

library(ranger)
# Crear un grid para los hiperparámetros
hyper_grid <- expand.grid(
  mtry        = seq(5, 10, by = 1),      # Número de variables seleccionadas al azar
  node_size   = seq(7, 9, by = 2),       # Tamaño mínimo de nodos
  sample_size = c(0.55, 0.632),          # Fracción de muestras para cada árbol
  Accuracy    = 0,                       # Métricas inicializadas en 0
  Sensitivity = 0,
  F1          = 0,
  Specificity =0,
  AUC         = 0

)

# Iterar sobre el grid
for (i in 1:nrow(hyper_grid)) {
  
  # Entrenar el modelo con ranger
  ranger_hc <- ranger(
    dependent.variable.name = "Y",        # Nombre de la variable dependiente
    data                    = trainData,  # Datos de entrenamiento
    num.trees               = 500,        # Número de árboles
    mtry                    = hyper_grid$mtry[i],
    min.node.size           = hyper_grid$node_size[i],
    sample.fraction         = hyper_grid$sample_size[i],
    probability             = TRUE,       # Probabilidades para clasificación
    seed                    = 123         # Semilla para reproducibilidad
  )
  
}
  # Predicciones en el conjunto de entrenamiento (fuera de bolsa)
  predictions <- as.data.frame(ranger_hc$predictions)
  
  # Convertir probabilidades a clases (asumiendo 2 = positivo, 1 = negativo)
  predicted_class <- ifelse(predictions[, 2] > 0.5, 2, 1)
  
  # Asegurarse de que los factores tengan la clase positiva como 2
  predicted_class <- factor(predicted_class, levels = c(1, 2))
  true_class <- factor(trainData$Y, levels = c(1, 2))
  
  # Calcular la matriz de confusión
  confusion <- caret::confusionMatrix(
    data = predicted_class,
    reference = true_class,
    positive = "2"  # Establecer explícitamente la clase positiva como 2
  )
  
#   # Guardar métricas en el grid
#   hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
#   hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
#   hyper_grid$F1[i] <- 2 * (confusion$byClass["Sensitivity"] * confusion$byClass["Pos Pred Value"]) /
#                       (confusion$byClass["Sensitivity"] + confusion$byClass["Pos Pred Value"])
# }
# 
# # Mostrar los mejores resultados ordenados por la métrica deseada (por ejemplo, F1)
# library(dplyr)
# hyper_grid %>% 
#   arrange(desc(F1)) %>% 
#   head(10)
  
 
  roc_obj <- roc(trainData$Y, predictions[, 2], levels = c(1, 2), direction = "<")
  auc_rangerCarga <- auc(roc_obj)
  

# Ver la matriz de confusión
print(confusion)

# Guardar métricas en el grid
hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
hyper_grid$F1[i] <- confusion$byClass["F1"]

best_row <- which.max(hyper_grid$Sensitivity)
best_rangerHC <- hyper_grid[best_row, ]
print(best_rangerHC)

# Reorganizar las columnas en el orden deseado
best_rangerHC <- best_rangerHC[, c("Accuracy", "Sensitivity", "Specificity", "AUC")]

# Imprimir los resultados en el orden especificado
print(best_rangerHC)
#
#



```




```{r}
summary(rf_ranger_hc)
```



#19.2 GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # Método para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


GBM_modelfHC <- train(
  Y ~ .,                        # Reemplaza Y con tu variable dependiente
  data = trainData,            # Tu conjunto de entrenamiento
  method = "gbm",              # Método GBM
  trControl = control1,         # Control de entrenamiento
  tuneGrid = expand.grid(
    interaction.depth = 3,     # Profundidad máxima de los árboles
    n.trees = 100,             # Número de árboles
    shrinkage = 0.1,           # Tasa de aprendizaje
    n.minobsinnode = 10        # Mínimo de observaciones en cada nodo
  ),
  metric = "Accuracy",         # Métrica de rendimiento
  verbose = TRUE               # Mostrar progreso
)
GBM_modelfHC

```

```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

#19.3 SVM

```{r}
SVM_modelfHC <- train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
SVM_modelfHC$bestTun    #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

SVM_modelfHC
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```




```{r}
plot(SVM_modelfHC)
```


#19.3 KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
KNN_modelofHC <- train(formula, 
                    data = trainData, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_modelofHC
```

```{r}
plot(KNN_modelofHC)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)

GLM_modefHC <- train(formula, 
                  data = trainData,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_modefHC 
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_modefHC$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_modefHC$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_modefHC$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# 20. Prediccion

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```

```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

#20.1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_filtrado_hc, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(prediccionesRF, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)

```

```{r}
# Precisión (<)
accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(paste("La precisión (Accuracy) es:", round(accuracy_RF,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RF,2)))

# Especificidad (TNR)
specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
print(paste("la especificidad es:", round(specificity_RF,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(paste("el VPP es:", round(ppv_RF,2)))

# Valor Predictivo Negativo (VPN)
npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(paste("el VPN es: ",  round(npv_RF,2)))

```

#20.2 Prediccion GBM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesGBM<-predict(GBM_modelfHC, newdata = testData_NOID)#matriz binarizada
print(prediccionesGBM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGBM <- table(prediccionesGBM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGBM)

```


```{r}
# Precisión (Accuracy)
accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
print(paste("La precisión (Accuracy) es:", round(accuracy_GBM,2)))

# Sensibilidad (Recall o TPR)
sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
print(paste("La sensibilidad es:", round(sensitivity_GBM,2)))

# Especificidad (TNR)
specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
print(paste("la especificidad es:", round(specificity_GBM,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
print(paste("el VPP es:", round(ppv_GBM,2)))

# Valor Predictivo Negativo (VPN)
npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
print(paste("el VPN es: ",  round(npv_GBM,2)))

```

#20.3 Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesSVM<-predict(SVM_modelfHC, newdata = testData_NOID)#matriz binarizada
print(prediccionesSVM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixSVM <- table(prediccionesSVM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
print(accuracy_SVM)
# Precisión positiva (VP) o sensibilidad
sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
print(sensitivity_SVM)
# Precisión negativa (VN) o especificidad
specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
specificity_SVM

# Valor predictivo positivo (VPP)
ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
print(ppv_SVM)
# Valor predictivo negativo (VPN)
npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
print(npv_SVM)


```


#20.4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN2f<-predict(KNN_modelo2f, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN2f)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixKNN2f <- table(prediccionesKNN2f, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN2f)

```

```{r}
#metricas
# Precisión
accuracy_KNN2f <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN2f)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN2f <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
print(sensitivity_KNN2f)
# Precisión negativa (VN) o especificidad
specificity_KNN2f <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
specificity_KNN2f

# Valor predictivo positivo (VPP)
ppv_KNN2f <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN2f)
# Valor predictivo negativo (VPN)
npv_KNN2f <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN2f)

```

#20: COMPARACION MODELOS

```{r}
resultados_modelos <- data.frame(
  Modelo = c("RF_model", "GBM_model", "SVM_model", "KNN_model"),
  Accuracy = c(accuracy_RF2f, accuracy_GBM2f, accuracy_SVM2f, accuracy_KNN2f),
  Sensibilidad=c(sensitivity_RF2f, sensitivity_GBM2f, sensitivity_SVM2f, sensitivity_KNN2f),
  Especificidad = c(specificity_RF2f, specificity_GBM2f, specificity_SVM2f, specificity_KNN2f)
)

# Imprimir la tabla
print(resultados_modelos)
```




COMBAT 2


```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(combat_corrected2f, Datos_filtrados$PCR.Cov)
combat_corrected_dicho2 <- dichotomize(combat_corrected2f, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA
```

```{r}
combat_corrected_dicho2<- cbind(combat_corrected_dicho2, label=Datos_filtrados$PCR.Cov)#1=neg,2=pos
```

```{r}
combat_corrected_dicho2<-as.data.frame(combat_corrected_dicho2)
combat_corrected_dicho2$label<-as.factor(combat_corrected_dicho2$label)
str(combat_corrected_dicho2)
```

```{r}
library(dplyr)
combat_corrected_dicho2 <- combat_corrected_dicho2 %>% rename(Y = label)
str(combat_corrected_dicho2$Y)
```
```{r}
combat_corrected_dicho2<-combat_corrected_dicho2[,-1]
```



```{r}


zero_var_indices <- nearZeroVar(combat_corrected_dicho2)
combat_corrected_dicho2 <- combat_corrected_dicho2
if (length(zero_var_indices) > 0) {
    combat_corrected_dicho2 <- combat_corrected_dicho2[, -zero_var_indices]
}

combat_corrected_dicho2
class(combat_corrected_dicho2)

dim(combat_corrected_dicho2)

```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_dicho2, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData$Y)
table(testData$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
#trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
#testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
#colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
#table(trainData$Y)
#table(testData$Y)

```

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

#19.1 Random forest

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

RF_model <- train(Y ~ ., data = trainData, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
RF_model
```

```{r}
plot(RF_model)
```


#con ranger

```{r}

library(ranger)
# Crear un grid para los hiperparámetros
hyper_grid <- expand.grid(
  mtry        = seq(5, 10, by = 1),      # Número de variables seleccionadas al azar
  node_size   = seq(7, 9, by = 2),       # Tamaño mínimo de nodos
  sample_size = c(0.55, 0.632),          # Fracción de muestras para cada árbol
  Accuracy    = 0,                       # Métricas inicializadas en 0
  Sensitivity = 0,
  F1          = 0
)

# Iterar sobre el grid
for (i in 1:nrow(hyper_grid)) {
  
  # Entrenar el modelo con ranger
  model <- ranger(
    dependent.variable.name = "Y",        # Nombre de la variable dependiente
    data                    = trainData,  # Datos de entrenamiento
    num.trees               = 500,        # Número de árboles
    mtry                    = hyper_grid$mtry[i],
    min.node.size           = hyper_grid$node_size[i],
    sample.fraction         = hyper_grid$sample_size[i],
    probability             = TRUE,       # Probabilidades para clasificación
    seed                    = 123         # Semilla para reproducibilidad
  )
  
}
  # Predicciones en el conjunto de entrenamiento (fuera de bolsa)
  predictions <- as.data.frame(model$predictions)
  
  # Convertir probabilidades a clases (asumiendo 2 = positivo, 1 = negativo)
  predicted_class <- ifelse(predictions[, 2] > 0.5, 2, 1)
  
  # Asegurarse de que los factores tengan la clase positiva como 2
  predicted_class <- factor(predicted_class, levels = c(1, 2))
  true_class <- factor(trainData$Y, levels = c(1, 2))
  
  # Calcular la matriz de confusión
  confusion <- caret::confusionMatrix(
    data = predicted_class,
    reference = true_class,
    positive = "2"  # Establecer explícitamente la clase positiva como 2
  )
  
#   # Guardar métricas en el grid
#   hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
#   hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
#   hyper_grid$F1[i] <- 2 * (confusion$byClass["Sensitivity"] * confusion$byClass["Pos Pred Value"]) /
#                       (confusion$byClass["Sensitivity"] + confusion$byClass["Pos Pred Value"])
# }
# 
# # Mostrar los mejores resultados ordenados por la métrica deseada (por ejemplo, F1)
# library(dplyr)
# hyper_grid %>% 
#   arrange(desc(F1)) %>% 
#   head(10)
  


# Ver la matriz de confusión
print(confusion)

# Guardar métricas en el grid
hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
hyper_grid$F1[i] <- confusion$byClass["F1"]

#



```







#19.2 GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # Método para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


GBM_model <- train(
  Y ~ .,                        # Reemplaza Y con tu variable dependiente
  data = trainData,            # Tu conjunto de entrenamiento
  method = "gbm",              # Método GBM
  trControl = control1,         # Control de entrenamiento
  tuneGrid = expand.grid(
    interaction.depth = 3,     # Profundidad máxima de los árboles
    n.trees = 100,             # Número de árboles
    shrinkage = 0.1,           # Tasa de aprendizaje
    n.minobsinnode = 10        # Mínimo de observaciones en cada nodo
  ),
  metric = "Accuracy",         # Métrica de rendimiento
  verbose = TRUE               # Mostrar progreso
)
GBM_model

```

```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

#19.3 SVM

```{r}
SVM_model <- train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
SVM_model$bestTun    #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

SVM_model
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```




```{r}
plot(SVM_model)
```



#19.3 KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
KNN_modelo <- train(formula, 
                    data = trainData, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_modelo
```

```{r}
plot(KNN_modelo)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)

GLM_model <- train(formula, 
                  data = trainData,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_model 
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_model$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_model$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_model$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# 20. Prediccion

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```

```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

#20.1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(prediccionesRF, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)

```

```{r}
# Precisión (Accuracy)
accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(paste("La precisión (Accuracy) es:", round(accuracy_RF,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RF,2)))

# Especificidad (TNR)
specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
print(paste("la especificidad es:", round(specificity_RF,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(paste("el VPP es:", round(ppv_RF,2)))

# Valor Predictivo Negativo (VPN)
npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(paste("el VPN es: ",  round(npv_RF,2)))

```

#20.2 Prediccion GBM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesGBM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGBM <- table(prediccionesGBM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGBM)

```


```{r}
# Precisión (Accuracy)
accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
print(paste("La precisión (Accuracy) es:", round(accuracy_GBM,2)))

# Sensibilidad (Recall o TPR)
sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
print(paste("La sensibilidad es:", round(sensitivity_GBM,2)))

# Especificidad (TNR)
specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
print(paste("la especificidad es:", round(specificity_GBM,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
print(paste("el VPP es:", round(ppv_GBM,2)))

# Valor Predictivo Negativo (VPN)
npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
print(paste("el VPN es: ",  round(npv_GBM,2)))

```

#20.3 Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesSVM<-predict(SVM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesSVM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixSVM <- table(prediccionesSVM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
print(accuracy_SVM)
# Precisión positiva (VP) o sensibilidad
sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
print(sensitivity_SVM)
# Precisión negativa (VN) o especificidad
specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
specificity_SVM

# Valor predictivo positivo (VPP)
ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
print(ppv_SVM)
# Valor predictivo negativo (VPN)
npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
print(npv_SVM)
```

#20.4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN<-predict(KNN_modelo, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixKNN <- table(prediccionesKNN, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisión
accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
print(sensitivity_KNN)
# Precisión negativa (VN) o especificidad
specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
specificity_KNN

# Valor predictivo positivo (VPP)
ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN)
# Valor predictivo negativo (VPN)
npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN)
```

#20: COMPARACION MODELOS

```{r}
resultados_modelos <- data.frame(
  Modelo = c("RF_model", "GBM_model", "SVM_model", "KNN_model"),
  Accuracy = c(accuracy_RF, accuracy_GBM, accuracy_SVM, accuracy_KNN),
  Sensibilidad=c(sensitivity_RF, sensitivity_GBM, sensitivity_SVM, sensitivity_KNN),
  Especificidad = c(specificity_RF, specificity_GBM, specificity_SVM, specificity_KNN)
)

# Imprimir la tabla
print(resultados_modelos)
```





SIN DICOTOMIZAR




# 21.Modelos (con datos combat_corrected pero sin dictomizar)
no DICOTOMIZADOS



```{r}
combat_corrected2<- cbind(combat_corrected2f, label=Datos_filtrados$PCR.Cov)#1=neg,2=pos
```

```{r}
combat_corrected2<-as.data.frame(combat_corrected2)
combat_corrected2$label<-as.factor(combat_corrected2$label)
str(combat_corrected2)
```

```{r}
library(dplyr)
combat_corrected2 <- combat_corrected2 %>% rename(Y = label)
str(combat_corrected2$Y)
```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected2, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData$Y)
table(testData$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
#trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
#testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
#colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
#table(trainData$Y)
#table(testData$Y)

```

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

#19.1 Random forest

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

RF_model <- train(Y ~ ., data = trainData, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
RF_model
```

```{r}
plot(RF_model)
```


#con ranger

```{r}

library(ranger)
# Crear un grid para los hiperparámetros
hyper_grid <- expand.grid(
  mtry        = seq(5, 10, by = 1),      # Número de variables seleccionadas al azar
  node_size   = seq(7, 9, by = 2),       # Tamaño mínimo de nodos
  sample_size = c(0.55, 0.632),          # Fracción de muestras para cada árbol
  Accuracy    = 0,                       # Métricas inicializadas en 0
  Sensitivity = 0,
  F1          = 0
)

# Iterar sobre el grid
for (i in 1:nrow(hyper_grid)) {
  
  # Entrenar el modelo con ranger
  model <- ranger(
    dependent.variable.name = "Y",        # Nombre de la variable dependiente
    data                    = trainData,  # Datos de entrenamiento
    num.trees               = 500,        # Número de árboles
    mtry                    = hyper_grid$mtry[i],
    min.node.size           = hyper_grid$node_size[i],
    sample.fraction         = hyper_grid$sample_size[i],
    probability             = TRUE,       # Probabilidades para clasificación
    seed                    = 123         # Semilla para reproducibilidad
  )
  
}
  # Predicciones en el conjunto de entrenamiento (fuera de bolsa)
  predictions <- as.data.frame(model$predictions)
  
  # Convertir probabilidades a clases (asumiendo 2 = positivo, 1 = negativo)
  predicted_class <- ifelse(predictions[, 2] > 0.5, 2, 1)
  
  # Asegurarse de que los factores tengan la clase positiva como 2
  predicted_class <- factor(predicted_class, levels = c(1, 2))
  true_class <- factor(trainData$Y, levels = c(1, 2))
  
  # Calcular la matriz de confusión
  confusion <- caret::confusionMatrix(
    data = predicted_class,
    reference = true_class,
    positive = "2"  # Establecer explícitamente la clase positiva como 2
  )
  
#   # Guardar métricas en el grid
#   hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
#   hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
#   hyper_grid$F1[i] <- 2 * (confusion$byClass["Sensitivity"] * confusion$byClass["Pos Pred Value"]) /
#                       (confusion$byClass["Sensitivity"] + confusion$byClass["Pos Pred Value"])
# }
# 
# # Mostrar los mejores resultados ordenados por la métrica deseada (por ejemplo, F1)
# library(dplyr)
# hyper_grid %>% 
#   arrange(desc(F1)) %>% 
#   head(10)
  


# Ver la matriz de confusión
print(confusion)

# Guardar métricas en el grid
hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
hyper_grid$F1[i] <- confusion$byClass["F1"]

#



```







#19.2 GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # Método para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


GBM_model <- train(
  Y ~ .,                        # Reemplaza Y con tu variable dependiente
  data = trainData,            # Tu conjunto de entrenamiento
  method = "gbm",              # Método GBM
  trControl = control1,         # Control de entrenamiento
  tuneGrid = expand.grid(
    interaction.depth = 3,     # Profundidad máxima de los árboles
    n.trees = 100,             # Número de árboles
    shrinkage = 0.1,           # Tasa de aprendizaje
    n.minobsinnode = 10        # Mínimo de observaciones en cada nodo
  ),
  metric = "Accuracy",         # Métrica de rendimiento
  verbose = TRUE               # Mostrar progreso
)
GBM_model

```

```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

#19.3 SVM

```{r}
SVM_model <- train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
SVM_model$bestTun    #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

SVM_model
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```




```{r}
plot(SVM_model)
```

#19.3 KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
KNN_modelo <- train(formula, 
                    data = trainData, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_modelo
```

```{r}
plot(KNN_modelo)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)

GLM_model <- train(formula, 
                  data = trainData,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_model 
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_model$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_model$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_model$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# 20. Prediccion

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```

```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

#20.1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(prediccionesRF, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)

```

```{r}
# Precisión (Accuracy)
accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(paste("La precisión (Accuracy) es:", round(accuracy_RF,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RF,2)))

# Especificidad (TNR)
specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
print(paste("la especificidad es:", round(specificity_RF,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(paste("el VPP es:", round(ppv_RF,2)))

# Valor Predictivo Negativo (VPN)
npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(paste("el VPN es: ",  round(npv_RF,2)))

```

#20.2 Prediccion GBM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesGBM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGBM <- table(prediccionesGBM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGBM)

```


```{r}
# Precisión (Accuracy)
accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
print(paste("La precisión (Accuracy) es:", round(accuracy_GBM,2)))

# Sensibilidad (Recall o TPR)
sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
print(paste("La sensibilidad es:", round(sensitivity_GBM,2)))

# Especificidad (TNR)
specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
print(paste("la especificidad es:", round(specificity_GBM,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
print(paste("el VPP es:", round(ppv_GBM,2)))

# Valor Predictivo Negativo (VPN)
npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
print(paste("el VPN es: ",  round(npv_GBM,2)))

```

#20.3 Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesSVM<-predict(SVM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesSVM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixSVM <- table(prediccionesSVM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
print(accuracy_SVM)
# Precisión positiva (VP) o sensibilidad
sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
print(sensitivity_SVM)
# Precisión negativa (VN) o especificidad
specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
specificity_SVM

# Valor predictivo positivo (VPP)
ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
print(ppv_SVM)
# Valor predictivo negativo (VPN)
npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
print(npv_SVM)
```

#20.4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN<-predict(KNN_modelo, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixKNN <- table(prediccionesKNN, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisión
accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
print(sensitivity_KNN)
# Precisión negativa (VN) o especificidad
specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
specificity_KNN

# Valor predictivo positivo (VPP)
ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN)
# Valor predictivo negativo (VPN)
npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN)
```

#20: COMPARACION MODELOS

```{r}
resultados_modelos <- data.frame(
  Modelo = c("RF_model", "GBM_model", "SVM_model", "KNN_model"),
  Accuracy = c(accuracy_RF, accuracy_GBM, accuracy_SVM, accuracy_KNN),
  Sensibilidad=c(sensitivity_RF, sensitivity_GBM, sensitivity_SVM, sensitivity_KNN),
  Especificidad = c(specificity_RF, specificity_GBM, specificity_SVM, specificity_KNN)
)

# Imprimir la tabla
print(resultados_modelos)
```








MODELOS SIN CORRECCION DE BATCH


====================================================



sin dicotomizar


```{r}
library(dplyr)

# Eliminar la columna 'label' usando dplyr
#featureMatrix <- featureMatrix %>% select(-label)


```




```{r}
featureMatrix_HC<- cbind(featureMatrix_numf, label=Datos_filtrados$PCR.Cov)#1=neg,2=pos
```



```{r}
# Renombrar la columna 'label' a 'Y' y convertirla en factor
names(featureMatrix_HC)[names(featureMatrix_HC) == "label"] <- "Y"

featureMatrix_HC <- as.data.frame(featureMatrix_HC)
str(featureMatrix_HC)

# Verificar estructura del dataframe después de los cambios


```






```{r}
library(dplyr)
featureMatrix_HC <- featureMatrix_HC %>% rename(Y = label)
featureMatrix_HC$Y  <- as.factor(featureMatrix_HC$Y)
```




```{r}
str(featureMatrix_HC)
```

```{r}
featureMatrix_HC<- featureMatrix_HC[,-1]
```




```{r}
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_HC, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)


# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
table(trainData$Y)
table(testData$Y)

```


```{r}
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```


```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```


```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```


#1 Random forest

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

RF_model <- train(Y ~ ., data = trainData, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
RF_model
```

```{r}
plot(RF_model)
```



```{r}

library(ranger)
# Crear un grid para los hiperparámetros
hyper_grid <- expand.grid(
  mtry        = seq(5, 10, by = 1),      # Número de variables seleccionadas al azar
  node_size   = seq(7, 9, by = 2),       # Tamaño mínimo de nodos
  sample_size = c(0.55, 0.632),          # Fracción de muestras para cada árbol
  Accuracy    = 0,                       # Métricas inicializadas en 0
  Sensitivity = 0,
  F1          = 0
)

# Iterar sobre el grid
for (i in 1:nrow(hyper_grid)) {
  
  # Entrenar el modelo con ranger
  model <- ranger(
    dependent.variable.name = "Y",        # Nombre de la variable dependiente
    data                    = trainData,  # Datos de entrenamiento
    num.trees               = 500,        # Número de árboles
    mtry                    = hyper_grid$mtry[i],
    min.node.size           = hyper_grid$node_size[i],
    sample.fraction         = hyper_grid$sample_size[i],
    probability             = TRUE,       # Probabilidades para clasificación
    seed                    = 123         # Semilla para reproducibilidad
  )
  
}
  # Predicciones en el conjunto de entrenamiento (fuera de bolsa)
  predictions <- as.data.frame(model$predictions)
  
  # Convertir probabilidades a clases (asumiendo 2 = positivo, 1 = negativo)
  predicted_class <- ifelse(predictions[, 2] > 0.5, 2, 1)
  
  # Asegurarse de que los factores tengan la clase positiva como 2
  predicted_class <- factor(predicted_class, levels = c(1, 2))
  true_class <- factor(trainData$Y, levels = c(1, 2))
  
  # Calcular la matriz de confusión
  confusion <- caret::confusionMatrix(
    data = predicted_class,
    reference = true_class,
    positive = "2"  # Establecer explícitamente la clase positiva como 2
  )
  
#   # Guardar métricas en el grid
#   hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
#   hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
#   hyper_grid$F1[i] <- 2 * (confusion$byClass["Sensitivity"] * confusion$byClass["Pos Pred Value"]) /
#                       (confusion$byClass["Sensitivity"] + confusion$byClass["Pos Pred Value"])
# }
# 
# # Mostrar los mejores resultados ordenados por la métrica deseada (por ejemplo, F1)
# library(dplyr)
# hyper_grid %>% 
#   arrange(desc(F1)) %>% 
#   head(10)
  


# Ver la matriz de confusión
print(confusion)

# Guardar métricas en el grid
hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
hyper_grid$F1[i] <- confusion$byClass["F1"]

#



```

#2 GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # Método para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


GBM_model <- train(
  Y ~ .,                        # Reemplaza Y con tu variable dependiente
  data = trainData,            # Tu conjunto de entrenamiento
  method = "gbm",              # Método GBM
  trControl = control1,         # Control de entrenamiento
  tuneGrid = expand.grid(
    interaction.depth = 3,     # Profundidad máxima de los árboles
    n.trees = 100,             # Número de árboles
    shrinkage = 0.1,           # Tasa de aprendizaje
    n.minobsinnode = 10        # Mínimo de observaciones en cada nodo
  ),
  metric = "Accuracy",         # Métrica de rendimiento
  verbose = TRUE               # Mostrar progreso
)
GBM_model

```



```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

#3 SVM


```{r}
SVM_model <- train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
SVM_model$bestTun    

#there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

SVM_model
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```

```{r}
plot(SVM_model)
```


#19.3 KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
KNN_modelo <- train(formula, 
                    data = trainData, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_modelo
```
```{r}
plot(KNN_modelo)
```

#4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)

GLM_model <- train(formula, 
                  data = trainData,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_model 
```


```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_model$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_model$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_model$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

Prediccion
==========================================================

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```


```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

#1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(prediccionesRF, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)

```

```{r}
#metricas
# Precisión
accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(accuracy_RF)
# Precisión positiva (VP) o sensibilidad
sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(sensitivity_RF)
# Precisión negativa (VN) o especificidad
specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[ ,1 ])
specificity_RF

# Valor predictivo positivo (VPP)
ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(ppv_RF)
# Valor predictivo negativo (VPN)
npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(npv_RF)
```



#2 Prediccion GBM
 
```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesGBM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGBM <- table(prediccionesGBM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGBM)

```

```{r}
#metricas
# Precisión
accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
print(accuracy_GBM)
# Precisión positiva (VP) o sensibilidad
sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
print(sensitivity_GBM)
# Precisión negativa (VN) o especificidad
specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
specificity_GBM

# Valor predictivo positivo (VPP)
ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
print(ppv_GBM)
# Valor predictivo negativo (VPN)
npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
print(npv_GBM)
```



#3 Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesSVM<-predict(SVM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesSVM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixSVM <- table(prediccionesSVM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixSVM)

```
```{r}
#metricas
# Precisión
accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
print(accuracy_SVM)
# Precisión positiva (VP) o sensibilidad
sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
print(sensitivity_SVM)
# Precisión negativa (VN) o especificidad
specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
specificity_SVM

# Valor predictivo positivo (VPP)
ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
print(ppv_SVM)
# Valor predictivo negativo (VPN)
npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
print(npv_SVM)
```





#4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN<-predict(KNN_modelo, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```


```{r}
confusion_matrixKNN <- table(prediccionesKNN, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisión
accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[, 2])
print(sensitivity_KNN)
# Precisión negativa (VN) o especificidad
specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[, 1])
specificity_KNN

# Valor predictivo positivo (VPP)
ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN)
# Valor predictivo negativo (VPN)
npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN)
```
#COMPARACION MODELOS

```{r}
resultados_modelos <- data.frame(
  Modelo = c("RF_model", "GBM_model", "SVM_model", "KNN_model"),
  Accuracy = c(accuracy_RF, accuracy_GBM, accuracy_SVM, accuracy_KNN),
  Sensibilidad=c(sensitivity_RF, sensitivity_GBM, sensitivity_SVM, sensitivity_KNN),
  Especificidad = c(specificity_RF, specificity_GBM, specificity_SVM, specificity_KNN)
)

# Imprimir la tabla
print(resultados_modelos)
```


 ==============================================
 

dicotomizados



DICOTOMIZADOS

```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(featureMatrix_numf, Datos_filtrados$PCR.Cov)
featureMatrix_dicho <- dichotomize(featureMatrix_numf, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA
```

```{r}
featureMatrix_dicho<- cbind(featureMatrix_dicho, label=Datos_filtrados$PCR.Cov)#1=neg,2=pos
```

```{r}
featureMatrix_dicho<-as.data.frame(featureMatrix_dicho)
featureMatrix_dicho$label<-as.factor(featureMatrix_dicho$label)
str(featureMatrix_dicho)
```

```{r}
library(dplyr)
featureMatrix_dicho <- featureMatrix_dicho %>% rename(Y = label)
str(featureMatrix_dicho$Y)
```



```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

# Identificar variables con varianza cero
nzv <- nearZeroVar(featureMatrix_dicho)

# Eliminar variables con varianza cero
featureMatrix_clean <- featureMatrix_dicho[, -nzv]

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(featureMatrix_clean, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData <- training(split_data)
testData <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData$Y)
table(testData$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
#trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
#testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
#colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
#table(trainData$Y)
#table(testData$Y)

```

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

#19.1 Random forest

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

RF_model <- train(Y ~ ., data = trainData, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
RF_model
```

```{r}
plot(RF_model)
```


#con ranger

```{r}

library(ranger)
# Crear un grid para los hiperparámetros
hyper_grid <- expand.grid(
  mtry        = seq(5, 10, by = 1),      # Número de variables seleccionadas al azar
  node_size   = seq(7, 9, by = 2),       # Tamaño mínimo de nodos
  sample_size = c(0.55, 0.632),          # Fracción de muestras para cada árbol
  Accuracy    = 0,                       # Métricas inicializadas en 0
  Sensitivity = 0,
  F1          = 0
)

# Iterar sobre el grid
for (i in 1:nrow(hyper_grid)) {
  
  # Entrenar el modelo con ranger
  model <- ranger(
    dependent.variable.name = "Y",        # Nombre de la variable dependiente
    data                    = trainData,  # Datos de entrenamiento
    num.trees               = 500,        # Número de árboles
    mtry                    = hyper_grid$mtry[i],
    min.node.size           = hyper_grid$node_size[i],
    sample.fraction         = hyper_grid$sample_size[i],
    probability             = TRUE,       # Probabilidades para clasificación
    seed                    = 123         # Semilla para reproducibilidad
  )
  
}
  # Predicciones en el conjunto de entrenamiento (fuera de bolsa)
  predictions <- as.data.frame(model$predictions)
  
  # Convertir probabilidades a clases (asumiendo 2 = positivo, 1 = negativo)
  predicted_class <- ifelse(predictions[, 2] > 0.5, 2, 1)
  
  # Asegurarse de que los factores tengan la clase positiva como 2
  predicted_class <- factor(predicted_class, levels = c(1, 2))
  true_class <- factor(trainData$Y, levels = c(1, 2))
  
  # Calcular la matriz de confusión
  confusion <- caret::confusionMatrix(
    data = predicted_class,
    reference = true_class,
    positive = "2"  # Establecer explícitamente la clase positiva como 2
  )
  
#   # Guardar métricas en el grid
#   hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
#   hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
#   hyper_grid$F1[i] <- 2 * (confusion$byClass["Sensitivity"] * confusion$byClass["Pos Pred Value"]) /
#                       (confusion$byClass["Sensitivity"] + confusion$byClass["Pos Pred Value"])
# }
# 
# # Mostrar los mejores resultados ordenados por la métrica deseada (por ejemplo, F1)
# library(dplyr)
# hyper_grid %>% 
#   arrange(desc(F1)) %>% 
#   head(10)
  


# Ver la matriz de confusión
print(confusion)

# Guardar métricas en el grid
hyper_grid$Accuracy[i] <- confusion$overall["Accuracy"]
hyper_grid$Sensitivity[i] <- confusion$byClass["Sensitivity"]
hyper_grid$F1[i] <- confusion$byClass["F1"]

#



```







#19.2 GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # Método para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


GBM_model <- train(
  Y ~ .,                        # Reemplaza Y con tu variable dependiente
  data = trainData,            # Tu conjunto de entrenamiento
  method = "gbm",              # Método GBM
  trControl = control1,         # Control de entrenamiento
  tuneGrid = expand.grid(
    interaction.depth = 3,     # Profundidad máxima de los árboles
    n.trees = 100,             # Número de árboles
    shrinkage = 0.1,           # Tasa de aprendizaje
    n.minobsinnode = 10        # Mínimo de observaciones en cada nodo
  ),
  metric = "Accuracy",         # Métrica de rendimiento
  verbose = TRUE               # Mostrar progreso
)
GBM_model

```

```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

#19.3 SVM

```{r}
SVM_model <- train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
SVM_model$bestTun    #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

SVM_model
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperparámetros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperparámetros
#   metric = "Accuracy"     # Métrica para evaluación
# )
```




```{r}
plot(SVM_model)
```

#19.3 KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
KNN_modelo <- train(formula, 
                    data = trainData, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_modelo
```

```{r}
plot(KNN_modelo)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)

GLM_model <- train(formula, 
                  data = trainData,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_model 
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_model$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_model$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_model$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# 20. Prediccion

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```

```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```

#20.1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(prediccionesRF, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)

```

```{r}
# Precisión (Accuracy)
accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(paste("La precisión (Accuracy) es:", round(accuracy_RF,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RF,2)))

# Especificidad (TNR)
specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
print(paste("la especificidad es:", round(specificity_RF,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(paste("el VPP es:", round(ppv_RF,2)))

# Valor Predictivo Negativo (VPN)
npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(paste("el VPN es: ",  round(npv_RF,2)))

```

#20.2 Prediccion GBM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesGBM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGBM <- table(prediccionesGBM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGBM)

```


```{r}
# Precisión (Accuracy)
accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
print(paste("La precisión (Accuracy) es:", round(accuracy_GBM,2)))

# Sensibilidad (Recall o TPR)
sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
print(paste("La sensibilidad es:", round(sensitivity_GBM,2)))

# Especificidad (TNR)
specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
print(paste("la especificidad es:", round(specificity_GBM,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
print(paste("el VPP es:", round(ppv_GBM,2)))

# Valor Predictivo Negativo (VPN)
npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
print(paste("el VPN es: ",  round(npv_GBM,2)))

```

#20.3 Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesSVM<-predict(SVM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesSVM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixSVM <- table(prediccionesSVM, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
print(accuracy_SVM)
# Precisión positiva (VP) o sensibilidad
sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
print(sensitivity_SVM)
# Precisión negativa (VN) o especificidad
specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
specificity_SVM

# Valor predictivo positivo (VPP)
ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
print(ppv_SVM)
# Valor predictivo negativo (VPN)
npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
print(npv_SVM)
```

#20.4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN<-predict(KNN_modelo, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixKNN <- table(prediccionesKNN, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisión
accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
print(sensitivity_KNN)
# Precisión negativa (VN) o especificidad
specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
specificity_KNN

# Valor predictivo positivo (VPP)
ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN)
# Valor predictivo negativo (VPN)
npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN)
```

#20: COMPARACION MODELOS

```{r}
resultados_modelos <- data.frame(
  Modelo = c("RF_model", "GBM_model", "SVM_model", "KNN_model"),
  Accuracy = c(accuracy_RF, accuracy_GBM, accuracy_SVM, accuracy_KNN),
  Sensibilidad=c(sensitivity_RF, sensitivity_GBM, sensitivity_SVM, sensitivity_KNN),
  Especificidad = c(specificity_RF, specificity_GBM, specificity_SVM, specificity_KNN)
)

# Imprimir la tabla
print(resultados_modelos)
```







