---
title: "Untitled"
author: "Karina Roitman"
date: "2025-01-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```

EQUIPO

```{r}
combat_corrected_fecha<- cbind(combat_corrected_fecha, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos
combat_corrected_fecha<-as.data.frame(combat_corrected_fecha)
combat_corrected_fecha$label <- as.factor(combat_corrected_fecha$label)
```


```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_fecha, strata = "label", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_fecha_dico <- training(split_data)
testData_fecha_dico <- testing(split_data)

```


```{r}
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
trainData_fecha_dico$label<-as.factor(trainData_fecha_dico$label)
str(trainData_fecha_dico)
```

```{r}
train_subset_fecha <- trainData_fecha_dico[, 1:63]
```


```{r}
library(dplyr)
trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = label)
#trainData_fecha_dico<-trainData_fecha_dico[,-ncol(trainData_fecha_dico)]
str(trainData_fecha_dico$Y)
```


```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(train_subset_fecha, trainData_fecha_dico$Y)
train_subset_fecha <- dichotomize(train_subset_fecha, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA

```


```{r}
trainData_fecha_dico<- cbind(train_subset_fecha, trainData_fecha_dico$Y)#1=neg,2=pos
```



```{r}
str(trainData_fecha_dico)
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
```





```{r}
library(dplyr)
trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = V64)
str(trainData_fecha_dico$Y)
```


```{r}
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
trainData_fecha_dico$Y<-as.factor(trainData_fecha_dico$Y)
str(trainData_fecha_dico)
```



```{r}
zero_var_indices <- nearZeroVar(trainData_fecha_dico[,1:63])
#train_subset_fecha <- train_subset_fecha
if (length(zero_var_indices) > 0) {
    trainData_fecha_dico <- trainData_fecha_dico[, -zero_var_indices]
}


```
QUEDAN 49 VARIABLES

```{r}

thr_filtered <- thr[colnames(trainData_fecha_dico[,1:49])]

```


```{r}
#trainData_fecha_dico <- cbind(train_subset_fecha, trainData_fecha_dico$Y)
```




```{r}
#library(dplyr)
#trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
#trainData_fecha_dico <- trainData_fecha_dico %>% rename(Y = V42)
#str(trainData_fecha_dico$Y)
```



```{r}
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
#testData_fecha_dico$label<-as.factor(testData_fecha_dico$label)

str(testData_fecha_dico)
```



```{r}
library(dplyr)
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
testData_fecha_dico <- testData_fecha_dico %>% rename(Y = label)
str(testData_fecha_dico$Y)
```



```{r}
testData_fecha_dico<-as.data.frame(testData_fecha_dico)

test_subset_fecha <- testData_fecha_dico[, colnames(trainData_fecha_dico[,1:49])]


#test_subset <-  test_subset[, colnames(trainData_equipo_dico)]

#Dicotimizacion de la matriz de intensidad
test_subset_fecha <- dichotomize(test_subset_fecha, thr_filtered) 


```


```{r}
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
testData_fecha_dico2<- cbind(test_subset_fecha, testData_fecha_dico$Y)#1=neg,2=pos
```

```{r}
library(dplyr)
testData_fecha_dico2<-as.data.frame(testData_fecha_dico2)
testData_fecha_dico2 <- testData_fecha_dico2 %>% rename(Y = V50)
str(testData_fecha_dico2$Y)
```

```{r}
testData_fecha_dico<-testData_fecha_dico2
```


```{r}
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
testData_fecha_dico$Y<-as.factor(testData_fecha_dico$Y)
str(testData_fecha_dico)
```




```{r}
trainData_fecha_dico<-as.data.frame(trainData_fecha_dico)
trainData_fecha_dico$Y<-as.factor(trainData_fecha_dico$Y)
testData_fecha_dico<-as.data.frame(testData_fecha_dico)
# Ver la distribución de clases en ambos conjuntos
table(trainData_fecha_dico$Y)
table(testData_fecha_dico$Y)


```






```{r}
library(caret)
set.seed(42)
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

#19.1 Random forest

```{r}
class(trainData_fecha_dico)
```


```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.
set.seed(42)
rf_fecha_dic <- caret::train(Y ~ ., data = trainData_fecha_dico, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
rf_fecha_dic$results
```

```{r}
plot(rf_fecha_dic)
```


#con ranger


COMBAT_CORRECTED_FECHA_DICHO

```{r}
Train.rf_fecha_dico  <- as.data.frame(trainData_fecha_dico) 

Test.rf_fecha_dico <- as.data.frame(testData_fecha_dico)
```

```{r}
objeto_recipe <- recipe(formula = Y ~ .,
                        data =  Train.rf_fecha_dico)

#objeto_recipe <- objeto_recipe %>% 
#  step_nzv(all_predictors())

trained_recipe <- prep(objeto_recipe, training = Train.rf_fecha_dico)

Train.rf_fecha_dico <- bake(trained_recipe, new_data = Train.rf_fecha_dico)
Test.rf_fecha_dico  <- bake(trained_recipe, new_data = Test.rf_fecha_dico)
```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

x <- Train.rf_fecha_dico[, -ncol(Train.rf_fecha_dico)] # se incluyen todas las columnas excepto la última

# if(ncol(x) <= 7){
#   
#   mtry <- c(1, 2, 3)
#   
# } else {
#   
#   mtry <- c(1, 2, sfecha(4, ncol(x) * 0.8, 2))
#   
# }
mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = sfecha(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_fecha_dico$Y <- factor(as.numeric(factor(Train.rf_fecha_dico$Y)))
Train.rf_fecha_dico$Y <- factor(Train.rf_fecha_dico$Y, levels = c("1", "2"))
levels(Train.rf_fecha_dico$Y) <- make.names(levels(Train.rf_fecha_dico$Y))

```





```{r}
class(Train.rf_fecha_dico)
```


```{r}
Train.rf_fecha_dico <- as.data.frame(Train.rf_fecha_dico)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_fecha_dico <- as.data.frame(Train.rf_fecha_dico)

# Convertimos Y a factor
Train.rf_fecha_dico$Y <- as.factor(Train.rf_fecha_dico$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_ranger_fecha_dic <- caret::train(Y ~ .,
                data = Train.rf_fecha_dico, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val,
                num.trees = n_trees)
               #allowParallel=FALSE)  # Aquí estaba el error, había texto adicional

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```




```{r}
Test.rf_fecha_dico$Y<-as.factor(Test.rf_fecha_dico$Y)
str(Test.rf_fecha_dico$Y)
```

```{r}
testRF_NOID_fecha_dico <- Test.rf_fecha_dico[, -which(names(Test.rf_fecha_dico) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_fecha_dic<-predict(results_ranger_fecha_dic, newdata = testRF_NOID_fecha_dico)#matriz binarizada
print(predRANGER_fecha_dic)


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y

prob_predRanger_fecha_dic <- predict(results_ranger_fecha_dic, newdata = testRF_NOID_fecha_dico, type = "prob")

colnames(prob_predRanger_fecha_dic)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_fecha_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))




```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_fecha_dic[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_FECHA_dich <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_FECHA_dich, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

confusion_matrixRANGER <- table(predRANGER_fecha_dic, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRANGER)
```


```{r}
# Precisión (Accuracy)
accuracy_RANGER_fecha_dic <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_fecha_dic,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RANGER_fecha_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RANGER_fecha_dic,2)))

# Especificidad (TNR)
specificity_RANGER_fecha_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
print(paste("la especificidad es:", round(specificity_RANGER_fecha_dic,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RANGER_fecha_dic <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
print(paste("el VPP es:", round(ppv_RANGER_fecha_dic,2)))

# Valor Predictivo Negativo (VPN)
npv_RANGER_fecha_dic <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
print(paste("el VPN es: ",  round(npv_RANGER_fecha_dic,2)))


```


```{r}

library(irr)

# Calcular Kappa
kappa_RANGERfecha_dic <- kappa2(cbind(predRANGER_fecha_dic, Test.rf_fecha_dico$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_RANGERfecha_dic$value, 3)))
kappa_RANGERfecha_dic<-round(kappa_RANGERfecha_dic$value, 3)
```





#19.3 KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)
set.seed(42)
# Ajustar el modelo KNN
KNN_fecha_dic <- caret::train(formula, 
                    data = trainData_fecha_dico, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_fecha_dic
```

```{r}
plot(KNN_fecha_dic)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
set.seed(42)
GLM_fecha_dic<- caret::train(formula, 
                  data = trainData_fecha_dico,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_fecha_dic 
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_fecha_dic$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_fecha_dic$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_fecha_dic$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# 20. Prediccion

```{r}
testData_fecha_dico$Y<-as.factor(testData_fecha_dico$Y)
str(testData_fecha_dico$Y)
```

```{r}
testData_NOID_fecha_dico <- testData_fecha_dico[, -which(names(testData_fecha_dico) == "Y")]

```

#20.1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRF_fecha_dic<-predict(rf_fecha_dic, newdata = testData_NOID_fecha_dico)#matriz binarizada
print(predRF_fecha_dic)


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y

prob_predRF_fechadic <- predict(rf_fecha_dic, newdata = testData_NOID_fecha_dico, type = "prob")

colnames(prob_predRF_fechadic)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(testData_fecha_dico$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))




```


```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF_fechadic[, "2"], levels = c(1, 2))
AUC_RF_fecha_dic <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_fecha_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```


```{r}
confusion_matrixRF <- table(predRF_fecha_dic, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)

```

```{r}
# Precisión (Accuracy)
accuracyRF_fecha_dic <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(paste("La precisión (Accuracy) es:", round(accuracyRF_fecha_dic,2)))

# Sensibilidad (Recall o TPR)
sensitivityRF_fecha_dic <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(paste("La sensibilidad es:", round(sensitivityRF_fecha_dic,2)))

# Especificidad (TNR)
specificityRF_fecha_dic <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
print(paste("la especificidad es:", round(specificityRF_fecha_dic,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppvRF_fecha_dic <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(paste("el VPP es:", round(ppvRF_fecha_dic,2)))

# Valor Predictivo Negativo (VPN)
npvRF_fecha_dic <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(paste("el VPN es: ",  round(npvRF_fecha_dic,2)))


```
```{r}

library(irr)

# Calcular Kappa
kappa_RFfecha_dic <- kappa2(cbind(predRF_fecha_dic, testData_fecha_dico$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_RFfecha_dic$value, 3)))
kappa_RFfecha_dic<-round(kappa_RFfecha_dic$value, 3)
```




#20.2 Prediccion GLM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_fecha_dic<-predict(GLM_fecha_dic, newdata = testData_NOID_fecha_dico)#matriz binarizada
print(predGLM_fecha_dic)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGLM <- table(predGLM_fecha_dic, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGLM)

```


```{r}
# Precisión (Accuracy)
accuracy_GLM_fecha_dic <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM)
print(paste("La precisión (Accuracy) es:", round(accuracy_GLM_fecha_dic,2)))

# Sensibilidad (Recall o TPR)
sensitivity_GLM_fecha_dic <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2])
print(paste("La sensibilidad es:", round(sensitivity_GLM_fecha_dic,2)))

# Especificidad (TNR)
specificity_GLM_fecha_dic <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1])
print(paste("la especificidad es:", round(specificity_GLM_fecha_dic,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_GLM_fecha_dic <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ])
print(paste("el VPP es:", round(ppv_GLM_fecha_dic,2)))

# Valor Predictivo Negativo (VPN)
npv_GLM_fecha_dic <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ])
print(paste("el VPN es: ",  round(npv_GLM_fecha_dic,2)))




# Predicciones probabilísticas con el modelo GLM
prob_predGLM <- predict(GLM_fecha_dic, newdata = testData_NOID_fecha_dico, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha_dico$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM[, "2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_fecha_dic <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_fecha_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```
```{r}

library(irr)

# Calcular Kappa
kappa_GLM_fecha_dic <- kappa2(cbind(predGLM_fecha_dic, testData_fecha_dico$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_GLM_fecha_dic$value, 3)))
kappa_GLM_fecha_dic<-round(kappa_GLM_fecha_dic$value, 3)
```



#20.3 Prediccion SVM

```{r}
# #prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
# predSVMC<-predict(SVM_c, newdata = testData_NOID)#matriz binarizada
# print(predSVMC)
# 
# 
# # Obtener las etiquetas reales del conjunto de datos de prueba
# y_test <- testData$Y
# #y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada


```

```{r}

# 
# # Predicción de probabilidades
# prob_predSVMC <- predict(SVM_c, newdata = testData_NOID, type = "prob")
# 
# # Verificar el resultado
# print(prob_predSVMC)

```


```{r}
# confusion_matrixSVM <- table(predSVMC, y_test)
# # Imprimir la matriz de confusión
# print(confusion_matrixSVM)

```

```{r}
#metricas
# Precisión
# accuracy_SVMC <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
# print(accuracy_SVMC)
# # Precisión positiva (VP) o sensibilidad
# sensitivity_SVMC <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[,2 ])
# print(sensitivity_SVMC)
# # Precisión negativa (VN) o especificidad
# specificity_SVMC <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[,1 ])
# specificity_SVMC
# 
# # Valor predictivo positivo (VPP)
# ppv_SVMC <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
# print(ppv_SVMC)
# # Valor predictivo negativo (VPN)
# npv_SVMC <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
# print(npv_SVMC)
# 
# 
# library(pROC)
# 
# # Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
# y_test_numeric <- as.numeric(y_test)
# 
# roc_curve <- roc(response = y_test_numeric, predictor = prob_predSVMC[, "2"], levels = c(1, 2))
# AUC_SVMC <- auc(roc_curve)
# 
# print(paste("El valor de AUC es:", round(AUC_SVMC, 3)))
# 
# # Visualiza la curva ROC
# plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)
```

#20.4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predKNN_fecha_dic<-predict(KNN_fecha_dic, newdata = testData_NOID_fecha_dico)#matriz binarizada
print(predKNN_fecha_dic)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha_dico$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixKNN <- table(predKNN_fecha_dic, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisión
accuracy_KNN_fecha_dic <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN_fecha_dic)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN_fecha_dic <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
print(sensitivity_KNN_fecha_dic)
# Precisión negativa (VN) o especificidad
specificity_KNN_fecha_dic <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
specificity_KNN_fecha_dic

# Valor predictivo positivo (VPP)
ppv_KNN_fecha_dic <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN_fecha_dic)
# Valor predictivo negativo (VPN)
npv_KNN_fecha_dic <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN_fecha_dic)



prob_predKNN_fecha <- predict(KNN_fecha_dic, newdata = testData_NOID_fecha_dico, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNN_fecha))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha_dico$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNN_fecha[, "2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_fecha_dic <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_fecha_dic, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```


```{r}

library(irr)

# Calcular Kappa
kappa_KNN_fecha_dic <- kappa2(cbind(predKNN_fecha_dic, testData_fecha_dico$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_KNN_fecha_dic$value, 3)))
kappa_KNN_fecha_dic<-round(kappa_KNN_fecha_dic$value, 3)
```


#20: COMPARACION MODELOS

```{r}
# resultados_modelos <- data.frame(
#   Modelo = c("RFC_model", "GLMC_model", "SVMC_model", "KNNC_model"),
#   Accuracy = c(accuracy_RFC, accuracy_GLMC , accuracy_SVMC, accuracy_KNNC),
#   Sensibilidad=c(sensitivity_RFC, sensitivity_GLMC, sensitivity_SVMC, sensitivity_KNNC),
#   Especificidad = c(specificity_RFC, specificity_GLMC, specificity_SVMC, specificity_KNNC),
#   AUC= c(AUC_RFC, AUC_GLMC, AUC_SVMC, AUC_KNNC)
# )
# 
# # Imprimir la tabla
# print(resultados_modelos)
```





SIN DICOTOMIZAR




# 21.Modelos (con datos combat_corrected pero sin dictomizar)



```{r}
combat_corrected_fecha<- cbind(combat_corrected_fecha, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos
```

```{r}
combat_corrected_fecha<-as.data.frame(combat_corrected_fecha)
combat_corrected_fecha$label<-as.factor(combat_corrected_fecha$label)
str(combat_corrected_fecha)
```



```{r}
library(dplyr)
combat_corrected_fecha<-combat_corrected_fecha[,-ncol(combat_corrected_fecha)]
combat_corrected_fecha <- combat_corrected_fecha %>% rename(Y = label)
str(combat_corrected_fecha$Y)
```



```{r}
zero_var_indices <- nearZeroVar(combat_corrected_fecha)
combat_corrected_fecha <- combat_corrected_fecha
if (length(zero_var_indices) > 0) {
    combat_corrected_fecha <- combat_corrected_fecha[, -zero_var_indices]
}

```


```{r}

library(tidymodels)
# set.seed asegura reproducibilidad
set.seed(42)

#Utilizo tidymodels para dividir de manera proporcional

split_data <- initial_split(combat_corrected_fecha, strata = "Y", prop = 0.8)

# Obtener los conjuntos de entrenamiento y prueba
trainData_fecha <- training(split_data)
testData_fecha <- testing(split_data)

# Ver la distribución de clases en ambos conjuntos
table(trainData_fecha$Y)
table(testData_fecha$Y)

#trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)


# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
#trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
#testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
#colnames(trainData)

# Verificar la proporción de clases (ajusta el nombre si no es 'covid')
#table(trainData$Y)
#table(testData$Y)

```

```{r}
set.seed(42)
library(caret)
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```

```{r}
# Especificar la variable dependiente en la fórmula
formula <- Y ~ .
```

```{r}
#accuracy=Es la proporción de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una métrica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```

#19.1 Random forest

```{r}
library(caret)
set.seed(42)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.
set.seed(42)
RF_fecha <- caret::train(Y ~ ., data = trainData_fecha, 
                      method = "rf",   # Método para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Parámetro mtry
                      metric = "Accuracy")  # Métrica para clasificación

```

```{r}
RF_fecha
```

```{r}
plot(RF_fecha)
```


#con ranger


COMBAT CORRECTED FECHA




```{r}
Train.rf_FECHA  <- as.data.frame(trainData_fecha) 

Test.rf_FECHA <- as.data.frame(testData_fecha)
```


```{r}
library(keras)
library(tensorflow)
library(reticulate)
library(caret)
```


```{r}

objeto_recipe <- recipe(formula = Y ~ .,
                        data =  Train.rf_FECHA)

objeto_recipe <- objeto_recipe %>% 
  step_nzv(all_predictors())

trained_recipe <- prep(objeto_recipe, training = Train.rf_FECHA)

Train.rf_FECHA <- bake(trained_recipe, new_data = Train.rf_FECHA)
Test.rf_FECHA  <- bake(trained_recipe, new_data = Test.rf_FECHA)
```

```{r}
# Submuestras y repeticiones

# particiones  <- 5
# repeticiones <- 15
particiones <- 3
repeticiones <- 5
```

```{r}
# Specify the tunning configuration (mtry hyperparameter depends on the number of columns)
seed.rf <- 42
set.seed(seed.rf) 

x <- Train.rf_FECHA[, -ncol(Train.rf_FECHA)] # se incluyen todas las columnas excepto la última

# if(ncol(x) <= 7){
#   
#   mtry <- c(1, 2, 3)
#   
# } else {
#   
#   mtry <- c(1, 2, sfecha(4, ncol(x) * 0.8, 2))
#   
# }
mtry <- c(1, 2, 3)
min.node.size <- seq(1, 10, 2)  # Reducir el rango
hiperparametros <- expand.grid(mtry =  mtry,
                               #min.node.size = sfecha(1, 30, 2),
                               min.node.size=min.node.size,
                               splitrule = "gini")
```


```{r}

# Seeds
seed.rf <- 42
set.seed(seed.rf)

seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)

for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(500, nrow(hiperparametros)) 
}

seeds[[(particiones * repeticiones) + 1]] <- sample.int(500, 1)

# Training control
```



```{r}
# Training control

cross_val <- trainControl(
  method = "repeatedcv",
  number = particiones,
  repeats = repeticiones,
  returnResamp = "final",
  verboseIter = FALSE,
  allowParallel = TRUE,
  classProbs = TRUE,
  seeds = seeds)

# Training 

# Convertir los niveles de Train.rf$sensi a números también

Train.rf_FECHA$Y <- factor(as.numeric(factor(Train.rf_FECHA$Y)))
Train.rf_FECHA$Y <- factor(Train.rf_FECHA$Y, levels = c("1", "2"))
levels(Train.rf_FECHA$Y) <- make.names(levels(Train.rf_FECHA$Y))

```





```{r}
class(Train.rf_FECHA)
```


```{r}
Train.rf_FECHA <- as.data.frame(Train.rf_FECHA)
```


```{r}
# Primero aseguramos que Train.rf es un dataframe
Train.rf_FECHA <- as.data.frame(Train.rf_FECHA)

# Convertimos Y a factor
Train.rf_FECHA$Y <- as.factor(Train.rf_FECHA$Y)

# Definimos número de árboles
#n_trees <- 500 # default
n_trees<-200
# Establecemos semilla para reproducibilidad
set.seed(80)

# Ejecutamos el entrenamiento
results_FECHA <- caret::train(Y ~ .,
                data = Train.rf_FECHA, 
                method = "ranger",
                tuneGrid = hiperparametros,
                metric = "Accuracy",
                importance = "impurity",
                trControl = cross_val,
                num.trees = n_trees)
               #allowParallel=FALSE)  

# Vector para probar diferentes números de árboles
#num_trees_range <- c(10, 50, 100, 200, 500, 1000, 1500)

```




```{r}
Test.rf_FECHA$Y<-as.factor(Test.rf_FECHA$Y)
str(Test.rf_FECHA$Y)
```

```{r}
testRF_NOID_FECHA <- Test.rf_FECHA[, -which(names(Test.rf_FECHA) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRANGER_FECHA<-predict(results_FECHA, newdata = testRF_NOID_FECHA)#matriz binarizada
print(predRANGER_FECHA)


# Obtener las etiquetas reales del conjunto de datos de prueba
#y_test <- testData$Y

prob_predRanger_FECHA <- predict(results_FECHA, newdata = testRF_NOID_FECHA, type = "prob")

colnames(prob_predRanger_FECHA)
# Asegúrate de que y_test sea un factor con los niveles correctos
y_test <- factor(Test.rf_FECHA$Y, levels = c(1, 2), labels = c("Cov.Neg", "Cov.Pos"))




```



```{r}
library(pROC)

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, 
                 predictor = prob_predRanger_FECHA[, "X2"], 
                 levels = c(1, 2))
AUC_Ranger_FECHA <- auc(roc_curve)
print(paste("El valor de AUC es:", round(AUC_Ranger_FECHA, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

confusion_matrixRANGER <- table(predRANGER_FECHA, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRANGER)
```


```{r}
# Precisión (Accuracy)
accuracy_RANGER_fecha <- sum(diag(confusion_matrixRANGER)) / sum(confusion_matrixRANGER)
print(paste("La precisión (Accuracy) es:", round(accuracy_RANGER_fecha,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RANGER_fecha <- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RANGER_fecha,2)))

# Especificidad (TNR)
specificity_RANGER_fecha <- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[, 1])
print(paste("la especificidad es:", round(specificity_RANGER_fecha,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RANGER_fecha<- confusion_matrixRANGER[2, 2] / sum(confusion_matrixRANGER[2, ])
print(paste("el VPP es:", round(ppv_RANGER_fecha,2)))

# Valor Predictivo Negativo (VPN)
npv_RANGER_fecha<- confusion_matrixRANGER[1, 1] / sum(confusion_matrixRANGER[1, ])
print(paste("el VPN es: ",  round(npv_RANGER_fecha,2)))


```



```{r}

library(irr)

# Calcular Kappa
kappa_RANGERfecha <- kappa2(cbind(predRANGER_FECHA, Test.rf_FECHA$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_RANGERfecha$value, 3)))
kappa_RANGERfecha<-round(kappa_RANGERfecha$value, 3)
```













#19.3 KNN

```{r}
tuneGrid <- expand.grid(k = 1:15)
set.seed(42)
# Ajustar el modelo KNN
KNN_fecha <- caret::train(formula, 
                    data = trainData_fecha, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_fecha
```

```{r}
plot(KNN_fecha)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)
set.seed(42)
GLM_fecha <- caret::train(formula, 
                  data = trainData_fecha,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_fecha 
```

```{r}
# Imprimir los mejores hiperparámetros
best_params <- GLM_fecha$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperparámetros en el data frame model$results
best_model_index <- apply(GLM_fecha$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_fecha$results[best_model_index, ]



# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

# 20. Prediccion

```{r}
testData_fecha$Y<-as.factor(testData_fecha$Y)
str(testData_fecha$Y)
```

```{r}
testData_NOID_fecha <- testData_fecha[, -which(names(testData_fecha) == "Y")]

```

#20.1 RF

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predRF_fecha<-predict(RF_fecha, newdata = testData_NOID_fecha)#matriz binarizada
print(predRF_fecha)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(predRF_fecha, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixRF)



```

```{r}
# Precisión (Accuracy)
accuracy_RF_fecha <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(paste("La precisión (Accuracy) es:", round(accuracy_RF_fecha,2)))

# Sensibilidad (Recall o TPR)
sensitivity_RF_fecha <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(paste("La sensibilidad es:", round(sensitivity_RF_fecha,2)))

# Especificidad (TNR)
specificityRF_fecha <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[, 1])
print(paste("la especificidad es:", round(specificityRF_fecha,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_RF_fecha <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(paste("el VPP es:", round(ppv_RF_fecha,2)))

# Valor Predictivo Negativo (VPN)
npv_RF_fecha <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
print(paste("el VPN es: ",  round(npv_RF_fecha,2)))




library(pROC)

prob_predRF_fecha <- predict(RF_fecha, newdata = testData_NOID_fecha, type = "prob")

# Asegúrate de que y_test sea numérico y esté alineado con los niveles 1 y 2
y_test_numeric <- as.numeric(y_test)

roc_curve <- roc(response = y_test_numeric, predictor = prob_predRF_fecha[, "2"], levels = c(1, 2))
AUC_RF_fecha <- auc(roc_curve)

print(paste("El valor de AUC es:", round(AUC_RF_fecha, 3)))

# Visualiza la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

```


```{r}

library(irr)

# Calcular Kappa
kappa_RFfecha <- kappa2(cbind(predRF_fecha, testData_fecha$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_RFfecha$value, 3)))
kappa_RFfecha<-round(kappa_RFfecha$value, 3)
```




#20.4 Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predKNN_fecha<-predict(KNN_fecha, newdata = testData_NOID_fecha)#matriz binarizada
print(predKNN_fecha)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixKNN <- table(predKNN_fecha, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisión
accuracy_KNN_fecha <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN_fecha)
# Precisión positiva (VP) o sensibilidad
sensitivity_KNN_fecha<- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[,2 ])
print(sensitivity_KNN_fecha)
# Precisión negativa (VN) o especificidad
specificity_KNN_fecha <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[,1 ])
specificity_KNN_fecha

# Valor predictivo positivo (VPP)
ppv_KNN_fecha <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(ppv_KNN_fecha)
# Valor predictivo negativo (VPN)
npv_KNN_fecha <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
print(npv_KNN_fecha)


prob_predKNNfecha <- predict(KNN_fecha, newdata = testData_NOID_fecha, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predKNNfecha))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_knn <- roc(response = y_test_numeric, predictor = prob_predKNNfecha[, "2"], levels = c(1, 2))

# Calcula el AUC
AUC_KNN_fecha <- auc(roc_curve_knn)
print(paste("El valor de AUC es:", round(AUC_KNN_fecha, 3)))

# Visualiza la curva ROC
plot(roc_curve_knn, main = "Curva ROC - KNN", col = "blue", lwd = 2)
```



```{r}

library(irr)

# Calcular Kappa
kappa_KNNfecha <- kappa2(cbind(predKNN_fecha, testData_fecha$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_KNNfecha$value, 3)))
kappa_KNNfecha<-round(kappa_KNNfecha$value, 3)
```




```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
predGLM_fecha<-predict(GLM_fecha, newdata = testData_NOID_fecha)#matriz binarizada
print(predGLM_fecha)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData_fecha$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGLM <- table(predGLM_fecha, y_test)
# Imprimir la matriz de confusión
print(confusion_matrixGLM)

```


```{r}
# Precisión (Accuracy)
accuracy_GLM_fecha <- sum(diag(confusion_matrixGLM)) / sum(confusion_matrixGLM)
print(paste("La precisión (Accuracy) es:", round(accuracy_GLM_fecha,2)))

# Sensibilidad (Recall o TPR)
sensitivity_GLM_fecha <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[, 2])
print(paste("La sensibilidad es:", round(sensitivity_GLM_fecha,2)))

# Especificidad (TNR)
specificity_GLM_fecha <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[, 1])
print(paste("la especificidad es:", round(specificity_GLM_fecha,2)))

# Valor Predictivo Positivo (PPV o Precision)
ppv_GLM_fecha <- confusion_matrixGLM[2, 2] / sum(confusion_matrixGLM[2, ])
print(paste("el VPP es:", round(ppv_GLM_fecha,2)))

# Valor Predictivo Negativo (VPN)
npv_GLM_fecha <- confusion_matrixGLM[1, 1] / sum(confusion_matrixGLM[1, ])
print(paste("el VPN es: ",  round(npv_GLM_fecha,2)))




# Predicciones probabilísticas con el modelo GLM
prob_predGLM_fecha <- predict(GLM_fecha, newdata = testData_NOID_fecha, type = "prob")

# Verifica los nombres de las columnas (clases)
print(colnames(prob_predGLM_fecha))  # Deberían ser "1" y "2" o los nombres de las clases

# Asegúrate de que y_test sea numérico con niveles 1 y 2
y_test_numeric <- as.numeric(testData_fecha$Y)

# Calcula la curva ROC usando la probabilidad de la clase positiva ("2")
roc_curve_glm <- roc(response = y_test_numeric, predictor = prob_predGLM_fecha[, "2"], levels = c(1, 2))

# Calcula el AUC
AUC_GLM_fecha <- auc(roc_curve_glm)
print(paste("El valor de AUC es:", round(AUC_GLM_fecha, 3)))

# Visualiza la curva ROC
plot(roc_curve_glm, main = "Curva ROC - GLM", col = "red", lwd = 2)
```

```{r}

library(irr)

# Calcular Kappa
kappa_GLM_fecha <- kappa2(cbind(predGLM_fecha, testData_fecha$Y))

# Ver el valor de Kappa
print(paste("El índice Kappa es:", round(kappa_GLM_fecha$value, 3)))
kappa_GLM_fecha<-round(kappa_GLM_fecha$value, 3)
```

#20: COMPARACION MODELOS

```{r}
resultados_modelos_fecha <- data.frame(
  Modelo = c("RF_fechadic", "GLMC_fechadic", "KNN_fechadic", "Ranger_fechadic",  "RF_fecha", "GLM_fecha", "KNN_fecha", "Ranger_fecha"),
  Accuracy = c(accuracyRF_fecha_dic, accuracy_GLM_fecha_dic , accuracy_KNN_fecha_dic, accuracy_RANGER_fecha_dic, accuracy_RF_fecha, accuracy_GLM_fecha , accuracy_KNN_fecha, accuracy_RANGER_fecha),
  Sensibilidad=c(sensitivityRF_fecha_dic, sensitivity_GLM_fecha_dic, sensitivity_KNN_fecha_dic, sensitivity_RANGER_fecha_dic, sensitivity_RF_fecha, sensitivity_GLM_fecha, sensitivity_KNN_fecha, sensitivity_RANGER_fecha),
  Especificidad = c(specificityRF_fecha_dic, specificity_GLM_fecha_dic, specificity_KNN_fecha_dic, specificity_RANGER_fecha_dic, specificityRF_fecha, specificity_GLM_fecha, specificity_KNN_fecha, specificity_RANGER_fecha),
  AUC= c(AUC_RF_fecha_dic, AUC_GLM_fecha_dic, AUC_KNN_fecha, AUC_Ranger_FECHA_dich, AUC_RF_fecha, AUC_GLM_fecha, AUC_KNN_fecha, AUC_Ranger_FECHA),
kappa= c(kappa_RFfecha_dic, kappa_GLM_fecha_dic, kappa_KNN_fecha_dic, kappa_RANGERfecha_dic, kappa_RFfecha, kappa_GLM_fecha, kappa_KNNfecha, kappa_RANGERfecha))


print(resultados_modelos_fecha)

```




```{r}


# Cargar la librería
library(writexl)

# Exportar a Excel
write_xlsx(resultados_modelos_fecha, "C:/Users/karin/Desktop/MCD/TESIS/resultados_modelos_fecha_CARGA.xlsx")
```



