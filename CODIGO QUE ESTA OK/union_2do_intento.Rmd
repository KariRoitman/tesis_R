---
title: "intento2UnionEspectros"
author: "Karina Roitman"
date: "2024-09-04"
output: html_document
---


---
title: "Train_test_juntos"
author: "Karina Roitman"
date: "2024-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


Objetivos
=======================================================================

1. Cargar librerias. 
=======================================================================

rmarkdown como estrategia sint√°ctica. (En bibliografia hay 2 archivos)

```{r}
library(rmarkdown)
```

```{r}

library(stringr)
?stringr
library(tidyr)
?tidyr
library(dplyr)
library(openxlsx)
library(purrr)
library(broom)
library(ggplot2)
library(MALDIquant)
?MALDIquant
library(MALDIquantForeign)
?MALDIquantForeign
```


```{r}
getwd()  # Verifica el directorio de trabajo actual

```

```{r}
setwd("C:/Users/karin/Desktop/MCD/TESIS")
```

2. Cargar datos
=======================================================================

```{r}
load("./Data_Kari/EspectrosINB/Average_ciego.INBIRS.1.rda")
load("./Data_Kari/EspectrosINB/Average_ciego.INBIRS.2.rda")
load("./Data_Kari/EspectrosINB/Average_ciego.INBIRS.3.rda")
load("./Data_Kari/EspectrosINB/Average_ciego.INBIRS.4.rda")
load("./Data_Kari/EspectrosHC/Average_HC_1.rda")
load("./Data_Kari/EspectrosHC/Average_HC_2.rda")
load("./Data_Kari/EspectrosHC/Average_HC_3.rda")
load("./Data_Kari/EspectrosHC/Average_HC_4.rda")
load("./Data_Kari/EspectrosHC/Average_HC_5.rda")
load("./Data_Kari/EspectrosHC/Average_HC_6.rda")
load("./Data_Kari/Espectros.Malb/Average_ciego.Malbran.1.rda")
load("./Data_Kari/Espectros.Malb/Average_ciego.Malbran.2.rda")
load("./Data_Kari/Espectros.Malb/Average_ciego.Malbran.3.rda")
load("./Data_Kari/EspectrosCR/Average_ciego.Costa.Rica.1.rda")
load("./Data_Kari/EspectrosCR/Average_ciego.Costa.Rica.3.rda")
```




3. Asigno batch, institucion y equipo
=======================================================================


```{r}
# Listado de tus dataframes
dataframes <- list(INBIRS.1.df.f.1)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "16_7i"
  df$institucion <- "inbirs"
  df$equipo <- "HC"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

INBIRS.1.df.f.1 <- dataframes[[1]]
```


```{r}
# Listado de tus dataframes
dataframes <- list(INBIRS.df.2.f)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "17_7i"
  df$institucion <- "inbirs"
  df$equipo <- "HC"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

INBIRS.df.2.f <- dataframes[[1]]
```


```{r}
# Listado de tus dataframes
dataframes <- list(INBIRS.df.3.f)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "13_8i"
  df$institucion <- "inbirs"
  df$equipo <- "HC"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

INBIRS.df.3.f <- dataframes[[1]]
```

```{r}
# Listado de tus dataframes
dataframes <- list(Espectros.INBIRS.4.f)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "18_8i"
  df$institucion <- "inbirs"
  df$equipo <- "HC"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales
Espectros.INBIRS.4.f<-dataframes[[1]]

```
HC1

```{r}
# Listado de tus dataframes
dataframes <- list(Categ.Hospi.1.Neg, Categ.Hospi.1.Pos, Categ.Hospi.2.Cnt, Categ.Hospi.2.Neg, Categ.Hospi.2.No.Covid, Categ.Hospi.2.Pos, Categ.Hospi.3.Neg, Categ.Hospi.3.No.Covid, Categ.Hospi.3.Pos)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "dia1hc"
  df$institucion <- "HdC"
  df$equipo <- "HC"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

Categ.Hospi.1.Neg<- dataframes[[1]]
Categ.Hospi.1.Pos<- dataframes[[2]]
Categ.Hospi.2.Cnt<- dataframes[[3]]
Categ.Hospi.2.Neg<- dataframes[[4]]
Categ.Hospi.2.No.Covid<- dataframes[[5]]
Categ.Hospi.2.Pos<- dataframes[[6]]
Categ.Hospi.3.Neg<- dataframes[[7]]
Categ.Hospi.3.No.Covid<- dataframes[[8]]
Categ.Hospi.3.Pos<- dataframes[[9]]

```

HC2

```{r}
dataframes <- list(Categ.Ciego.clin.1.Exp1.covid, Categ.Ciego.clin.1.Exp1.flu)

dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "dia2hc"
  df$institucion <- "HdC"
  df$equipo <- "HC"
  return(df)
})

Categ.Ciego.clin.1.Exp1.covid<-dataframes[[1]]
Categ.Ciego.clin.1.Exp1.flu<-dataframes[[2]]
```

HC3

```{r}
dataframes <- list(Categ.Neg.New.clin.1, Categ.Pos.New.clin.1)

dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "dia3hc"
  df$institucion <- "HdC"
  df$equipo <- "HC"
  return(df)
})

Categ.Neg.New.clin.1<-dataframes[[1]]
Categ.Pos.New.clin.1<-dataframes[[2]]
```

HC4
```{r}
dataframes <- list(Categ.Ciego.clin25.6.2.covid, Categ.Ciego25.6.clin.2.flu)

dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "dia4hc"
  df$institucion <- "HdC"
  df$equipo <- "HC"
  return(df)
})

Categ.Ciego.clin25.6.2.covid<-dataframes[[1]]
Categ.Ciego25.6.clin.2.flu<-dataframes[[2]]
```


HC5

```{r}
dataframes <- list(Categ.Ciego_3_7.4)

dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "dia5hc"
  df$institucion <- "HdC"
  df$equipo <- "HC"
  return(df)
})

Categ.Ciego_3_7.4<-dataframes[[1]]
```

HC6
```{r}
dataframes <- list(Categ.Ciego_8_7.4)

dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "dia2hc"
  df$institucion <- "HdC"
  df$equipo <- "HC"
  return(df)
})

Categ.Ciego_8_7.4<-dataframes[[1]]

```

MALBRAN 1

```{r}
# Listado de tus dataframes
dataframes <- list(Categ.Malbran.1.1.1)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "1Malbran"
  df$institucion <- "Malbran"
  df$equipo <- "Malbran"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

Categ.Malbran.1.1.1 <- dataframes[[1]]
```
MALBRAN2

```{r}
# Listado de tus dataframes
dataframes <- list(Categ.Malbran.2.1.1)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "2Malbran"
  df$institucion <- "Malbran"
  df$equipo <- "Malbran"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

Categ.Malbran.2.1.1 <- dataframes[[1]]
```


MALBRAN3

```{r}
# Listado de tus dataframes
dataframes <- list(Malbran_3_4.1)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "3Malbran"
  df$institucion <- "Malbran"
  df$equipo <- "Malbran"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

Malbran_3_4.1 <- dataframes[[1]]
```

CR1

```{r}
# Listado de tus dataframes
dataframes <- list(CostaRica.1.1)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "1CR"
  df$institucion <- "CR"
  df$equipo <- "CR"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

CostaRica.1.1 <- dataframes[[1]]
```

CR2

```{r}
# Listado de tus dataframes
dataframes <- list(CostaRica.3.1.1.1)

# Usas lapply para agregar las columnas a cada dataframe
dataframes <- lapply(dataframes, function(df) {
  df$fecha <- "2CR"
  df$institucion <- "CR"
  df$equipo <- "CR"
  return(df)
})

# Asignas los dataframes modificados de vuelta a sus nombres originales

CostaRica.3.1.1.1<- dataframes[[1]]


CostaRica.3.1.1.1$PCR.Cov <- CostaRica.3.1.1.1$Virus
```

4. Listado de todos los dataframes
=======================================================================

```{r}
dataframes <- list(INBIRS.1.df.f.1,
INBIRS.df.2.f,
INBIRS.df.3.f,
Espectros.INBIRS.4.f,
Categ.Hospi.1.Neg,
Categ.Hospi.1.Pos,
Categ.Hospi.2.Cnt,
Categ.Hospi.2.Neg,
Categ.Hospi.2.No.Covid,
Categ.Hospi.2.Pos,
Categ.Hospi.3.Neg,
Categ.Hospi.3.No.Covid,
Categ.Hospi.3.Pos,
Categ.Ciego.clin.1.Exp1.covid,
Categ.Ciego.clin.1.Exp1.flu,
Categ.Neg.New.clin.1,
Categ.Pos.New.clin.1,
Categ.Ciego.clin25.6.2.covid,
Categ.Ciego25.6.clin.2.flu,
Categ.Ciego_3_7.4,
Categ.Ciego_8_7.4,
Categ.Malbran.1.1.1,
Categ.Malbran.2.1.1,
Malbran_3_4.1,
CostaRica.1.1,
CostaRica.3.1.1.1)




# Contar filas para cada dataframe
filas_count <- sapply(dataframes, nrow)

# Mostrar los resultados
print(filas_count)
sum(filas_count)
```

5. Union de todos los espectros
=======================================================================


```{r , message=FALSE, echo=FALSE, warning=FALSE, out.width="100%"}

Espec.Union.Clin.Pos.Neg <- c(
                            Espectros.INBIRS.1,
                            Espectros.INBIRS.2,
                            Espectros.INBIRS.3,
                            Espectros.INBIRS.4,
                            Espectros.HC.1.Neg,
                            Espectros.HC.1.HighPos,
                            Espectros.HC.2.Cnt,
                            Espectros.HC.2.Neg,
                            Espectros.HC.2.No.covid,
                            Espectros.HC.2.IntPos,
                            Espectros.HC.3.Neg,
                            Espectros.HC.3.No.covid,
                            Espectros.HC.3.LowPos,
                            Espectros.Ciego.Exp1.covid,
                            Espectros.Ciego.Exp1.flu,
                            Espectros.Neg.New.Clin,
                            Espectros.Pos.New.Clin, 
                            Espectros.Ciego.25.6.covid,
                            Espectros.Ciego.25.6.flu,
                            Espectros.Ciego.4.2,
                            Espectros.Ciego.5,
                            Espectros.Malbran.24,
                            Espectros.Malb.2.24,
                            Espectros.Malb.3_4,
                            Espectros.CR1,
                            Espectros.CR3
                            )

Espectra.1 <- alignSpectra(Espec.Union.Clin.Pos.Neg, halfWindowSize=50, SNR=3, 
                    tolerance=0.5, warpingMethod="quadratic")
```


```{r}
library(purrr)
```


```{r}
INBIRS.1.df.f.1<-INBIRS.1.df.f.1[,c(1,6:10)]
INBIRS.1.df.f.1<-data.frame(purrr::map(INBIRS.1.df.f.1, as.character),
                               stringsAsFactors = FALSE)

INBIRS.df.2.f<-INBIRS.df.2.f[,c(1,6:10)]
INBIRS.df.2.f<-data.frame(purrr::map(INBIRS.df.2.f, as.character ),
                               stringsAsFactors = FALSE)

INBIRS.df.3.f<-INBIRS.df.3.f[,c(1,6:10)]
INBIRS.df.3.f<-data.frame(purrr::map(INBIRS.df.3.f, as.character ),
                               stringsAsFactors = FALSE)


Espectros.INBIRS.4.f<-Espectros.INBIRS.4.f[,c(1,6:10)]
Espectros.INBIRS.4.f<-data.frame(purrr::map(Espectros.INBIRS.4.f, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.1.Neg<- Categ.Hospi.1.Neg[,c(1,5,7,10:12)]
#Categ.Hospi.1.Neg<- data.frame(map(Categ.Hospi.1.Neg, as.character),
 #                              stringsAsFactors = FALSE)
Categ.Hospi.1.Neg<-data.frame(purrr::map(Categ.Hospi.1.Neg, as.character ),
                              stringsAsFactors = FALSE)

Categ.Hospi.1.Pos<- Categ.Hospi.1.Pos[,c(1,5,7,10:12)]
Categ.Hospi.1.Pos<-data.frame(purrr::map(Categ.Hospi.1.Pos, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.2.Cnt<- Categ.Hospi.2.Cnt[,c(1,5,8,11:13)]
Categ.Hospi.2.Cnt<-data.frame(purrr::map(Categ.Hospi.2.Cnt, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.2.Neg<- Categ.Hospi.2.Neg[,c(1,5,8,11:13)]
Categ.Hospi.2.Neg<-data.frame(purrr::map(Categ.Hospi.2.Neg, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.2.No.Covid<- Categ.Hospi.2.No.Covid[,c(1,5,8,11:13)]
Categ.Hospi.2.No.Covid<-data.frame(purrr::map(Categ.Hospi.2.No.Covid, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.2.Pos<- Categ.Hospi.2.Pos[,c(1,5,8,11:13)]
Categ.Hospi.2.Pos<-data.frame(purrr::map(Categ.Hospi.2.Pos, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.3.Neg<- Categ.Hospi.3.Neg[,c(1,5,8,11:13)]
Categ.Hospi.3.Neg<-data.frame(purrr::map(Categ.Hospi.3.Neg, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.3.No.Covid<- Categ.Hospi.3.No.Covid[,c(1,5,8,11:13)]
Categ.Hospi.3.No.Covid<-data.frame(purrr::map(Categ.Hospi.3.No.Covid, as.character ),
                               stringsAsFactors = FALSE)

Categ.Hospi.3.Pos<- Categ.Hospi.3.Pos[,c(1,5,8,11:13)]
Categ.Hospi.3.Pos<-data.frame(purrr::map(Categ.Hospi.3.Pos, as.character ),
                               stringsAsFactors = FALSE)

Categ.Ciego.clin.1.Exp1.covid<-Categ.Ciego.clin.1.Exp1.covid[,c(1,6,7,9:11)]
Categ.Ciego.clin.1.Exp1.covid<-data.frame(purrr::map(Categ.Ciego.clin.1.Exp1.covid, as.character ),
                               stringsAsFactors = FALSE)

Categ.Ciego.clin.1.Exp1.flu<-Categ.Ciego.clin.1.Exp1.flu[,c(1,6,7,9:11)]
Categ.Ciego.clin.1.Exp1.flu<-data.frame(purrr::map(Categ.Ciego.clin.1.Exp1.flu, as.character ),
                               stringsAsFactors = FALSE)

Categ.Neg.New.clin.1<-Categ.Neg.New.clin.1[,c(1,6:10)]
Categ.Neg.New.clin.1<-data.frame(purrr::map(Categ.Neg.New.clin.1, as.character ),
                               stringsAsFactors = FALSE)

Categ.Pos.New.clin.1<-Categ.Pos.New.clin.1[,c(1,6:10)]
Categ.Pos.New.clin.1<-data.frame(purrr::map(Categ.Pos.New.clin.1, as.character ),
                               stringsAsFactors = FALSE)

Categ.Ciego.clin25.6.2.covid<-Categ.Ciego.clin25.6.2.covid[,c(1,6,7, 9:11)]
Categ.Ciego.clin25.6.2.covid<-data.frame(purrr::map(Categ.Ciego.clin25.6.2.covid, as.character ),
                               stringsAsFactors = FALSE)

Categ.Ciego25.6.clin.2.flu<-Categ.Ciego25.6.clin.2.flu[,c(1,6,7, 9:11)]
Categ.Ciego25.6.clin.2.flu<-data.frame(purrr::map(Categ.Ciego25.6.clin.2.flu, as.character ),
                               stringsAsFactors = FALSE)

Categ.Ciego_3_7.4<-Categ.Ciego_3_7.4[,c(1,6:10)]
Categ.Ciego_3_7.4<-data.frame(purrr::map(Categ.Ciego_3_7.4, as.character ),
                               stringsAsFactors = FALSE)

Categ.Ciego_8_7.4<-Categ.Ciego_8_7.4[,c(1,6:10)]
Categ.Ciego_8_7.4<-data.frame(purrr::map(Categ.Ciego_8_7.4, as.character ),
                               stringsAsFactors = FALSE)


Categ.Malbran.1.1.1<-Categ.Malbran.1.1.1[,c(1,5:9)]
Categ.Malbran.1.1.1<-data.frame(purrr::map(Categ.Malbran.1.1.1, as.character ),
                               stringsAsFactors = FALSE)

Categ.Malbran.2.1.1<-Categ.Malbran.2.1.1[,c(1,5:9)]
Categ.Malbran.2.1.1<-data.frame(purrr::map(Categ.Malbran.2.1.1, as.character ),
                               stringsAsFactors = FALSE)

Malbran_3_4.1<-Malbran_3_4.1[,c(1,6:10)]
Malbran_3_4.1<-data.frame(purrr::map(Malbran_3_4.1, as.character ),
                               stringsAsFactors = FALSE)

CostaRica.1.1<-CostaRica.1.1[,c(1,6:10)]
CostaRica.1.1<-data.frame(purrr::map(CostaRica.1.1, as.character ),
                               stringsAsFactors = FALSE)

CostaRica.1.1$Carga<-as.character(CostaRica.1.1$Carga)

CostaRica.3.1.1.1<-CostaRica.3.1.1.1[,c(1,6:10)]
CostaRica.3.1.1.1<-data.frame(purrr::map(CostaRica.3.1.1.1, as.character),
                               stringsAsFactors = FALSE)
CostaRica.3.1.1.1$Carga<-as.character(CostaRica.3.1.1.1$Carga)

```

```{r}
dataframes <- list(INBIRS.1.df.f.1,
INBIRS.df.2.f,
INBIRS.df.3.f,
Espectros.INBIRS.4.f,
Categ.Hospi.1.Neg,
Categ.Hospi.1.Pos,
Categ.Hospi.2.Cnt,
Categ.Hospi.2.Neg,
Categ.Hospi.2.No.Covid,
Categ.Hospi.2.Pos,
Categ.Hospi.3.Neg,
Categ.Hospi.3.No.Covid,
Categ.Hospi.3.Pos,
Categ.Ciego.clin.1.Exp1.covid,
Categ.Ciego.clin.1.Exp1.flu,
Categ.Neg.New.clin.1,
Categ.Pos.New.clin.1,
Categ.Ciego.clin25.6.2.covid,
Categ.Ciego25.6.clin.2.flu,
Categ.Ciego_3_7.4,
Categ.Ciego_8_7.4,
Categ.Malbran.1.1.1,
Categ.Malbran.2.1.1,
Malbran_3_4.1,
CostaRica.1.1,
CostaRica.3.1.1.1)




# Contar filas para cada dataframe
filas_count <- sapply(dataframes, nrow)

# Mostrar los resultados
print(filas_count)
sum(filas_count)
```


6. Union de todos los dataframes
=======================================================================

```{r}
Base.Union.Clinc.Covid <- INBIRS.1.df.f.1 %>%
  bind_rows(INBIRS.df.2.f) %>%
  bind_rows(INBIRS.df.3.f) %>%
  bind_rows(Espectros.INBIRS.4.f) %>%
bind_rows(Categ.Hospi.1.Neg)%>%
bind_rows(Categ.Hospi.1.Pos)%>%
bind_rows(Categ.Hospi.2.Cnt)%>%
bind_rows(Categ.Hospi.2.Neg)%>%
bind_rows(Categ.Hospi.2.No.Covid)%>%
bind_rows(Categ.Hospi.2.Pos)%>%
bind_rows(Categ.Hospi.3.Neg)%>%
bind_rows(Categ.Hospi.3.No.Covid)%>%
bind_rows(Categ.Hospi.3.Pos)%>%
bind_rows(Categ.Ciego.clin.1.Exp1.covid)%>%
bind_rows(Categ.Ciego.clin.1.Exp1.flu)%>%
bind_rows(Categ.Neg.New.clin.1)%>%
bind_rows(Categ.Pos.New.clin.1)%>%
bind_rows(Categ.Ciego.clin25.6.2.covid)%>%
bind_rows(Categ.Ciego25.6.clin.2.flu)%>%
bind_rows(Categ.Ciego_3_7.4)%>%
bind_rows(Categ.Ciego_8_7.4)%>%
bind_rows(Categ.Malbran.1.1.1)%>%
bind_rows(Categ.Malbran.2.1.1)%>%
bind_rows(Malbran_3_4.1)%>%
bind_rows(CostaRica.1.1)%>%
bind_rows(CostaRica.3.1.1.1)
```

```{r}
Espectra.Orig <- data.frame(names(Espectra.1),
                                  stringsAsFactors = FALSE)
names(Espectra.Orig)<- c("spot.a.1")

Datos_actualizados<-  Espectra.Orig %>%
  left_join( Base.Union.Clinc.Covid, by="spot.a.1")
```

```{r}
# dev.new()
# 
# # Inicializar el gr√°fico con el primer espectro
# plot(Espectra.1[[1]], main = "Espectros Superpuestos", col = "blue", type = "l")
# 
# # Iterar sobre los espectros restantes y agregarlos al gr√°fico
# for (i in 2:length(Espectra.1)) {
#   lines(Espectra.1[[i]], col = i)  # A√±adir l√≠neas para cada espectro, con diferentes colores
# }
# 
# # A√±adir una leyenda para indicar los colores de cada espectro
# legend("topright", legend = 1:length(Espectra.1), col = 1:length(Espectra.1),
#        title = "Espectros", cex = 0.8)
```


7. Deteccion y filtrado de picos
=======================================================================

```{r , message=FALSE, echo=FALSE, warning=FALSE}

peaks <- detectPeaks(Espectra.1, SNR = 3, 
                     method="MAD", halfWindowSize=50)
peaks <- binPeaks(peaks,tolerance=0.5)

species.Ave<-factor(Datos_actualizados$PCR.Cov) 
spot.factor.Avera<-factor(Datos_actualizados$spot.a.1) 

peaks <- filterPeaks(peaks, minFrequency=c(0.33, 0.33),
                     labels = species.Ave,
                     mergeWhitelists=TRUE)

featureMatrix <- intensityMatrix(peaks, Espectra.1)
```


```{r}
featureMatrix<- cbind(featureMatrix, label=Datos_actualizados$spot.a.1, covid=Datos_actualizados$PCR.Cov, carga=Datos_actualizados$Carga, dia=Datos_actualizados$batch, equipo=Datos_actualizados$equipo, fecha=Datos_actualizados$fecha, institucion=Datos_actualizados$institucion)
```


8. EDA
=======================================================================


```{r}
data_freq <- as.data.frame(table(Datos_actualizados$institucion, Datos_actualizados$PCR.Cov))
colnames(data_freq) <- c("institucion", "covid_status", "frecuencia")

# Gr√°fico de barras superpuestas
ggplot(data_freq, aes(x = institucion, y = frecuencia, fill = covid_status)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Frecuencia por Instituci√≥n",
       x = "Instituci√≥n",
       y = "Frecuencia") +
  scale_fill_manual(values = c("Cov.Neg" = "blue", "Cov.Pos" = "red")) +
  theme_minimal()
```




```{r}

print(data_freq)
```



```{r}
Datos_actualizados<- Datos_actualizados[ , -10]

Datos_actualizados$id <- 1:nrow(Datos_actualizados)
Datos_actualizados <- Datos_actualizados[ , c("id", names(Datos_actualizados)[-ncol(Datos_actualizados)])]
```


8. Matriz de intensidades con label
=======================================================================

```{r}

# Crear una nueva columna con secuencia del 1 hasta el n√∫mero de filas
featureMatrix<- cbind(id=Datos_actualizados$id, featureMatrix) 
```

9. Matriz numerica
=======================================================================

```{r}
# Exclude non-numerical columns 
featureMatrix_num <- featureMatrix[, -c(1, (ncol(featureMatrix)-5):ncol(featureMatrix))]
```

```{r}
featureMatrix_num <- apply(featureMatrix_num, 2, function(x) as.numeric(as.character(x)))

```

```{r}
sapply(featureMatrix_num, class)
```


```{r}
class(featureMatrix_num)
```


10. PCA mixOmics
====================================================================

```{r}

#  install.packages("BiocManager") 
## install mixOmics 
#BiocManager::install('mixOmics')
```
```{r}
library(mixOmics)

#Performs a principal components analysis on the given data matrix that can contain missing values. If data are complete 'pca' uses Singular Value Decomposition, if there are some missing values, it uses the NIPALS algorithm.
```

```{r}
featureMatrix_num <- apply(featureMatrix_num, 2, as.numeric)

str(featureMatrix_num)
```


```{r}
?pca
```


```{r}

pca.before <- pca(featureMatrix_num, ncomp = 2)   #ncomp=	Integer, if data is complete ncomp decides the number of components and associated eigenvalues to display from the pcasvd algorithm and if the data has missing values, ncomp gives the number of components to keep to perform the reconstitution of the data using the NIPALS algorithm. If NULL, function sets ncomp = min(nrow(X), ncol(X))
```

```{r}
#pca.before$institucion <- Datos_actualizados$institucion
pca.before$fecha <- Datos_actualizados$fecha
pca.before$equipo <- Datos_actualizados$equipo
pca.before$covid<- Datos_actualizados$PCR.Cov
```


```{r}
pca_coor<-as.data.frame(pca.before$variates$X) # Coordenadas principales obtenidas del PCA
pca.before$X

```


```{r}
dim(pca_coor)
```

```{r}
colnames(pca.before$variates$X) <- paste0("PC", 1:ncol((pca.before$variates$X)))
```


```{r}
library(ggExtra)
#expl_var <- pca.before$sdev^2 / sum(pca.before$sdev^2)  # Varianza explicada por cada componente

# Crear un gr√°fico de dispersi√≥n con ggplot2
p <- ggplot(pca_coor, aes(x = pca_coor$PC1, y = pca_coor$PC2, color = pca.before$equipo, shape = pca.before$covid)) +
  geom_point(size = 3) +  # Tama√±o de los puntos
  labs(title = 'PCA con mixOmics',
       x = 'PC1: Varianza Explicada',
       y = 'PC2: Varianza Explicada',
       color = 'Batch', shape = 'Tratamiento') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# A√±adir gr√°ficos de densidad marginal con ggExtra
p_with_density <- ggMarginal(p, type = "density", margins = "both", groupColour = TRUE, groupFill = TRUE)

# Mostrar el gr√°fico final
print(p_with_density)
```





```{r}
ggplot(pca_coor, aes(x = pca_coor$PC1, y = pca_coor$PC2, color = pca.before$fecha, shape = pca.before$covid))  +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "PCA - Componentes Principales",
       x = "Componente Principal 1",
       y = "Componente Principal 2") +
  scale_color_manual(values = c( "red", "blue", "green", "purple", "orange", "pink", "yellow", "cyan", "brown", "magenta", "gray", "black", "darkgreen", "darkblue")) + # Personaliza los colores si es necesario
  scale_shape_manual(values = c(16, 17)) + # Personaliza las formas si es necesario
  theme_minimal() # O puedes usar otro tema si prefieres
```


11. PCA con prcomp
=======================================================================


```{r}
# Perform PCA on numerical columns only
pca_res <- prcomp(featureMatrix_num, scale. = TRUE)

```


```{r}
porcentaje_varianza_explicada <- (pca_res$sdev^2) / sum(pca_res$sdev^2) * 100


plot(porcentaje_varianza_explicada, type = "b", 
     xlab = "Componente Principal", 
     ylab = "Porcentaje de Varianza Explicada",
     main = "Porcentaje de Varianza Explicada por Componente",
     xaxt = "n")  # Suprime los valores predeterminados del eje X

# A√±adir los n√∫meros manualmente a lo largo del eje X para todos los componentes
axis(1, at = 1:length(porcentaje_varianza_explicada), labels = 1:length(porcentaje_varianza_explicada))

```


```{r}
summary(pca_res)
```


```{r}

# Now you can add back the categorical data for plotting purposes
pca_data <- as.data.frame(pca_res$x)
#pca_data$institucion <- Datos_actualizados$institucion
pca_data$fecha <- Datos_actualizados$fecha
pca_data$equipo <- Datos_actualizados$equipo
pca_data$covid<- Datos_actualizados$PCR.Cov

```

```{r}
ggplot(pca_data, aes(x = PC1, y = PC2, color = equipo, shape = covid)) +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "PCA - Componentes Principales",
       x = "Componente Principal 1",
       y = "Componente Principal 2") +
  scale_color_manual(values = c( "blue", "green", "purple")) + # Personaliza los colores si es necesario
  scale_shape_manual(values = c(16, 17)) + # Personaliza las formas si es necesario
  theme_minimal() # O puedes usar otro tema si prefieres
```


```{r}
ggplot(pca_data, aes(x = pca_data$PC2, y = pca_data$PC3, color = equipo, shape = covid)) +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "PCA - Componentes Principales",
       x = "Componente Principal 2",
       y = "Componente Principal 3") +
  scale_color_manual(values = c( "blue", "green", "purple")) + # Personaliza los colores si es necesario
  scale_shape_manual(values = c(16, 17)) + # Personaliza las formas si es necesario
  theme_minimal()
```


```{r}
# ggplot(pca_data, aes(x = PC1, y = PC2, color = institucion, shape = covid)) +
#   geom_point(size = 3) + # Tama√±o de los puntos
#   labs(title = "PCA - Componentes Principales",
#        x = "Componente Principal 1",
#        y = "Componente Principal 2") +
#   scale_color_manual(values = c( "red","blue", "green", "purple")) + # Personaliza los colores si es necesario
#   scale_shape_manual(values = c(16, 17)) + # Personaliza las formas si es necesario
#   theme_minimal() # O puedes usar otro tema si prefieres
```



```{r}
ggplot(pca_data, aes(x = PC1, y = PC2, color = fecha, shape = covid)) +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "PCA - Componentes Principales",
       x = "Componente Principal 1",
       y = "Componente Principal 2") +
  scale_color_manual(values = c( "red", "blue", "green", "purple", "orange", "pink", "yellow", "cyan", "brown", "magenta", "gray", "black", "darkgreen", "darkblue")) +
  scale_shape_manual(values = c(16, 17)) +
  theme_minimal()

```

```{r}
valores_unicos_fecha <- unique(featureMatrix[,"fecha"])

# Mostrar los valores √∫nicos
print(valores_unicos_fecha)
```
```{r}
ggplot(pca_data, aes(x = PC1, y = PC2, color=covid, shape = covid)) +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "PCA - Componentes Principales",
       x = "Componente Principal 1",
       y = "Componente Principal 2") +
 scale_color_manual(values = c( "red","blue"))+
  scale_shape_manual(values = c(16, 17)) +
  theme_minimal()
```

```{r}
ggplot(pca_data, aes(x = pca_data$PC1, y = pca_data$PC3, color=covid, shape = covid)) +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "PCA - Componentes Principales",
       x = "Componente Principal 1",
       y = "Componente Principal 3") +
 scale_color_manual(values = c( "red","blue"))+
  scale_shape_manual(values = c(16, 17)) +
  theme_minimal()
```


12. UMAP
====================================================================



```{r}
library(umap)
set.seed(42)
umap<-umap(featureMatrix_num)
```



```{r}

library(uwot)
```

```{r}
set.seed(123)
umap_result <- umap(featureMatrix_num)
```




```{r}
umap_df <- as.data.frame(umap_result)
colnames(umap_df) <- c("V1", "V2")
```



```{r}
#umap_df$institucion <- Datos_actualizados$institucion
umap_df$fecha <- Datos_actualizados$fecha
umap_df$equipo <- Datos_actualizados$equipo
umap_df$covid<- Datos_actualizados$PCR.Cov
```


```{r}

ggplot(umap_df, aes(x = V1, y = V2, color = equipo, shape=covid)) +
  geom_point() +
  labs(title = "UMAP para Detecci√≥n de Efectos de Batch", x = "UMAP1", y = "UMAP2") +
    scale_shape_manual(values = c(16, 17)) +
  theme_minimal()

```

```{r}

ggplot(umap_df, aes(x = V1, y = V2, color = covid, shape=covid)) +
  geom_point() +
  labs(title = "UMAP para Detecci√≥n de Efectos de Batch", x = "UMAP1", y = "UMAP2") +
    scale_shape_manual(values = c(16, 17)) +
  theme_minimal()

```
```{r}
ggplot(umap_df,aes(x = V1, y = V2, color = fecha, shape=covid)) +
  geom_point(size = 3) + # Tama√±o de los puntos
  labs(title = "UMAP POR FECHA",
       x = "UNMAP 1",
       y = "UMAP 2") +
  scale_color_manual(values = c( "red", "blue", "green", "purple", "orange", "pink", "yellow", "cyan", "brown", "magenta", "gray", "black", "darkgreen", "darkblue")) +
  scale_shape_manual(values = c(16, 17)) +
  theme_minimal() # O puedes usar otro tema si prefieres
```


```{r}
#UMAP intenta encontrar una representaci√≥n (no lineal) de pocas dimensiones de los datos que preserve las distancias entre cada puntos y sus vecinos en el espacio multi-dimensional

#Comparison among PCA, t-SNE and UMAP : https://aurigait.com/blog/blog-easy-explanation-of-dimensionality-reduction-and-techniques/



library(umap)


# Ajustar par√°metros directamente en la funci√≥n umap
umap_v2 <- umap(featureMatrix_num, n_neighbors = 5, metric = "cosine", min_dist = 0.1)

# Verificar la estructura del objeto umap_v2
str(umap_v2) # Esto te mostrar√° la estructura para asegurarte de que layout es un componente v√°lido.

# Si la estructura es correcta, continuar con la conversi√≥n a dataframe
umap_coor2 <- as.data.frame(umap_v2)

# Asignar nombres de columnas
colnames(umap_coor2) <- c("UMAP1", "UMAP2")

# A√±adir las columnas con los metadatos (COVID, equipo, etc.)
umap_coor2$covid <- Datos_actualizados$PCR.Cov
umap_coor2$equipo <- Datos_actualizados$equipo

# Graficar UMAP con ggplot2
library(ggplot2)
ggplot(umap_coor2, aes(x = UMAP1, y = UMAP2, color = equipo, shape = covid)) +
  geom_point(size = 3) +
  labs(title = "UMAP para Detecci√≥n de Efectos de Batch",
       x = "UMAP1",
       y = "UMAP2") +
  theme_minimal()


```

```{r}

# 
# ggplot(umap_df, aes(x = V1, y = V2, color = institucion, shape=covid)) +
#   geom_point() +
#   labs(title = "UMAP para Detecci√≥n de Efectos de Batch", x = "UMAP1", y = "UMAP2") +
#     scale_shape_manual(values = c(16, 17)) +
#   theme_minimal()

```




```{r}
# Ajustar par√°metros directamente en la funci√≥n umap
umap_v3 <- umap2(featureMatrix_num)

# Verificar la estructura del objeto umap_v2
str(umap_v3) # Esto te mostrar√° la estructura para asegurarte de que layout es un componente v√°lido.

# Si la estructura es correcta, continuar con la conversi√≥n a dataframe
umap_coor3 <- as.data.frame(umap_v3)

# Asignar nombres de columnas
colnames(umap_coor3) <- c("UMAP1", "UMAP2")

# A√±adir las columnas con los metadatos (COVID, equipo, etc.)
umap_coor3$covid <- Datos_actualizados$PCR.Cov
umap_coor3$equipo <- Datos_actualizados$equipo

# Graficar UMAP con ggplot2
library(ggplot2)
ggplot(umap_coor3, aes(x = UMAP1, y = UMAP2, color = equipo, shape = covid)) +
  geom_point(size = 3) +
  labs(title = "UMAP para Detecci√≥n de Efectos de Batch",
       x = "UMAP1",
       y = "UMAP2") +
  theme_minimal()


```


13. t-SNE
====================================================================

```{r}
#tsne
library(Rtsne)
library(ggplot2)
library(readxl)
library(RColorBrewer)
```
```{r}
?Rtsne
```


```{r}
set.seed(9)
tsne_model <- Rtsne(featureMatrix_num, check_duplicates=FALSE, 
                      pca=TRUE, perplexity=30, theta=0.5, dims=2)
```

```{r}
#tsne_model$institucion <- Datos_actualizados$institucion
tsne_model$fecha <- Datos_actualizados$fecha
tsne_model$equipo <- Datos_actualizados$equipo
tsne_model$covid<- Datos_actualizados$PCR.Cov
```

```{r}
tsne_data <- as.data.frame(tsne_model$Y)
colnames(tsne_data) <- c("Dim1", "Dim2")
```


```{r}
#tsne_data$institucion <- Datos_actualizados$institucion
tsne_data$fecha <- Datos_actualizados$fecha
tsne_data$equipo <- Datos_actualizados$equipo
tsne_data$covid<- Datos_actualizados$PCR.Cov
```

```{r}
ggplot(tsne_data, aes(x=Dim1, y=Dim2, color=equipo)) +
  geom_point(size=1, alpha=0.7) +
  ggtitle("t-SNE Visualization by Batch") +
  theme_minimal(base_size=15) +
  scale_color_manual(values=c("#D32F2F", "#1976D2", "#388E3C"))  # Use a color palette
```



14. Imposibilidad de analizar Cts:
====================================================================

```{r}

cantidad_sd_0_na <- sum(Datos_actualizados$Carga %in% c("SD", "0") | is.na(Datos_actualizados$Carga))

# Mostrar el resultado
cantidad_sd_0_na
```


```{r}
library(dplyr)
# Convertir las variables categ√≥ricas en factores
Datos_actualizados <- Datos_actualizados[,c(1:7)] %>%
  mutate(across(c(PCR.Cov, fecha, equipo), as.factor))

# Convertir la matriz de datos a una matriz num√©rica si a√∫n no lo es
expression_data <- as.matrix(featureMatrix_num)

```




```{r}
# Prueba de normalidad
shapiro.test(pca.before$X[, 1])

# Prueba de homocedasticidad
library(car)
leveneTest(pca.before$X[, 1] ~ Datos_actualizados$PCR.Cov)

```


```{r}
# Inicializar un dataframe para almacenar los resultados
shapiro_results <- data.frame(Componente = integer(), PValue = numeric(), stringsAsFactors = FALSE)

# Iterar sobre cada componente principal
for (i in 1:ncol(pca.before$X)) {
  p_value <- shapiro.test(pca.before$X[, i])$p.value
  shapiro_results <- rbind(shapiro_results, data.frame(Componente = i, PValue = p_value))
}

# Visualizar los resultados
print(shapiro_results)

```



15. Kruskal wallis para detectar el peso de cada factor en el efecto batch
====================================================

```{r}
# Cargar librer√≠as necesarias
library(ggplot2)
library(dplyr)

# 1. Realizar PCA
pca_result <- prcomp(featureMatrix_num, scale. = TRUE)

# 2. Obtener las puntuaciones de los componentes principales
pca_scores <- pca_result$x

# 3. Agregar los resultados PCA al dataframe de metadatos
metadata_pca <- cbind(Datos_actualizados[,c(4:7)], pca_scores)

# 4. Funci√≥n para realizar Kruskal-Wallis sobre los componentes principales
kruskal_pca <- function(pca_scores, metadata, factor) {
  results <- data.frame()
  for (i in 1:ncol(pca_scores)) {  # Para cada componente principal
    pca_component <- pca_scores[, i]  # Extraer la puntuaci√≥n del componente
    model <- kruskal.test(pca_component ~ metadata[[factor]])  # Realizar la prueba de Kruskal-Wallis
    factor_effect <- model$p.value  # Obtener el p-valor
    results <- rbind(results, data.frame(PC = i, Factor = factor, PValue = factor_effect))  # Guardar resultados
  }
  return(results)
}

# 5. Lista de factores que quieres analizar
factors <- c("PCR.Cov", "fecha", "equipo")

# Ejecutar la funci√≥n para cada factor y combinar los resultados
kruskal_results <- do.call(rbind, lapply(factors, function(f) kruskal_pca(pca_scores, Datos_actualizados, f)))

# 6. Resumir el p-valor medio para cada factor
factor_summary <- kruskal_results %>%
  group_by(Factor) %>%
  summarise(MeanPValue = mean(PValue))

# 7. Visualizar los resultados en un gr√°fico de barras
ggplot(factor_summary, aes(x = reorder(Factor, MeanPValue), y = MeanPValue, fill = Factor)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Importancia de los Factores en el Efecto Batch",
       x = "Factor",
       y = "P-valor Medio",
       fill = "Factor")
```



16. Kruskal wallis para detectar el peso de cada factor en el efecto batch en analisis de UMAP
====================================================


```{r}
# Prueba de normalidad Shapiro-Wilk en la primera dimensi√≥n de UMAP
shapiro.test(umap_v3[, 1])

# Prueba de normalidad Shapiro-Wilk en la segunda dimensi√≥n de UMAP
shapiro.test(umap_v3[, 2])


```





```{r}
# Si umap_v3 es una matriz, simplemente lo usas as√≠:
umap_scores <- umap_v3  # Tomar el resultado de UMAP directamente

# Luego sigues con el mismo an√°lisis
metadata_umap <- cbind(Datos_actualizados[,c(4,5,7)], umap_scores)

# Funci√≥n para realizar Kruskal-Wallis sobre las dimensiones de UMAP
kruskal_umap <- function(umap_scores, metadata, factor) {
  results <- data.frame()
  for (i in 1:ncol(umap_scores)) {  # Para cada dimensi√≥n de UMAP
    umap_component <- umap_scores[, i]  # Extraer la puntuaci√≥n de la dimensi√≥n
    model <- kruskal.test(umap_component ~ metadata[[factor]])  # Realizar la prueba de Kruskal-Wallis
    factor_effect <- model$p.value  # Obtener el p-valor
    results <- rbind(results, data.frame(UMAP_Dim = i, Factor = factor, PValue = factor_effect))  # Guardar resultados
  }
  return(results)
}

# Lista de factores que quieres analizar
factors <- c("PCR.Cov", "fecha", "equipo")

# Ejecutar la funci√≥n para cada factor y combinar los resultados
kruskal_results_umap <- do.call(rbind, lapply(factors, function(f) kruskal_umap(umap_scores, Datos_actualizados, f)))

# Resumir el p-valor medio para cada factor
factor_summary_umap <- kruskal_results_umap %>%
  group_by(Factor) %>%
  summarise(MeanPValue = mean(PValue))

# Visualizar los resultados en un gr√°fico de barras
ggplot(factor_summary_umap, aes(x = reorder(Factor, MeanPValue), y = MeanPValue, fill = Factor)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Importancia de los Factores en el Efecto Batch (UMAP)",
       x = "Factor",
       y = "P-valor Medio",
       fill = "Factor")

```



17.PVCA
====================================================



```{r}
#PVCA combina PCA y ANOVA (an√°lisis de varianza) para descomponer la varianza total en componentes atribuibles a diferentes factores (variables categ√≥ricas o continuas). Esto permite medir la proporci√≥n de la varianza atribuible a diferentes fuentes.

library(lme4)
library(ggplot2)

# Function to perform PVCA
perform_pvca <- function(featureMatrix_num, Datos_actualizados, factors_to_analyze) {
  print("Starting PVCA analysis...")
  
  # Check inputs
  if (!is.matrix(featureMatrix_num) || !is.numeric(featureMatrix_num)) {
    stop("featureMatrix_num must be a numeric matrix")
  }
  if (!is.data.frame(Datos_actualizados)) {
    stop("Datos_actualizados must be a data frame")
  }
  if (!all(factors_to_analyze %in% names(Datos_actualizados))) {
    stop("Not all factors_to_analyze are present in Datos_actualizados")
  }
  if (nrow(featureMatrix_num) != nrow(Datos_actualizados)) {
    stop("Number of rows in featureMatrix_num and Datos_actualizados must be the same")
  }
  
  # Perform PCA
  print("Performing PCA...")
  pca_result <- prcomp(featureMatrix_num, scale. = TRUE)
  
  # Calculate proportion of variance explained by each PC
  var_prop <- pca_result$sdev^2 / sum(pca_result$sdev^2)
  
  # Initialize results matrix
  n_factors <- length(factors_to_analyze)
  n_pcs <- ncol(pca_result$x)
  vc_matrix <- matrix(0, nrow = n_factors, ncol = n_pcs)
  
  # Perform variance component analysis for each PC
  print("Performing variance component analysis...")
  for (i in 1:n_pcs) {
    pc_data <- data.frame(PC = pca_result$x[, i], Datos_actualizados)
    
    # Create formula for mixed model
    formula <- as.formula(paste("PC ~", paste("(1|", factors_to_analyze, ")", collapse = " + ")))
    
    # Fit mixed model
    tryCatch({
      mixed_model <- lmer(formula, data = pc_data, REML = TRUE)
      
      # Extract variance components
      vc <- as.data.frame(VarCorr(mixed_model))
      vc_matrix[, i] <- vc$vcov[1:n_factors]
    }, error = function(e) {
      warning(paste("Error in PC", i, ":", e$message))
    })
  }
  
  # Weight the variance components by the proportion of variance explained
  weighted_vc <- vc_matrix %*% diag(var_prop)
  
  # Calculate the proportion of variance explained by each factor
  pvca_results <- rowSums(weighted_vc) / sum(var_prop)
  names(pvca_results) <- factors_to_analyze
  
  print("PVCA analysis completed.")
  return(pvca_results)
}

# Function to plot PVCA results
plot_pvca <- function(pvca_results) {
  print("Creating PVCA plot...")
  results_df <- data.frame(
    Factor = names(pvca_results),
    Proportion = pvca_results
  )
  
  p <- ggplot(results_df, aes(x = reorder(Factor, -Proportion), y = Proportion)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    theme_minimal() +
    labs(x = "Factors", y = "Proportion of Variance Explained", 
         title = "Principal Variance Component Analysis") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
  print("Plot created.")
}

# Usage with your existing data:
print("Preparing to run PVCA with your data...")

# Asumiendo que ya tiene featureMatrix_num y Datos_actualizados
# Reemplace 'sus_factores' con los nombres reales de sus factores
factors_to_analyze <- c('PCR.Cov', 'equipo', 'fecha')  # Por ejemplo: c("Factor1", "Factor2", "Factor3")

print("Running PVCA...")
results <- perform_pvca(featureMatrix_num, Datos_actualizados, factors_to_analyze)
print("PVCA Results:")
print(results)

print("Creating plot...")
plot_pvca(results)

print("Script execution completed.")
```



```{r}
help("isSingular")
```


```{r}
dim(featureMatrix_num)
dim(Datos_actualizados)
```


```{r}
require(dplyr)
require(tibble)
require(ggplot2)
```


18.Correcion efecto batch: Combat
====================================================

```{r}
# Cargar el paquete sva
library(sva)

```

```{r}
# Ejemplo de datos de entrada
data_matrix <- featureMatrix_num  # Tu matriz de datos
metadata <- Datos_actualizados    # Dataframe de metadatos con informaci√≥n de batch
batch_info <- metadata$equipo      # Vector con la informaci√≥n de batch

```

```{r}
# Aplicar la correcci√≥n de ComBat
combat_corrected <- ComBat(dat = t(data_matrix),   # Se necesita transponer la matriz
                           batch = batch_info,     # Vector de batches
                           par.prior = TRUE,       # Usa informaci√≥n a priori
                           prior.plots = FALSE)    # No generar plots de prior

# Nota: Si tu batch es un factor con m√°s de dos niveles, esto funcionar√° bien

```

```{r}
# Transponer de vuelta los datos corregidos
combat_corrected <- t(combat_corrected)

# Guardar la matriz corregida
write.csv(combat_corrected, "corrected_featureMatrix.csv")

```

```{r}
# Asignar colores manualmente en funci√≥n de batch_info
batch_colors <- as.factor(batch_info)   # Convertir batch_info a factor
levels(batch_colors)  # Ver los niveles de batch_info

# Definir colores manualmente (puedes cambiar los nombres de niveles y colores)
col_vector <- batch_colors
levels(col_vector) <- c("red", "blue", "green")  # Asigna colores a cada batch
```


```{r}
equipo_colors <- as.factor(metadata$equipo)  # Convertir 'equipo' a factor

# Asignar colores
palette_colors <- rainbow(length(unique(equipo_colors)))  # Colores √∫nicos
col_vector <- palette_colors[equipo_colors]
```

```{r}
pca_after <- prcomp(combat_corrected, scale. = TRUE)

# Plot PCA despu√©s de la correcci√≥n
plot(pca_after$x[,1:2], col = col_vector, main = "PCA despu√©s de la correcci√≥n")
legend("topright", 
       legend = levels(batch_colors), 
       col = palette_colors, 
       pch = 16, 
       title = "Batch")
```

```{r}
library(lme4)
library(ggplot2)

# Funci√≥n para realizar PVCA en los datos corregidos
perform_pvca <- function(combat_corrected, Datos_actualizados, factors_to_analyze) {
  print("Starting PVCA analysis...")

  # Comprobaciones de entrada
  if (!is.matrix(combat_corrected) || !is.numeric(combat_corrected)) {
    stop("combat_corrected must be a numeric matrix")
  }
  if (!is.data.frame(Datos_actualizados)) {
    stop("Datos_actualizados must be a data frame")
  }
  if (!all(factors_to_analyze %in% names(Datos_actualizados))) {
    stop("Not all factors_to_analyze are present in Datos_actualizados")
  }
  if (nrow(combat_corrected) != nrow(Datos_actualizados)) {
    stop("Number of rows in combat_corrected and Datos_actualizados must be the same")
  }
  
  # Realizar PCA sobre los datos corregidos por ComBat
  print("Performing PCA on ComBat corrected data...")
  pca_result <- prcomp(combat_corrected, scale. = TRUE)
  
  # Calcular proporci√≥n de varianza explicada por cada componente
  var_prop <- pca_result$sdev^2 / sum(pca_result$sdev^2)
  
  # Inicializar la matriz de resultados
  n_factors <- length(factors_to_analyze)
  n_pcs <- ncol(pca_result$x)
  vc_matrix <- matrix(0, nrow = n_factors, ncol = n_pcs)
  
  # Realizar an√°lisis de componentes de varianza para cada PC
  print("Performing variance component analysis...")
  for (i in 1:n_pcs) {
    pc_data <- data.frame(PC = pca_result$x[, i], Datos_actualizados)
    
    # Crear f√≥rmula para el modelo mixto
    formula <- as.formula(paste("PC ~", paste("(1|", factors_to_analyze, ")", collapse = " + ")))
    
    # Ajustar el modelo mixto
    tryCatch({
      mixed_model <- lmer(formula, data = pc_data, REML = TRUE)
      
      # Extraer componentes de varianza
      vc <- as.data.frame(VarCorr(mixed_model))
      vc_matrix[, i] <- vc$vcov[1:n_factors]
    }, error = function(e) {
      warning(paste("Error in PC", i, ":", e$message))
    })
  }
  
  # Ponderar los componentes de varianza por la proporci√≥n de varianza explicada
  weighted_vc <- vc_matrix %*% diag(var_prop)
  
  # Calcular la proporci√≥n de varianza explicada por cada factor
  pvca_results <- rowSums(weighted_vc) / sum(var_prop)
  names(pvca_results) <- factors_to_analyze
  
  print("PVCA analysis completed.")
  return(pvca_results)
}

# Funci√≥n para graficar los resultados de PVCA
plot_pvca <- function(pvca_results) {
  print("Creating PVCA plot...")
  results_df <- data.frame(
    Factor = names(pvca_results),
    Proportion = pvca_results
  )
  
  p <- ggplot(results_df, aes(x = reorder(Factor, -Proportion), y = Proportion)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    theme_minimal() +
    labs(x = "Factors", y = "Proportion of Variance Explained", 
         title = "Principal Variance Component Analysis After ComBat") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
  print("Plot created.")
}

# Ejecuci√≥n del PVCA sobre los datos corregidos
print("Preparing to run PVCA after ComBat correction...")

# Aseg√∫rate de que `combat_corrected` (matriz corregida) y `Datos_actualizados` est√©n disponibles
# Los factores a analizar son los mismos que antes
factors_to_analyze <- c('PCR.Cov', 'equipo', 'fecha')

print("Running PVCA on ComBat corrected data...")
combat_pvca_results <- perform_pvca(combat_corrected, Datos_actualizados, factors_to_analyze)

print("PVCA Results after ComBat:")
print(combat_pvca_results)

print("Creating plot for PVCA results after ComBat...")
plot_pvca(combat_pvca_results)

print("Script execution completed.")


```



```{r}
library(umap)


# Ajustar par√°metros directamente en la funci√≥n umap
umap_v4 <- umap(combat_corrected, n_neighbors = 15, metric = "cosine", min_dist = 0.1)

# Verificar la estructura del objeto umap_v2
str(umap_v4) # Esto te mostrar√° la estructura para asegurarte de que layout es un componente v√°lido.

# Si la estructura es correcta, continuar con la conversi√≥n a dataframe
umap_coor4 <- as.data.frame(umap_v4)

# Asignar nombres de columnas
colnames(umap_coor4) <- c("UMAP1", "UMAP2")

# A√±adir las columnas con los metadatos (COVID, equipo, etc.)
umap_coor4$covid <- Datos_actualizados$PCR.Cov
umap_coor4$equipo <- Datos_actualizados$equipo

# Graficar UMAP con ggplot2
library(ggplot2)
ggplot(umap_coor4, aes(x = UMAP1, y = UMAP2, color = equipo, shape = covid)) +
  geom_point(size = 3) +
  labs(title = "UMAP luego de correcci√≥n por COMBAT",
       x = "UMAP1",
       y = "UMAP2") +
  theme_minimal()
```


```{r}


umap_scores <- umap_v4  # Tomar el resultado de UMAP directamente

# Luego sigues con el mismo an√°lisis
metadata_umap <- cbind(Datos_actualizados[,c(4,5,7)], umap_scores)

# Funci√≥n para realizar Kruskal-Wallis sobre las dimensiones de UMAP
kruskal_umap <- function(umap_scores, metadata, factor) {
  results <- data.frame()
  for (i in 1:ncol(umap_scores)) {  # Para cada dimensi√≥n de UMAP
    umap_component <- umap_scores[, i]  # Extraer la puntuaci√≥n de la dimensi√≥n
    model <- kruskal.test(umap_component ~ metadata[[factor]])  # Realizar la prueba de Kruskal-Wallis
    factor_effect <- model$p.value  # Obtener el p-valor
    results <- rbind(results, data.frame(UMAP_Dim = i, Factor = factor, PValue = factor_effect))  # Guardar resultados
  }
  return(results)
}

# Lista de factores que quieres analizar
factors <- c("PCR.Cov", "fecha", "equipo")

# Ejecutar la funci√≥n para cada factor y combinar los resultados
kruskal_results_umap <- do.call(rbind, lapply(factors, function(f) kruskal_umap(umap_scores, Datos_actualizados, f)))

# Resumir el p-valor medio para cada factor
factor_summary_umap <- kruskal_results_umap %>%
  group_by(Factor) %>%
  summarise(MeanPValue = mean(PValue))

# Visualizar los resultados en un gr√°fico de barras
ggplot(factor_summary_umap, aes(x = reorder(Factor, MeanPValue), y = MeanPValue, fill = Factor)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Importancia de los Factores en el Efecto Batch (UMAP) despues de COMBAT",
       x = "Factor",
       y = "P-valor Medio",
       fill = "Factor")

```







```{r}
library(MALDIquant)
library(MALDIquantForeign)
library(readBrukerFlexData)
library(ggplot2)
library(caret)
library(stats)
library(binda)
library(factoextra)
library(binda)
library(dplyr)
library(crossval)
library(ggrepel)
library(corrr)
library(ggplot2)
library(FactoMineR)
```

19.Modelos (con datos combat_corrected)
====================================================

```{r}
#Dicotimizacion de la matriz de intensidad
 #optimizeThreshold uses (approximate) mutual information to determine the optimal thresholds. Specifically, the thresholds are chosen to maximize the mutual information between response and each variable
thr <- optimizeThreshold(combat_corrected, Datos_actualizados$PCR.Cov)
combat_corrected_dicho <- dichotomize(combat_corrected, thr) #2 MATRIZ DE INTENSIDAD DICOTOMIZADA
```


```{r}
combat_corrected_dicho<- cbind(combat_corrected_dicho, label=Datos_actualizados$PCR.Cov)#1=neg,2=pos
```



```{r}
combat_corrected_dicho<-as.data.frame(combat_corrected_dicho)
combat_corrected_dicho$label<-as.factor(combat_corrected_dicho$label)
str(combat_corrected_dicho)
```
```{r}
library(dplyr)
combat_corrected_dicho <- combat_corrected_dicho %>% rename(Y = label)
str(combat_corrected_dicho$Y)
```


```{r}
# set.seed asegura reproducibilidad
set.seed(42)

# Crear partici√≥n usando la columna 18 (aseg√∫rate de que esta columna es la de las etiquetas)
trainIndex <- createDataPartition(combat_corrected_dicho[, 18], p = 0.8, list = FALSE)

# Crear conjuntos de entrenamiento y prueba, asegurando que ambos son dataframes
trainData <- as.data.frame(combat_corrected_dicho[trainIndex, , drop = FALSE])
testData <- as.data.frame(combat_corrected_dicho[-trainIndex, , drop = FALSE])

# Verificar los nombres de columnas (ajusta si la columna no se llama 'covid')
colnames(trainData)

# Verificar la proporci√≥n de clases (ajusta el nombre si no es 'covid')
table(trainData$Y)
table(testData$Y)

```


```{r}
control1 <- trainControl(method = "cv", number = 5)
#en esta primera vez pruebo con crossval
```


```{r}
# Especificar la variable dependiente en la f√≥rmula
formula <- Y ~ .
```


```{r}
#accuracy=Es la proporci√≥n de predicciones correctas realizadas por el modelo con respecto al total de predicciones.

#kappa= Es una m√©trica que mide el grado de acuerdo entre las predicciones del modelo y las clases verdaderas, ajustando por la posibilidad de que el acuerdo se deba al azar.
```


#19.1 Random forest

```{r}
library(caret)
control1 <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 5, 10)) #the randomForest package only has one available tuning parameter, mtry.

RF_model <- train(Y ~ ., data = trainData, 
                      method = "rf",   # M√©todo para random forest
                      trControl = control1, 
                      tuneGrid = grid_rf,  # Par√°metro mtry
                      metric = "Accuracy")  # M√©trica para clasificaci√≥n

```

```{r}
RF_model
```

```{r}
plot(RF_model)
```


#19.2 GBM

```{r}
# grid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5, 7, 9, 11), 
#   n.trees = (1:10) * 100, 
#   shrinkage = c(0.01, 0.05, 0.1, 0.2),
#   n.minobsinnode = c(5, 10, 15, 20, 25)
# )

# 
# GBM_model<-train(Y ~ ., data = trainData, 
#                       method = "gbm",   # M√©todo para random forest
#                       trControl = control1, 
#                       tuneGrid = grid_gbm, 
#                       metric = "Accuracy")


GBM_model <- train(
  Y ~ .,                        # Reemplaza Y con tu variable dependiente
  data = trainData,            # Tu conjunto de entrenamiento
  method = "gbm",              # M√©todo GBM
  trControl = control1,         # Control de entrenamiento
  tuneGrid = expand.grid(
    interaction.depth = 3,     # Profundidad m√°xima de los √°rboles
    n.trees = 100,             # N√∫mero de √°rboles
    shrinkage = 0.1,           # Tasa de aprendizaje
    n.minobsinnode = 10        # M√≠nimo de observaciones en cada nodo
  ),
  metric = "Accuracy",         # M√©trica de rendimiento
  verbose = TRUE               # Mostrar progreso
)
GBM_model

```



```{r}
# trellis.par.set(caretTheme())
# plot(GBM_model)
# 
# trellis.par.set(caretTheme())
# plot(GBM_model, metric = "Kappa")
```

#19.3 SVM


```{r}
SVM_model <- train(Y ~., data = trainData, method = "svmRadial", trControl = control1, preProcess = c("center","scale"), tuneLength = 10)
# Print the best tuning parameter sigma and C that maximizes model accuracy
SVM_model$bestTun    #there is a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
#sigma: es propio del tipo de kernel que estamos utilizando y en el fondo va a regular el overfitting del modelo

SVM_model
#grid search SVM

# tune_grid <- expand.grid(
#   C = 2^(-5:5),      # Rango de valores de C: 0.03125, 0.0625, ..., 32
#   sigma = c(0.01, 0.1, 0.5, 1)  # Rango de valores de sigma
# )
# 
# # Ajustar el modelo SVM con tuning de hiperpar√°metros
# SVM_model <- train(
#   Y ~ ., 
#   data = trainData, 
#   method = "svmRadial", 
#   trControl = control1, 
#   preProcess = c("center", "scale"), 
#   tuneGrid = tune_grid,  # Usar el grid de hiperpar√°metros
#   metric = "Accuracy"     # M√©trica para evaluaci√≥n
# )
```

```{r}
plot(SVM_model)
```


#19.3 KNN


```{r}
tuneGrid <- expand.grid(k = 1:15)

# Ajustar el modelo KNN
KNN_modelo <- train(formula, 
                    data = trainData, 
                    trControl = control1, 
                    method = "knn", 
                    metric = "Accuracy", 
                    preProcess = c("center","scale"),
                    tuneGrid = tuneGrid)

KNN_modelo
```
```{r}
plot(KNN_modelo)
```

#19.4 GLMNET

```{r}

tuneGrid <- expand.grid(
  alpha = seq(0, 1, by = 0.1),  # From 0 (Ridge) to 1 (Lasso)
  lambda = 10^seq(-3, 3, length = 100)  # Logarithmic scale for lambda
)

GLM_model <- train(formula, 
                  data = trainData,  
                   method = "glmnet",
                   metric = "Accuracy",
                   tuneLength = 3,
                   trControl = control1,tuneGrid=tuneGrid)

GLM_model 
```


```{r}
# Imprimir los mejores hiperpar√°metros
best_params <- GLM_model$bestTune
print("Best Hyperparameters:")
print(best_params)

# Extraer accuracy y kappa del mejor modelo
# Buscamos la fila que corresponde a los mejores hiperpar√°metros en el data frame model$results
best_model_index <- apply(GLM_model$results, 1, function(row) {
  all(row[1:length(best_params)] == as.numeric(best_params))
})

best_model_results <- GLM_model$results[best_model_index, ]

# Imprimir accuracy y kappa del mejor modelo
accuracy <- best_model_results$Accuracy
kappa <- best_model_results$Kappa

print("Accuracy of the Best Model:")
print(accuracy)

print("Kappa of the Best Model:")
print(kappa)

```

20. Prediccion
==========================================================

```{r}
testData$Y<-as.factor(testData$Y)
str(testData$Y)
```
```{r}
testData_NOID <- testData[, -which(names(testData) == "Y")]

```



```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesRF<-predict(RF_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesRF)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixRF <- table(prediccionesRF, y_test)
# Imprimir la matriz de confusi√≥n
print(confusion_matrixRF)

```

```{r}
#metricas
# Precisi√≥n
accuracy_RF <- sum(diag(confusion_matrixRF)) / sum(confusion_matrixRF)
print(accuracy_RF)
# Precisi√≥n positiva (VP) o sensibilidad
sensitivity_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[2, ])
print(sensitivity_RF)
# Precisi√≥n negativa (VN) o especificidad
specificity_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF[1, ])
specificity_RF

# Valor predictivo positivo (VPP)
ppv_RF <- confusion_matrixRF[2, 2] / sum(confusion_matrixRF[, 2])
print(ppv_RF)
# Valor predictivo negativo (VPN)
npv_RF <- confusion_matrixRF[1, 1] / sum(confusion_matrixRF)
print(npv_RF)
```


#20. Prediccion GBM
 
```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesGBM<-predict(GBM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesGBM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixGBM <- table(prediccionesGBM, y_test)
# Imprimir la matriz de confusi√≥n
print(confusion_matrixGBM)

```

```{r}
#metricas
# Precisi√≥n
accuracy_GBM <- sum(diag(confusion_matrixGBM)) / sum(confusion_matrixGBM)
print(accuracy_GBM)
# Precisi√≥n positiva (VP) o sensibilidad
sensitivity_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[2, ])
print(sensitivity_GBM)
# Precisi√≥n negativa (VN) o especificidad
specificity_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[1, ])
specificity_GBM

# Valor predictivo positivo (VPP)
ppv_GBM <- confusion_matrixGBM[2, 2] / sum(confusion_matrixGBM[, 2])
print(ppv_GBM)
# Valor predictivo negativo (VPN)
npv_GBM <- confusion_matrixGBM[1, 1] / sum(confusion_matrixGBM[, 1])
print(npv_GBM)
```



#20. Prediccion SVM

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesSVM<-predict(SVM_model, newdata = testData_NOID)#matriz binarizada
print(prediccionesSVM)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```

```{r}
confusion_matrixSVM <- table(prediccionesSVM, y_test)
# Imprimir la matriz de confusi√≥n
print(confusion_matrixSVM)

```
```{r}
#metricas
# Precisi√≥n
accuracy_SVM <- sum(diag(confusion_matrixSVM)) / sum(confusion_matrixSVM)
print(accuracy_SVM)
# Precisi√≥n positiva (VP) o sensibilidad
sensitivity_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[2, ])
print(sensitivity_SVM)
# Precisi√≥n negativa (VN) o especificidad
specificity_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[1, ])
specificity_SVM

# Valor predictivo positivo (VPP)
ppv_SVM <- confusion_matrixSVM[2, 2] / sum(confusion_matrixSVM[, 2])
print(ppv_SVM)
# Valor predictivo negativo (VPN)
npv_SVM <- confusion_matrixSVM[1, 1] / sum(confusion_matrixSVM[, 1])
print(npv_SVM)
```





#20. Prediccion KNN

```{r}
#prediccionesRF1<-predict(resultadosRF, newdata = dataNOID_test)
prediccionesKNN<-predict(KNN_modelo, newdata = testData_NOID)#matriz binarizada
print(prediccionesKNN)


# Obtener las etiquetas reales del conjunto de datos de prueba
y_test <- testData$Y
#y_testDIC <- FeatureMatrix_Dic_df$Y#Matriz binarizada

```


```{r}
confusion_matrixKNN <- table(prediccionesKNN, y_test)
# Imprimir la matriz de confusi√≥n
print(confusion_matrixKNN)

```

```{r}
#metricas
# Precisi√≥n
accuracy_KNN <- sum(diag(confusion_matrixKNN)) / sum(confusion_matrixKNN)
print(accuracy_KNN)
# Precisi√≥n positiva (VP) o sensibilidad
sensitivity_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[2, ])
print(sensitivity_KNN)
# Precisi√≥n negativa (VN) o especificidad
specificity_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[1, ])
specificity_KNN

# Valor predictivo positivo (VPP)
ppv_KNN <- confusion_matrixKNN[2, 2] / sum(confusion_matrixKNN[, 2])
print(ppv_KNN)
# Valor predictivo negativo (VPN)
npv_KNN <- confusion_matrixKNN[1, 1] / sum(confusion_matrixKNN[, 1])
print(npv_KNN)
```

```{r}
resultados_modelos <- data.frame(
  Modelo = c("RF_model", "GBM_model", "SVM_model", "KNN_model"),
  Accuracy = c(accuracy_RF, accuracy_GBM, accuracy_SVM, accuracy_KNN),
  Sensibilidad=c(sensitivity_RF, sensitivity_GBM, sensitivity_SVM, sensitivity_KNN),
  Especificidad = c(specificity_RF, specificity_GBM, specificity_SVM, specificity_KNN)
)

# Imprimir la tabla
print(resultados_modelos)
```

