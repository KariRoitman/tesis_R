---
title: "Referencias"
output: html_document
date: "2024-08-19"
---

PCA
=====================================================================

```{r}
# Principal component analyses (PCA)

library(FactoMineR)
library(factoextra)

data( decathlon2)
View(decathlon2)

# Active individuals (in light blue, rows 1: 23) : 
# individuals that are used during the PCA

# Supplementary individuals (in dark blue, rows 24: 27) : 
# the coordinates of these individuals will be predicted using the 
# PCA information and parameters obtained with active individuals/
# variables.

# Active variables (in pink, columns 1: 10) : 
# variables that are used for the principal component analysis.

# Supplementary variables: will be predicted

# 1) subsetting

decathlon2.active <- decathlon2[ 1:23, 1:10]

# Data must be standarize (scale)

res.pca <- PCA( decathlon2.active,graph = FALSE)

print( res.pca)

# eigenvalue measures the amount of variation retained by each 
# principal component. 
# 1) cutoff is 1 for standarize data
# 2) porcentaje of achieved variation wanted (80%)

get_eigenvalue(res.pca)
fviz_eig(res.pca)
 
   
get_pca_var(res.pca)
fviz_pca_var(res.pca)

get_pca_ind(res.pca)
fviz_pca_ind(res.pca)

fviz_pca_biplot(res.pca)
```

Clustering 
=======================================================================

```{r}
devtools::install_github("kassambara/factoextra")
data("USArrests")

head(USArrests, 3)

library("cluster")
library("factoextra")
```

# To perform a cluster analysis in R, generally, the data should be prepared as follow:

```{r}
# 1) Rows are observations (individuals) and columns are variables
df <- USArrests
# 2) Any missing value in the data must be removed or estimated
df <- na.omit(df)
# 3) The data must be standardized (i.e., scaled) to make variables comparable. 
#    Recall that, standardization consists of transforming the variables such that they 
#     have mean zero and standard deviation one. This is particularly recommended when variables are
#     measured in diﬀerent scales (e.g: kilograms, kilometers, centimeters, ...); otherwise,
#     the dissimilarity measures obtained will be severely aﬀected.

df<-scale(df)
head(df, n = 3)
```

# Clustering Distance Measures

# The classiﬁcation of observations into groups requires some methods for computing
# the distance or the (dis)similarity between each pair of observations. The result of 
# this computation is known as a dissimilarity or distance matrix.

1) The classical methods for distance measures are Euclidean and Manhattan distances 

2) Correlation-based distances, which is widely used for gene expression data analyses:

   *a) Pearson correlation distance: degree of a linear relationship between two proﬁles, 
               parametric correlation which depends on the distribution of the data
   *b) Spearman correlation distance: correlation between the rank of y and x.
   *c) Kendall correlation distance: correspondence between the ranking of xand y variables.

    *b and c) non-parametric and they are used to perform rank-based correlation analysis

 Correlation-based distance considers two objects to be similar if their features are highly 
 correlated, even though the observed values may be far apart in terms of Euclidean distance.

 Pearson’s correlation is quite sensitive to outliers. This can be mitigated by using 
 Spearman’s correlation instead of Pearson’s correlation.

 If we want to identify clusters of observations with the same overall proﬁles regardless of 
 their magnitudes, then we should go with correlation-based distance  as a dissimilarity measure.

 If Euclidean distance is chosen, then observations with high values of features will be
 clustered together. The same holds true for observations with low values of features.

 If standarization is made, Therefore, the result obtained with Pearson correlation measures 
 and standardized Euclidean distances are comparable.

# distance matrix computation 

```{r}
set.seed(123)
ss <-sample(1:50, 15)
# Subset the 15 rows
df <- USArrests[ss, ]
df.scaled <-scale(df)

# dist(), just accepts numeric data as input 

# get_dist(), just accepts numeric data as input. supports correlation-based distance
# measures including “pearson”, “kendall” and “spearman” methods

# daisy(), Able to handle other variable types (e.g. nominal, ordinal, (a)symmetric binary). 
# In that case, the Gower’s coeﬃcient willbe automatically used as the metric.

# 1) Ecuclidean distance 

dist.eucl <-dist(df.scaled, method = "euclidean")

round(as.matrix(dist.eucl)[1:3, 1:3], 1)

# Hence, if we want to compute pairwise distances between variables, we must start by 
# transposing the data to have variablesin the rows of the data set before using the
# dist() function. The function t() is used for transposing the data.

# 2) Computing correlation based distances

library("factoextra")

dist.cor <-get_dist(df.scaled, method = "pearson")
round(as.matrix(dist.cor)[1:3, 1:3], 1)

# 3) Computing distances for mixed data

library(cluster)

data(flower)
dd <- daisy(flower)
round(as.matrix(dd)[1:3, 1:3], 2)

# 4) Visualizing distance matrices

# A simple solution for visualizing the distance matrices is to use the function fviz_dist().
# Other specialized methods, such as agglomerative hierarchical clustering or heatmap.

fviz_dist(dist.eucl)

# The color level is proportional to the value of the dissimilarity between observations:
# Red: high similarity (ie: low dissimilarity) |Blue: low similarity
```


1) Partitioning clustering
=======================================================================

 Are clustering methods (3) used to classify observations, withina data set, into multiple groups 
 based on their similarity. The algorithms require the analyst to specify the number of 
 clusters to be generated.
 1) K-means clustering (MacQueen, 1967), in which, each cluster is represented by the center or 
     means of the data points belonging to the cluster. The K-means method is sensitive to 
     anomalous data points and outliers.
 2) K-medoids clustering or PAM (Partitioning Around Medoids, Kaufman & Rousseeuw, 1990), 
    in which, each cluster is represented by one of the objects in
    the cluster. PAM is less sensitive to outliers compared to k-means.
 3) CLARA algorithm (Clustering Large Applications), which is an extension to PAM adapted for 
    large data sets.

2) K-means clustering
=======================================================================

 Most commonly used unsupervised machine learning algorithm for partitioning a given data set 
 into a set of k groups.  Where k represents the number of groups pre-speciﬁed by the analyst. 
 objects within the same cluster are as similar as possible (i.e., high intra-class similarity),
 whereas objects from diﬀerent clusters are as dissimilar as possible (i.e., low inter-class 
 similarity).

 The basic idea behind k-means clustering consists of deﬁning clusters so that the total
 intra-cluster variation (known as total within-cluster variation) is minimized. 
 The standard algorithm is the Hartigan-Wong algorithm (1979), which deﬁnes the total within-
 cluster variation as the sum of squared distances Euclidean distances between items and the 
 corresponding centroid. 

 The ﬁrst step when using k-means clustering is to indicate the number of clusters (k)
 that will be generated in the ﬁnal solution. The algorithm starts by randomly selecting k 
 objects from the data set to serve as the initial centers for the clusters. The selected objects 
 are also known as cluster means or centroids. Each of the remaining objects is assigned to 
 it’s closest centroid, where closest isdeﬁned using the Euclidean distance between the object 
 and the cluster mean. This step is called “cluster assignment step”. After the assignment step,
 the algorithm computes the new mean value of each cluster.The term cluster “centroid update” 
 is used to design this step.
 
```{r}
df <- USArrests
df <- na.omit(df)
df<-scale(df)
head(df, n = 3)
```
 
# a) Estimating the optimal number of clusters

# The idea is to compute k-means clustering using diﬀerent values of clusters k. Next, the wss 
# (within sum of square) is drawn according to the number of clusters. The location of a bend
# (knee) in the plot is generally considered as an indicator of the appropriate number of clusters

```{r}
fviz_nbclust(df, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)
```

# b) Computing k-means clustering

# R will try 25 diﬀerent random starting assignments and then select the best results 
# corresponding to the one withthe lowest within cluster variation.

```{r}
set.seed(123)
km.res <-kmeans(df, 4, nstart = 25)
print(km.res)
```

# The printed output displays:

    • the cluster means or centers: a matrix, which rows are cluster number (1 to 4)
     and columns are variables
   •  the clustering vector: A vector of integers (from 1:k) indicating the cluster to
      which each point is allocated

```{r}
#mean of each variables by clusters using the original data:
aggregate(USArrests, by=list(cluster=km.res$cluster), mean) 

dd <-cbind(USArrests, cluster = km.res$cluster)
head(dd)
```

c) Accessing to the results of kmeans() function

```{r}
# Cluster number for each of the observations
head(km.res$cluster, 4)
km.res$size

# Cluster means
km.res$centers
```


d) Visualizing k-means clusters

 Now, we want to visualize the data in a scatter plot with coloring each data point
 according to its cluster assignment. The problem is that the data contains more than 2 
 variables and the question is what variables to choose for the xy scatter plot.
 A solution is to reduce the number of dimensions by applying a dimensionality reduction
 algorithm, such as Principal Component Analysis (PCA), that operates on the four variables and 
 outputs two new variables (that represent the original variables) that you can use to do the plot

 if we have a multi-dimensional data set, a solution is to perform
 Principal Component Analysis (PCA) and to plot data points according to the ﬁrst
 two principal components coordinates.

```{r}

fviz_cluster(km.res, data = df, 
             palette =c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
             ellipse.type = "euclid", # Concentration ellipse
             star.plot = TRUE, # Add segments from centroids to items
             repel = TRUE, # Avoid label overplotting (slow)
             ggtheme =theme_minimal())
```

# Dissavantages and solutions

 1) assumes prior knowledge of the data and requires the analyst to choose the
    appropriate number of cluster (k) in advance. 
 Solution: Compute k-means for a range of k values, for exampleby varying k between 2 and 10. 
 Then, choose the best k by comparing the clustering results obtained for the diﬀerent k values.
 2) The ﬁnal results obtained is sensitive to the initial random selection of cluster centers.
 Compute K-means algorithm several times with diﬀerent initial cluster centers. The run with the 
 lowest total within-cluster sum of square is selected as the ﬁnal clustering solution
 3) It’s sensitive to outliers. 
 USe PAM algorithm, which is less sensitive to outliers.
 4) Order of data


3) Hierarchical K-Means Clustering
=======================================================================

 The algorithm is summarized as follow:
 1. Compute hierarchical clustering and cut the tree into k-clusters
 2. Compute the center (i.e the mean) of each cluster
 3. Compute k-means by using the set of cluster centers (deﬁned in step 2) as the
  initial cluster centers

```{r}
df <-scale(USArrests)
res.hk <-hkmeans(df, 4)
names(res.hk)
res.hk
fviz_dend(res.hk, cex = 0.6, palette = "jco",rect = TRUE, rect_border = "jco", rect_fill = TRUE)
fviz_cluster(res.hk, palette = "jco", repel = TRUE,ggtheme =theme_classic())
```



Machine_learnig 
=====================================================================

https://rpubs.com/Edimer/536034
https://rpubs.com/NguyenKhanh20/1069336
http://topepo.github.io/caret/train-models-by-tag.html#random-forest

https://www.stepbystepdatascience.com/feature-selection-with-caret




